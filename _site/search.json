[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Public Notes",
    "section": "",
    "text": "Now generated with quarto. Add or edit posts on github here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nAttenuation bias in PrediXcan\n\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2023\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqqunif function with filtered p-values\n\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to read gencode gtf file\n\n\n\n\n\n\n\nanalysis\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInflation Brainxcan\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCistrome DB data\n\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2023\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nERAP2 fine-mapping\n\n\n\n\n\n\n\nanalysis\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2023\n\n\nHaky Im\n\n\n\n\n\n\n  \n\n\n\n\nMultiple Testing Vignette\n\n\n\n\n\n\n\nvignette\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2023\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJane Austen Corpus\n\n\n\n\n\n\n\nanalysis\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2023\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngtex-sample-size-by-tissue\n\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMigrating\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2023\n\n\nHaky\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictDB weight distribution\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2023\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower calculator for mol QTLs\n\n\n\n\n\n\n\nhow to\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to create a new blog post\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2023\n\n\nHaky Im\n\n\n\n\n\n\n  \n\n\n\n\nHow to prepare effective presentation slides\n\n\n\n\n\n\n\nhow to\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2022\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPARG Fall Social 2022\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2022\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nukbREST SQLite Setup\n\n\n\n\n\n\n\ninstalllation\n\n\nhow to\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nSabrina Mi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to annotate genes - BioMart Basics\n\n\n\n\n\n\n\ncheatsheet\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2022\n\n\nSabrina Mi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTranscriptome QGT Lab 2022 Setup\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2022\n\n\n\n\n\n\n  \n\n\n\n\nTranscriptome QGT Training 2023\n\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2022\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to query eqtl info from GTEx API (a simple way)\n\n\n\n\n\n\n\nhow to\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2022\n\n\nCharles Zhou\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow-to-open-jupyter-notebook-on-CRI-or-RCC\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2022\n\n\nCharles Zhou\n\n\n\n\n\n\n  \n\n\n\n\nPublishing a ShinyApp\n\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2022\n\n\nEthan Tai\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiobank Japan Data in CRI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2022\n\n\nSabrina Mi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nARIC PWAS models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2021\n\n\nSabrina Mi\n\n\n\n\n\n\n  \n\n\n\n\nARIC EA hg38 validation\n\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2021\n\n\nSabrina Mi\n\n\n\n\n\n\n  \n\n\n\n\nARIC EA hg38 validation height\n\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2021\n\n\nSabrina Mi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nARIC PWAS models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2021\n\n\nSabrina Mi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCovariances EA hg38\n\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2021\n\n\nSabrina Mi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeights AA\n\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2021\n\n\nSabrina Mi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeights EA\n\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2021\n\n\nSabrina Mi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJapan Biobank Data Summary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2021\n\n\nSabrina Mi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling R packages without admin access\n\n\n\n\n\n\n\nhow to\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2021\n\n\nBerkely Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to annotate a gene with the cytogenetic band\n\n\n\n\n\n\n\nhow to\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2021\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTracking the traffic to webpages\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 26, 2021\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nError: 0 % SNPs used\n\n\n\n\n\n\n\nFAQ\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2021\n\n\nFestus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGet list of predictor SNPs and weights in predictdb\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2021\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDownload LD blocks\n\n\n\n\n\n\n\nhow to\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2021\n\n\nFestus Nyasimi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMount Gardner file stystem to your computer\n\n\n\n\n\n\n\nhow to\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2021\n\n\nFestus Nyasimi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinks to How To’s\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2021\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Gene Expression Prediction Models\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2021\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictDB Tutorial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2021\n\n\nFestus Nyasimi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGithub Cheatsheet\n\n\n\n\n\n\n\ncheatsheet\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2021\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a new post\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2021\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubsetting HapMap3 SNPs\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2021\n\n\nYanyu Liang\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use CRI cluster\n\n\n\n\n\n\n\nhow_to\n\n\ncri\n\n\ngardner\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling Conda on CRI\n\n\n\n\n\n\n\ninstalllation\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2021\n\n\nNatasha Santhnam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeritability Calculation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2021\n\n\nNatasha Santhanam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuerying PredictDB sqlite databases\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2021\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBionimbus PDC\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nMar 31, 2021\n\n\nSabrina Mi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSPrediXcan Harmonization Errors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2021\n\n\nSabrina Mi\n\n\n\n\n\n\n  \n\n\n\n\nPsychENCODE Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2020\n\n\nSabrina Mi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrediXcan 0% variant mapping issue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2020\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to interpret a p-value of 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2020\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCRI Gardner upgrade news\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2020\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Large Files\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2020\n\n\nSabrina Mi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to convert GTEx v8 model to hg19 based on UK Biobank SNP set mapping\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2020\n\n\nSabrina Mi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling/Running Tensorqtl on CRI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2020\n\n\nNatasha\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Environments in CRI\n\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2020\n\n\nNatasha Santhanam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling tensorqtl module\n\n\n\n\n\n\n\ninstalllation\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2020\n\n\nFestus Nyasimi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuerying bgen files in CRI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2020\n\n\nSabrina Mi\n\n\n\n\n\n\n  \n\n\n\n\nCalculating H2 across GTEX Brain Samples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2020\n\n\nNatasha Santhanam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploration on regressing out PCs\n\n\n\n\n\n\n\n\n\n\nSome going-to-nowhere exploration on regressing out PCA\n\n\n\n\n\n\nNov 4, 2020\n\n\nYanyu Liang\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGTEx reaction rates h2 similar to permuted h2\n\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2020\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfiguring Windows Subsystem for Linux\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2020\n\n\nIan Waters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngwas-catalog\n\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2020\n\n\nHae Kyung Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Prepare a Post Implementation Report?\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputing on ANL Servers\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Use AWS\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Submit to Zenodo\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Use Workflowr\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDownloading and Decrypting dbGaP Data\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Configure Custom SSH Connection\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMolecular data available in GDC\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Use RCC Cluster\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2020\n\n\n\n\n\n\n  \n\n\n\n\nDownloading Data from Biobank Japan\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2020\n\n\nSabrina Mi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuerying BigQuery\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Training: plink\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2020\n\n\n\n\n\n\n  \n\n\n\n\nUsing R_Markdown\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2020\n\n\nIan Waters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Training: R\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 5, 2020\n\n\nLaura Vairus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Training: Command Line\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 2, 2020\n\n\nLaura Vairus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConverting Fusion weight to PredictDB format\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2020\n\n\nSabrina Mi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroStatGen R Studio Servers using Google Cloud\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2020\n\n\nOwen Melia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGWAS on ANL Servers\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndbGaP Project Renewal\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Download the Data from the UK Biobank\n\n\n\n\n\n\n\nhow_to\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIris prediction with neural network\n\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 1999\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niris dataset analysis\n\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 1999\n\n\nHaky Im\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/2020-07-02-hands-on-training-command-line/index.html",
    "href": "post/2020-07-02-hands-on-training-command-line/index.html",
    "title": "Hands-On Training: Command Line",
    "section": "",
    "text": "In this tutorial, we will learn some basic Unix/Linux commands to perform tasks in the command line.  The command line is an interface that allows you to store, manage, and process data.  Terminal is an app that gives you access to the command line. \n(Note: This tutorial was made for Mac users. For Windows users, follow the tutorial here) (WIP) \nTo start, open Terminal using the computer’s search button.  Whenever you’re in Terminal, you are “standing” in a certain file on your computer.  From there, you can move around folders, create files, and much more. \nFor now, here are a few basic commands and their functions to get you started:\n\n\n\n\n\n\n\n\nCommand\nDenotation\nFunction\n\n\n\n\npwd\n(present working directory)\nshows you the folder you are currently in\n\n\nls\n(list files)\nlists all of the items in your current folder\n\n\ncd\n(change directory)\nmove from folder to folder\n\n\nmkdir\n(make directory)\ncreates a folder in your current folder\n\n\ntouch\n(touch)\ncreates a file\n\n\nmv\n(move/rename)\nmoves or renames files/folders\n\n\ncp\n(copy)\ncopies a file to a new location\n\n\ncp -r\n(copy recursive)\ncopies a folder and everything in it to a new location\n\n\nrm\n(remove here)\ndeletes a file\n\n\nrm -rf\n(remove here recursive force)\ndeletes a folder"
  },
  {
    "objectID": "post/2020-07-02-hands-on-training-command-line/index.html#hands-on-practice",
    "href": "post/2020-07-02-hands-on-training-command-line/index.html#hands-on-practice",
    "title": "Hands-On Training: Command Line",
    "section": "Hands-On Practice",
    "text": "Hands-On Practice\n\nFollow along the commands on Terminal to practice\n\n\nFirst, find out what folder you are currently in\npwd\n\nFind out what files/folders you have in that folder\nls\n\nGo back one folder\ncd ..\n\ncd .. will take you back to the parent folder of the one you are currently in\n\n\nGo to your home directory\ncd\n\ncd ~ will have the same effect\n\n\nGo into your Desktop folder\ncd ~/Desktop/\n\nThe command line is case sensitive, so capitals are important!\n\n\nMake a folder named “Folder” in your Desktop\nmkdir Folder\n\nMake a text file named “file.txt”\ntouch file.txt\n\nMove “file.txt” into “Folder”\nmv file.txt Folder/\n\nmv works for moving both files and folders\n\n\nRename the file to “file2.txt”\nmv file.txt file2.txt\n\nRemember you have to enter “Folder” to work with your file\n\n\nMake a copy of “file2.txt” and move it to your Documents\ncp file2.txt ~/Documents/file2.txt\n\nTo copy files, you must enter the name of the file you want to copy, a space, and then the path to where you want to move it.\nthe .. takes the file from the Desktop to the home directory, it’s parent folder; into the Documents folder; and drops it inside\n\n\nCopy “Folder” and the file inside it to your Documents\ncp -r Folder ../Documents/Folder\n\nRemember you have to go to your Desktop to work with “Folder”\nThe -r (recursive) copies everything inside the folder\nIf you don’t use -r, the command line will give you an error saying “Folder is a directory (not copied)”\n\n\nDelete the file2.txt that you copied into Documents\nrm file2.txt\n\nDelete the “Folder” that you copied into Documents\nrm -rf Folder\n\n-rf stands for “recusive” and “force”\nRecursive deletes everything inside the folder\nForce forces the system to delete the folder. This is a safety feature to make sure you don’t accidentally delete anything\nREMEMBER: Be careful whenever you delete a file/folder because you can’t undo it; Once it’s gone, it’s gone"
  },
  {
    "objectID": "post/2020-07-02-hands-on-training-command-line/index.html#extra-notes",
    "href": "post/2020-07-02-hands-on-training-command-line/index.html#extra-notes",
    "title": "Hands-On Training: Command Line",
    "section": "Extra Notes",
    "text": "Extra Notes\n\nPaths\n\nWhen moving to a folder, you must type out the full “path” to it\nFor example, let’s say you want to go to a folder (named “Folder”) in your Desktop.\nRight now, you are in your “home directory.” If you type cd Folder, Terminal will give you an error, saying “there is no such file or directory.” This is because the command line only looks in your current folder, and there is no “Folder” in your home directory\nInstead, you have to type cd Desktop/Folder. The command line looks for “Desktop” in your home directory, then for “Folder” in your Desktop. \n\nTab (auto-completion)\n\nPressing tab can be extremely helpful and time-saving\nAfter typing in the first few letters of a file/folder you want to interact with, pressing tab will autocomplete the name of it\nIn the case of mutiple files/folders with similar starting letters, pressing tab twice will print a list of the possiblilities\nIf tab doesn’t seem to work, that might indicate that you’re in the wrong folder \n\nArrow Keys\n\nThis is another time-saving trick\nYou can use the up and down keys to copy in previous commands into you current line\nThis can be helpful if you had just a one-letter typo in a long, complicated, previous command"
  },
  {
    "objectID": "post/2020-07-02-hands-on-training-command-line/index.html#for-windows-users",
    "href": "post/2020-07-02-hands-on-training-command-line/index.html#for-windows-users",
    "title": "Hands-On Training: Command Line",
    "section": "For Windows Users",
    "text": "For Windows Users"
  },
  {
    "objectID": "post/2020-07-02-hands-on-training-command-line/index.html#quiz",
    "href": "post/2020-07-02-hands-on-training-command-line/index.html#quiz",
    "title": "Hands-On Training: Command Line",
    "section": "Quiz",
    "text": "Quiz"
  },
  {
    "objectID": "post/2001-07-30-gwas-on-anl-servers/index.html",
    "href": "post/2001-07-30-gwas-on-anl-servers/index.html",
    "title": "GWAS on ANL Servers",
    "section": "",
    "text": "Steps to running the GWAS on ANL’s servers."
  },
  {
    "objectID": "post/2001-07-30-gwas-on-anl-servers/index.html#install-hail",
    "href": "post/2001-07-30-gwas-on-anl-servers/index.html#install-hail",
    "title": "GWAS on ANL Servers",
    "section": "Install Hail",
    "text": "Install Hail\nThis was done by Tom Brettin. Hail is working on the nucleus machine, but not washington at the moment."
  },
  {
    "objectID": "post/2001-07-30-gwas-on-anl-servers/index.html#filter-genotype-files",
    "href": "post/2001-07-30-gwas-on-anl-servers/index.html#filter-genotype-files",
    "title": "GWAS on ANL Servers",
    "section": "Filter Genotype Files",
    "text": "Filter Genotype Files\nDownstream analysis can be a lot faster with a smaller genetic dataset. We filtered for individuals in the brain imaging cohort, and for typical GWAS population conditions: white British ancestry, not related to others in cohort, good SNP call rate, etc.\nThe exact list of eids is at /vol/bmd/meliao/data/eids/intersection_brain_asthma.txt, which is so named because the sample filters wre taken from the ukbREST example asthma query. The filtering was run with plink using file filter_bgen_files.sh.\nThis filtering has mostly been done in plink2."
  },
  {
    "objectID": "post/2001-07-30-gwas-on-anl-servers/index.html#phenotype-and-covariate-files",
    "href": "post/2001-07-30-gwas-on-anl-servers/index.html#phenotype-and-covariate-files",
    "title": "GWAS on ANL Servers",
    "section": "Phenotype and Covariate Files",
    "text": "Phenotype and Covariate Files"
  },
  {
    "objectID": "post/2001-07-30-gwas-on-anl-servers/index.html#set-up-gwas-environment",
    "href": "post/2001-07-30-gwas-on-anl-servers/index.html#set-up-gwas-environment",
    "title": "GWAS on ANL Servers",
    "section": "Set up GWAS environment",
    "text": "Set up GWAS environment\nThere is a hail environment available on the ANL servers accessible by the command\nconda activate /vol/bmd/software/condaenvs/hail"
  },
  {
    "objectID": "post/2001-07-30-gwas-on-anl-servers/index.html#interpret-results",
    "href": "post/2001-07-30-gwas-on-anl-servers/index.html#interpret-results",
    "title": "GWAS on ANL Servers",
    "section": "Interpret Results",
    "text": "Interpret Results"
  },
  {
    "objectID": "post/2020-11-10-querying-bgen-files-in-cri/index.html",
    "href": "post/2020-11-10-querying-bgen-files-in-cri/index.html",
    "title": "Querying bgen files in CRI",
    "section": "",
    "text": "A bgen file has a header block with information about the file, including number of samples, the number of variant data blocks, and flags which describe how data is stores. A variant data block contains data for a single snp, including ID, position, and alleles. bgens from UKBiobank also have a sample identifier block with has an identifier for each sample."
  },
  {
    "objectID": "post/2020-11-10-querying-bgen-files-in-cri/index.html#querying-a-subset-of-a-bgen-genotype",
    "href": "post/2020-11-10-querying-bgen-files-in-cri/index.html#querying-a-subset-of-a-bgen-genotype",
    "title": "Querying bgen files in CRI",
    "section": "Querying a subset of a bgen genotype",
    "text": "Querying a subset of a bgen genotype\nStart an interactive job, qsub -I. We can query data for a region of a chromosome in a bgen file:\n\n\nShow the code\nmodule load gcc/6.2.0; module load bgen/1.1.3\nbgenix -g /gpfs/data/ukb-share/genotypes/v3/ukb_imp_chr17_v3.bgen -incl-range 17:46018872-46026674 > pnpo_37.bgen\n\n\nThis outputs a bgen containing data only for snps on chromosome 17, between positions 46018872 46026674.\nBefore querying it, we need to create an index file:\n\n\nShow the code\nbgenix -g pnpo_37.bgen -index\n\n\nMore documentation is here."
  },
  {
    "objectID": "post/2020-11-10-querying-bgen-files-in-cri/index.html#convert-to-vcf",
    "href": "post/2020-11-10-querying-bgen-files-in-cri/index.html#convert-to-vcf",
    "title": "Querying bgen files in CRI",
    "section": "Convert to VCF",
    "text": "Convert to VCF\nFor some reason, using the bgenix -vcf argument to convert the bgen output to a vcf is unreliable, so we use qctool to convert instead. qctool is installed in /gpfs/data/im-lab/nas40t2/bin. Note that qctool requires gcc, so this will need to be run in a job with the gcc module loaded.\n\n\nShow the code\nexport PATH=$PATH:/gpfs/data/im-lab/nas40t2/bin/software\n\nbgenix -g pnpo_37.bgen | qctool -g - -filetype bgen -s /gpfs/data/ukb-share/genotypes/ukb19526_imp_chr1_v3_s487395.sample -og ~/PNPO_37.vcf\n\n\nRun qctool -help for a list of options, or for more documentation, https://www.well.ox.ac.uk/~gav/qctool_v2/index.html."
  },
  {
    "objectID": "post/2022-05-22-how-to-query-eqtl-info-from-gtex-api-a-simple-way/index.html",
    "href": "post/2022-05-22-how-to-query-eqtl-info-from-gtex-api-a-simple-way/index.html",
    "title": "How to query eqtl info from GTEx API (a simple way)",
    "section": "",
    "text": "This is a simple and convenient way to get some information about certain eqtls from GTEx API. But it may not be a good way to do big query because it’s not so fast.\n\n\nimport requests\nimport json\nimport pandas as pd \nimport time # if you want to measure the query speed\n\n\n\n\n\ngencode = \"ENSG00000116127.17\"\ngene_name = \"ALMS1\"\ntissue = \"Heart_Left_Ventricle\" # provide an exmple\n\n\n\n\ngene_eqtls = requests.get('https://gtexportal.org/rest/v1/association/singleTissueEqtl', params = {\"gencodeId\" : gencode,\"tissueSiteDetailId\" : tissue, \"datasetId\" : \"gtex_v8\", \"variantId\" : 'chr2_73325414_G_A_b38'})\n\ndata = json.loads(gene_eqtls.text)\ndata\nThe result should be like:\n{'singleTissueEqtl': [{'chromosome': 'chr2',\n   'datasetId': 'gtex_v8',\n   'gencodeId': 'ENSG00000116127.17',\n   'geneSymbol': 'ALMS1',\n   'geneSymbolUpper': 'ALMS1',\n   'nes': 0.18502,\n   'pValue': 1.1244e-05,\n   'pos': 73325414,\n   'snpId': 'rs7573275',\n   'tissueSiteDetailId': 'Heart_Left_Ventricle',\n   'variantId': 'chr2_73325414_G_A_b38'}]}\nFor some datasets like singleTissueEqtl with URL https://gtexportal.org/rest/v1/association/singleTissueEqtl, we can ask multiple variants with a single query. For example, we want effect sizes of two variants in a certain tissue.\ngencode = \"ENSG00000227232.5\"\ntissue = \"Lung\"\nvariantId = (\"chr1_665098_G_A_b38\",\"chr1_88794_T_A_b38\")\n\ntest = requests.get('https://gtexportal.org/rest/v1/association/singleTissueEqtl', \n                   params = {\"gencodeId\" : gencode,\"tissueSiteDetailId\" : tissue, \"datasetId\" : \"gtex_v8\", \"variantId\" : variantId})\n\ndata_test = json.loads(test.text)\n\nfor i in range(len(data_test['singleTissueEqtl'])):\n             print(data_test['singleTissueEqtl'][i]['nes'])\nThe result:\n0.376198\n1.07028\nFor some other dataset like dyneqtl with the URL https://gtexportal.org/rest/v1/association/dyneqtl, we can’t ask multiple variants/genes/tissues with a single query."
  },
  {
    "objectID": "post/2023-04-27-how-to-read-gencode-gff/index.html",
    "href": "post/2023-04-27-how-to-read-gencode-gff/index.html",
    "title": "How to read gencode gtf file",
    "section": "",
    "text": "Show the code\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\nSLUG=\"how-to-read-gencode-gtf-file\" ## copy the slug from the header\nbDATE='2023-04-27' ## copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n\n## move data to DATA\n#tempodata=(\"~/Downloads/tempo/gwas_catalog_v1.0.2-associations_e105_r2022-04-07.tsv\")\n#system(glue::glue(\"cp {tempodata} {DATA}/\"))\n\n#system(glue(\"open {DATA}\")) ## this will open the folder \n\n\n\ninstall if necessary\n\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"rtracklayer\")\n\n\nShow the code\n# Load the rtracklayer package\nlibrary(rtracklayer)\n\n# Define the path to your GFF file\ngff_file_path <- glue(\"{DATA}/gencode.v43.annotation.gff3.gz\")\n\n# Read the GFF file\ngff_data <- import(gff_file_path, format = \"gff3\")\n\n# Convert the GFF data to a data frame\ngff_data_frame <- as.data.frame(gff_data)\n\n# Display the first few rows of the data frame\nhead(gff_data_frame)\n\n\n\n\nShow the code\n\"ENSG00000164308\"\n\n## all gene_id start with ENSG\ntable(substr(gff_data_frame$gene_id,1,4))\n## all entries have a gene_id\ndim(gff_data_frame)\n## number of genes with positive vs negative strands\n\n\nto load version 75 of ensembl\n\n\nShow the code\nlibrary(biomaRt)\n\n# mart <- useMart(biomart = \"ENSEMBL_MART_ENSEMBL\", \n#                 host = \"https://dec2013.archive.ensembl.org\", # This host corresponds to Ensembl version 75\n#                 path = \"/biomart/martservice\")\nmart <- useMart(biomart = \"ENSEMBL_MART_ENSEMBL\", \n                host = \"https://feb2014.archive.ensembl.org\", # This host corresponds to Ensembl version 75\n                path = \"/biomart/martservice\")"
  },
  {
    "objectID": "post/2021-06-16-github-cheatsheet/index.html",
    "href": "post/2021-06-16-github-cheatsheet/index.html",
    "title": "Github Cheatsheet",
    "section": "",
    "text": "-- Remove the history from\nrm -rf .git\n\n-- recreate the repos from the current content only\ngit init\ngit add .\ngit commit -m \"Initial commit\"\n\n-- push to the github remote repos ensuring you overwrite history\ngit remote add origin https://github.com/<YOUR ACCOUNT>/<YOUR REPOS>.git\ngit push -u --force origin main \nNote: older repos may have master as the main branch\ngit push -u --force origin master"
  },
  {
    "objectID": "post/2021-06-16-github-cheatsheet/index.html#get-rid-of-ds_store-files",
    "href": "post/2021-06-16-github-cheatsheet/index.html#get-rid-of-ds_store-files",
    "title": "Github Cheatsheet",
    "section": "get rid of DS_Store files",
    "text": "get rid of DS_Store files\nfind . -name .DS_Store -print0 | xargs -0 git rm -f --ignore-unmatch\necho .DS_Store >> .gitignore\ngit add .gitignore\ngit commit -m '.DS_Store banished!'"
  },
  {
    "objectID": "post/2021-06-16-github-cheatsheet/index.html#amend-last-commit-message",
    "href": "post/2021-06-16-github-cheatsheet/index.html#amend-last-commit-message",
    "title": "Github Cheatsheet",
    "section": "amend last commit message",
    "text": "amend last commit message\nYou can change the most recent commit message using the git commit --amend command."
  },
  {
    "objectID": "post/2020-07-14-hands-on-training-plink/index.html",
    "href": "post/2020-07-14-hands-on-training-plink/index.html",
    "title": "Hands-On Training: plink",
    "section": "",
    "text": "LATEST VERSION IN https://bios25328.hakyimlab.org/post/2021/04/07/plink-tutorial/"
  },
  {
    "objectID": "post/2020-07-14-hands-on-training-plink/index.html#basic-commands",
    "href": "post/2020-07-14-hands-on-training-plink/index.html#basic-commands",
    "title": "Hands-On Training: plink",
    "section": "Basic Commands",
    "text": "Basic Commands\n\nhere’s a list of some of the basic commands you will learn about\nWe will go more in depth for each one below\n\n\n\n\nCommand\nFunction\n\n\n\n\n–file\nloads a file in ascii format\n\n\n–bfile\nloads a file in binary format\n\n\n–make-bed\nconverts an ascii file to binary\n\n\n–out\nspecifies what you want to name your output\n\n\n–freq\ngenerates minor allele frequencies\n\n\n\nA more detailed tutorial for GWAS analysis is here."
  },
  {
    "objectID": "post/2020-07-14-hands-on-training-plink/index.html#introduction",
    "href": "post/2020-07-14-hands-on-training-plink/index.html#introduction",
    "title": "Hands-On Training: plink",
    "section": "Introduction",
    "text": "Introduction\nWe will learn to run a GWAS using plink.\nThis tutorial follows a plink tutorial you can find here\n\nFirst, download plink into a new file /Users//demo/\n\n\n\nShow the code\nDEMO=\"/Users/<yourusername>/demo\"\n\n\n\nDownload the example data we will be using.\nNote that this data is not real and is only intended to be used as an example for using plink.\n\n\nWhenever running plink commands, you first have to indicate you’re using plink by typing the full path to it.\n\nTo avoid writing out the same path over and over again, we will define a variable that contains the path to plink.\n\n\n\nShow the code\nplink=\"Users/<username>/demo/plink\""
  },
  {
    "objectID": "post/2020-07-14-hands-on-training-plink/index.html#ped-and-map-files",
    "href": "post/2020-07-14-hands-on-training-plink/index.html#ped-and-map-files",
    "title": "Hands-On Training: plink",
    "section": "PED and MAP files",
    "text": "PED and MAP files\n\nPED and MAP files are plain text files that contain a table of part of your data\nTogether, they make up all of the data of the study you’re doing\nLook at the first few lines of your PED file by copy and pasting this command\n\n\n\nShow the code\ncut -d ' ' -f  1-6  hapmap1.ped | head\n\n\n\nEach row represents a different individual.\n\nPED files start with 6 informative columns about each individual.\n\n\n\nFamily ID\nIndividual ID\nPaternal ID\nMaternal ID\nSex\n\n1 = male, 2 = female, other = unknown\n\nPhenotype\n\nCan be quantitative or an affection status\nThe default values for affection status is: 1 = unaffected, 2 = affected, 0 = missing, -9 = missing\n\n\n\nEvery column after these is the data for the individual’s genotypes\n\nEach marker is biallelic, meaning there’s two genotypes for each column, paternal and maternal.\n\nNow look at the beginning of your MAP file\n\n\n\nShow the code\nhead hapmap1.map\n\n\n\nEach row represents a different SNP.\nthere are 4 informative columns.\n\n\nChromosome #\nSNP identifier\nGenetic distance (in centimorgans)\nBase-pair position\n\n\nevery row here corresponds to a genotype column in the ped file to indicate the position of those genotypes\n\nthe first row here corresponds to the first genotype column in the .ped (after the first ID-ing 6), the second row here to the next column there, and so on"
  },
  {
    "objectID": "post/2020-07-14-hands-on-training-plink/index.html#binary-files",
    "href": "post/2020-07-14-hands-on-training-plink/index.html#binary-files",
    "title": "Hands-On Training: plink",
    "section": "Binary Files",
    "text": "Binary Files\nIt’s helpful to compress your data to speed up analysis and data processing\nOne way to do this is to convert your files to binary format\n\nConvert your .ped and .map files to binary\n\n\n\nShow the code\nplink --file hapmap1 --make-bed --out hapmap1\n\n\nThese are the three plink commands we used:\n- –file hapmap1: specifies that we want to use the hapmap1 files\n- –make-bed: converts that file to binary\n- –out hapmap1: names our output files hapmap1\n- if you don’t specify a name for your output, it defaults to “plink” - If you look at your hapmap folder, you’ll see four new files - .bed: binary .ped - .bim: binary .map - .fam: copy of the first 6 column of .ped - .log: a log of what commands were run and what it printed"
  },
  {
    "objectID": "post/2020-07-14-hands-on-training-plink/index.html#minor-allele-frequencies",
    "href": "post/2020-07-14-hands-on-training-plink/index.html#minor-allele-frequencies",
    "title": "Hands-On Training: plink",
    "section": "Minor Allele Frequencies",
    "text": "Minor Allele Frequencies\n\nFind the minor allele frequencies of your data and look at the head\n\n\n\nShow the code\nplink --file hapmap1 --freq --out maf\nhead maf.freq\n\n\n\nThere are 6 columns\n\n\nChromosome #\nSNP ID\nMinor Allele\nMajor Allele\nMinor Allele Frequencies\nNumber of Allele Observations"
  },
  {
    "objectID": "post/2020-07-14-hands-on-training-plink/index.html#run-a-gwas",
    "href": "post/2020-07-14-hands-on-training-plink/index.html#run-a-gwas",
    "title": "Hands-On Training: plink",
    "section": "Run a GWAS",
    "text": "Run a GWAS\n\nWhen you “run a GWAS,” it just means you’re finding the association between your SNPs and the phenotypes.\nIn plink, you use the –assoc command to do this\nRun a GWAS and look at the head\n\n\n\nShow the code\nplink --file hapmap1 --assoc --out as\nhead as.assoc\n\n\n\nThere are 10 columns\n\n\nChromosome #\nSNP ID\nBase pair\nMinor Allele\nFrequency of the minor allele in affected individuals\nFrequency of the minor allele in unaffected idividuals\nMajor Allele\nChi-squared statistic\n\nThe difference between the data you observed and the data you expected under the null hypothesis\n\nP-value\nOdds ratio"
  },
  {
    "objectID": "post/2020-07-14-hands-on-training-plink/index.html#manhattan-plot",
    "href": "post/2020-07-14-hands-on-training-plink/index.html#manhattan-plot",
    "title": "Hands-On Training: plink",
    "section": "Manhattan Plot",
    "text": "Manhattan Plot\nNow we will graph a Manhattan plot of our association data We will do this in R using the “qqman” package - [ ] First, download qqman\n\n\nShow the code\nlibrary(qqman)\n\n\n\nRead your association table and set it to a variable\n\n\n\nShow the code\nmydata <- read_table(\"/Users/<username>/...\", header = TRUE)\n\n\n\nNow if you try to plot mydata, R will give you an error because there are NA values in it.\nTo get rid of the NA values, use the command na.omit\nTake out the NA values\n\n\n\nShow the code\nmydata2 <- na.omit(mydata)\n\n\n\nNow you’re ready to graph your manhattan plot!\n\n\n\nShow the code\nmanhattan(mydata2)"
  },
  {
    "objectID": "post/2020-07-14-hands-on-training-plink/index.html#qq-plot",
    "href": "post/2020-07-14-hands-on-training-plink/index.html#qq-plot",
    "title": "Hands-On Training: plink",
    "section": "QQ Plot",
    "text": "QQ Plot\nthe qq() function takes in a vector instead of a table.\nWhen graphing our data in qq plots we use the p-values - [ ] Graph your p values\n\n\nShow the code\nqqplot(mydata2$P)\n\n\nwe will read the hapmap1.ped file. typically genotype files are too large to load into memory but in this example it takes 28MB, which is fine with current computers. As a rule of thumb (in a laptop with 16Gb of ram, try not to load files that are more than 1GB)\n\n\nShow the code\nhapmap1_ped = read_table(\"/Users/lvairus/Desktop/hapmap1/hapmap1.ped\")"
  },
  {
    "objectID": "post/2021-07-19-get-list-of-predictor-snps-and-weights-in-predictdb/index.html",
    "href": "post/2021-07-19-get-list-of-predictor-snps-and-weights-in-predictdb/index.html",
    "title": "Get list of predictor SNPs and weights in predictdb",
    "section": "",
    "text": "To get a list of SNPs and the corresponding weights to predict expression levels (or splicing) of a given gene, you will first need to download the databases where the prediction models are stored. For example, you can download them from here more specifically from this tar file\nOn CRI they are located in /gpfs/data/im-lab/nas40t2/Data/PredictDB/GTEx_v8/models_v1/eqtl/mashr/\nHere I will mount the drive to my local machine following these instructions\nlibrary(tidyverse)\n## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n## ✓ ggplot2 3.3.3     ✓ purrr   0.3.4\n## ✓ tibble  3.1.2     ✓ dplyr   1.0.6\n## ✓ tidyr   1.1.3     ✓ stringr 1.4.0\n## ✓ readr   1.4.0     ✓ forcats 0.5.1\n\n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## x dplyr::filter() masks stats::filter()\n## x dplyr::lag()    masks stats::lag()\nmodel_dir = \"/Volumes/im-lab/nas40t2/Data/PredictDB/GTEx_v8/models_v1/eqtl/mashr\"\nGet ensemblid for the gene. For example GSDMA’s ensid is ENSG00000167914\n## install.packages(\"RSQLite\")\nlibrary(\"RSQLite\")\nsqlite <- dbDriver(\"SQLite\")\ndf = data.frame()\ndbnamelist= list.files(model_dir,pattern = \"*.db\")\nfor(dbname in dbnamelist)\n{\n  print(\"--\")\n  print(dbname)\n  ## connect to db\n  db = dbConnect(sqlite,glue::glue(\"{model_dir}/{dbname}\"))\n  ## list tables\n  tempo <- dbGetQuery(db,\"select * from weights where gene like 'ENSG00000167914%'\") ## % is wildcard, to avoid dealing with ENSG version number\n  if(nrow(tempo)>0) \n  {\n    tempo$tissue <- gsub(\"mashr_\",\"\",gsub(\".db\",\"\",dbname))\n    df = rbind(df,tempo)\n  }\n}\n## [1] \"--\"\n## [1] \"mashr_Adipose_Subcutaneous.db\"\n## [1] \"--\"\n## [1] \"mashr_Adipose_Visceral_Omentum.db\"\n## [1] \"--\"\n## [1] \"mashr_Adrenal_Gland.db\"\n## [1] \"--\"\n## [1] \"mashr_Artery_Aorta.db\"\n## [1] \"--\"\n## [1] \"mashr_Artery_Coronary.db\"\n## [1] \"--\"\n## [1] \"mashr_Artery_Tibial.db\"\n## [1] \"--\"\n## [1] \"mashr_Brain_Amygdala.db\"\n## [1] \"--\"\n## [1] \"mashr_Brain_Anterior_cingulate_cortex_BA24.db\"\n## [1] \"--\"\n## [1] \"mashr_Brain_Caudate_basal_ganglia.db\"\n## [1] \"--\"\n## [1] \"mashr_Brain_Cerebellar_Hemisphere.db\"\n## [1] \"--\"\n## [1] \"mashr_Brain_Cerebellum.db\"\n## [1] \"--\"\n## [1] \"mashr_Brain_Cortex.db\"\n## [1] \"--\"\n## [1] \"mashr_Brain_Frontal_Cortex_BA9.db\"\n## [1] \"--\"\n## [1] \"mashr_Brain_Hippocampus.db\"\n## [1] \"--\"\n## [1] \"mashr_Brain_Hypothalamus.db\"\n## [1] \"--\"\n## [1] \"mashr_Brain_Nucleus_accumbens_basal_ganglia.db\"\n## [1] \"--\"\n## [1] \"mashr_Brain_Putamen_basal_ganglia.db\"\n## [1] \"--\"\n## [1] \"mashr_Brain_Spinal_cord_cervical_c-1.db\"\n## [1] \"--\"\n## [1] \"mashr_Brain_Substantia_nigra.db\"\n## [1] \"--\"\n## [1] \"mashr_Breast_Mammary_Tissue.db\"\n## [1] \"--\"\n## [1] \"mashr_Cells_Cultured_fibroblasts.db\"\n## [1] \"--\"\n## [1] \"mashr_Cells_EBV-transformed_lymphocytes.db\"\n## [1] \"--\"\n## [1] \"mashr_Colon_Sigmoid.db\"\n## [1] \"--\"\n## [1] \"mashr_Colon_Transverse.db\"\n## [1] \"--\"\n## [1] \"mashr_Esophagus_Gastroesophageal_Junction.db\"\n## [1] \"--\"\n## [1] \"mashr_Esophagus_Mucosa.db\"\n## [1] \"--\"\n## [1] \"mashr_Esophagus_Muscularis.db\"\n## [1] \"--\"\n## [1] \"mashr_Heart_Atrial_Appendage.db\"\n## [1] \"--\"\n## [1] \"mashr_Heart_Left_Ventricle.db\"\n## [1] \"--\"\n## [1] \"mashr_Kidney_Cortex.db\"\n## [1] \"--\"\n## [1] \"mashr_Liver.db\"\n## [1] \"--\"\n## [1] \"mashr_Lung.db\"\n## [1] \"--\"\n## [1] \"mashr_Minor_Salivary_Gland.db\"\n## [1] \"--\"\n## [1] \"mashr_Muscle_Skeletal.db\"\n## [1] \"--\"\n## [1] \"mashr_Nerve_Tibial.db\"\n## [1] \"--\"\n## [1] \"mashr_Ovary.db\"\n## [1] \"--\"\n## [1] \"mashr_Pancreas.db\"\n## [1] \"--\"\n## [1] \"mashr_Pituitary.db\"\n## [1] \"--\"\n## [1] \"mashr_Prostate.db\"\n## [1] \"--\"\n## [1] \"mashr_Skin_Not_Sun_Exposed_Suprapubic.db\"\n## [1] \"--\"\n## [1] \"mashr_Skin_Sun_Exposed_Lower_leg.db\"\n## [1] \"--\"\n## [1] \"mashr_Small_Intestine_Terminal_Ileum.db\"\n## [1] \"--\"\n## [1] \"mashr_Spleen.db\"\n## [1] \"--\"\n## [1] \"mashr_Stomach.db\"\n## [1] \"--\"\n## [1] \"mashr_Testis.db\"\n## [1] \"--\"\n## [1] \"mashr_Thyroid.db\"\n## [1] \"--\"\n## [1] \"mashr_Uterus.db\"\n## [1] \"--\"\n## [1] \"mashr_Vagina.db\"\n## [1] \"--\"\n## [1] \"mashr_Whole_Blood.db\"\ndf %>% count(rsid,varID) %>% arrange(desc( n))\n##                      rsid                   varID  n\n## 1               rs3859191  chr17_39972461_G_A_b38 17\n## 2              rs28618095  chr17_39952822_T_C_b38 14\n## 3               rs4794821  chr17_39967950_T_C_b38  5\n## 4               rs4458030  chr17_39965453_G_A_b38  4\n## 5               rs4065876  chr17_39973253_G_A_b38  3\n## 6  chr17_39954836_G_C_b38  chr17_39954836_G_C_b38  2\n## 7               rs3916061  chr17_39971460_A_G_b38  2\n## 8              rs56326707  chr17_39973886_C_T_b38  2\n## 9              rs59269632  chr17_39969978_A_G_b38  2\n## 10             rs60667221  chr17_39954137_T_A_b38  2\n## 11               rs921651  chr17_39977669_G_A_b38  2\n## 12            rs113277605  chr17_40000142_C_T_b38  1\n## 13              rs3859192  chr17_39972395_C_T_b38  1\n## 14            rs397713502 chr17_39972331_A_AG_b38  1\n## 15            rs398100509 chr17_39966991_AC_A_b38  1\n## 16              rs4239225  chr17_39970859_G_A_b38  1\n## 17             rs56946324  chr17_39951713_C_A_b38  1\n## 18             rs60725845  chr17_39968377_T_G_b38  1\n## 19              rs7214085  chr17_40012842_T_C_b38  1\n## 20              rs7221814  chr17_39933464_A_G_b38  1\n## 21             rs72832971  chr17_39952366_C_T_b38  1\n## 22            rs796403983 chr17_39972785_AG_A_b38  1\n## 23              rs8077456  chr17_39972512_G_C_b38  1\nwrite_csv(df,\"~/Downloads/GSDMA-weights.csv\")\nAlso checkout how to query sqlite database this post"
  },
  {
    "objectID": "post/2023-04-07-inflation-brainxcan/index.html",
    "href": "post/2023-04-07-inflation-brainxcan/index.html",
    "title": "Inflation Brainxcan",
    "section": "",
    "text": "Summary\n\n\n\nWhen both \\(Y\\) and a mediator IDP are polygenic, the regression test \\(Y\\) on IDP is inflated with \\(\\text{Var}(Z_\\text{bxcan}) = 1 + N h_1^2 \\cdot \\frac{\\text{tr}(R'R)}{\\text{tr}^2(R)}\\). I’ll use simulations to understand this relationship as functions of \\(h_Y\\), \\(h_\\text{IDP}\\), \\(N\\), \\(M_Y\\), and \\(M_\\text{IDP}\\).\n\n\n\n\\(h_Y\\) is the heritability of \\(Y\\)\n\\(h_\\text{IDP}\\)\n\\(N\\), is the sample size\n\\(M_Y\\) is the number of causal variants for Y\n\\(M_\\text{IDP}\\) is the number of causal variants for the IDP\n\n\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\n\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\nSLUG=\"inflation-brainxcan\" ## copy the slug from the header\nbDATE='2023-04-07' ## copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n##system(glue(\"open {DATA}\")) ## this will open the folder \n\n\nnsam = 10000\nnsim = 100\nmsnp = 1000\n\nh2Y = 0.5\nh2IDP = 0.4\n\n\nsimulate Xk\n\n\nXmat = matrix(rbinom(nsam*msnp, 2, 0.4), nsam, msnp)\n\n\nsimulate polygenic Y (nsam x nsim)\n\n\\[\\begin{align}\nY &= \\beta \\cdot \\text{IDP} + \\sum_k X_k \\cdot b_k + \\epsilon\n\\end{align}\\]\n\n## betamat msnp x 1 \nbetamat = matrix(rnorm(msnp),msnp, 1)\nepsimat = matrix(rnorm(nsam),nsam, 1)\nepsimat = scale(epsimat) * sqrt(1 - h2Y)\ngYmat = Xmat %*% betamat\ngYmat = scale(gYmat) * sqrt(h2Y)\nYmat = gYmat + epsimat\n\n\nsimulate polygenic IDP (nsam x nsim)\n\n\\[\\begin{align}\n\\text{IDP} &= \\sum_k \\gamma_k \\cdot X_k + \\epsilon',\n\\end{align}\\]\n\ngammamat = matrix(rnorm(msnp * nsim),msnp, nsim)\nepsimat2 = matrix(rnorm(nsam * nsim),nsam, nsim)\n## epsimat2 = scale(epsimat2) * sqrt(1 - h2IDP)\ngIDPmat = Xmat %*% gammamat\n## gIDPmat = scale(gIDPmat) * sqrt(h2ID) ## %%HERE fix this scaling, allow multiple h2 for IDPs?\nIDPmat = gIDPmat + epsimat2\n\n\ncalculate p-value of regression, Y ~ IDP\n\n\nsuppressMessages(devtools::source_gist(\"115403f16bec0a0e871f3616d552ce9b\") ) ## load fn_ratxcan, fast regression and other convenience functions to correlate subsets of columns of two matrices\nsuppressMessages(devtools::source_gist(\"38431b74c6c0bf90c12f\") ) ## load qqunif\n\n## run fast_predixcan_assoc\nidnum=1:nsam\nidvec = glue(\"id-{idnum}\")\nres <- fast_predixcan_assoc(data.frame(IID=idvec,IDPmat), data.frame(IID=idvec,Ymat), idlist=idvec)\n\n[1] 10000   100\n[1] 10000     1\nA sample size of 10000 was used\n\nqqunif(res$pval)\n\n\n\n\n\ncalculate pvec of cor(Y, IDP): this is the same as running linear regression\n\n\n# > summary(lm(Ymat~ IDPmat[,32]))\n# \n# Call:\n# lm(formula = Ymat ~ IDPmat[, 32])\n# \n# Residuals:\n#      Min       1Q   Median       3Q      Max \n# -201.069  -46.406   -0.164   46.472  205.379 \n# \n# Coefficients:\n#               Estimate Std. Error t value Pr(>|t|)    \n# (Intercept)  -37.14480    4.61728  -8.045 2.44e-15 ***\n# IDPmat[, 32]  -0.08780    0.03051  -2.878  0.00409 ** \n# ---\n# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n# \n# Residual standard error: 68.63 on 998 degrees of freedom\n# Multiple R-squared:  0.008231,    Adjusted R-squared:  0.007237 \n# F-statistic: 8.282 on 1 and 998 DF,  p-value: 0.004088\n# \n# > cor.test(Ymat, IDPmat[,32])\n# \n#   Pearson's product-moment correlation\n# \n# data:  Ymat and IDPmat[, 32]\n# t = -2.8779, df = 998, p-value = 0.004088\n# alternative hypothesis: true correlation is not equal to 0\n# 95 percent confidence interval:\n#  -0.15186241 -0.02889285\n# sample estimates:\n#         cor \n# -0.09072344 \n\n\ncalculate pvec of cor(sum bk Xk, sum gammak Xk)\n\n\n## run fast_predixcan_assoc\nidnum=1:nsam\nidvec = glue(\"id-{idnum}\")\ngres <- fast_predixcan_assoc(data.frame(IID=idvec,gIDPmat), data.frame(IID=idvec,gYmat), idlist=idvec)\n\n[1] 10000   100\n[1] 10000     1\nA sample size of 10000 was used\n\nqqunif(gres$pval)\n\n\n\n\n\ncheck FDR of BrainXcan schizophrenia associations\n\n\n## google sheets prepared by Yanyu for revision 4/7/2023\ns2 <- read_csv(glue(\"{DATA}/Table_S2.xlsx - ..csv\"))\n\nRows: 459 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): IDP, modality, subtype, pc1_name, region, side, measurement_type, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ns8 <- read_csv(glue(\"{DATA}/Table_S8-w-factor.xlsx - Table_S8-w-factor.csv\"))\n\nRows: 16380 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): model, IDP, phenotype\ndbl (7): bhat, pval, zscore, nsnp_used, nsnp_total, z_adj_perm_null, pval_ad...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntempo <- s8 %>% left_join(s2 %>% select(IDP,modality,notes,subtype),by=c(\"IDP\")) %>% filter(model==\"ridge\",phenotype==\"SCZ_PGC_2020\") %>% filter(substr(subtype, 1, 2) != 'w-' | is.na(subtype) ) %>% filter(!grepl(\"ProbTrack-1\",IDP))\n\n## qq <- tempo %>% filter(modality==\"dMRI\") %>% .[[\"pval_adj_perm_null\"]] %>% qvalue::qvalue()\n\nqq <- tempo  %>% .[[\"pval_adj_perm_null\"]] %>% qvalue::qvalue()\n\n\nread hapmap file\n\n\n# if (!require(\"BiocManager\", quietly = TRUE))\n#     install.packages(\"BiocManager\")\n# BiocManager::install(\"snpStats\")\nsuppressMessages(library(snpStats))\n# Set the path to the binary files (without file extensions)\nplink_file_path <- \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/Reference-Data/GWAS-tutorial-Marees/data_1_QC_GWAS/HapMap_3_r3_1\"\n# Read the binary files into an object of class \"snpMatrix\"\nsnp_matrix <- read.plink(plink_file_path)"
  },
  {
    "objectID": "post/2020-07-30-how-to-prepare-a-post-implementation-report/index.html",
    "href": "post/2020-07-30-how-to-prepare-a-post-implementation-report/index.html",
    "title": "How to Prepare a Post Implementation Report?",
    "section": "",
    "text": "This is a guide on how to prepare a Post-Implementation Report for cloud credits, for future reference. This guide is based on the one prepared for the deadline of February 28th 2019.\nA Post-Implementation Report (PIR) provides data to compare the Cloud Credits model to traditional mechanisms of funding computation-based scientific research. Comparisons based on information collectively derived from PIRs include cost-benefit, scalability, research productivity, and the shared collection of valuable reusable digital objects."
  },
  {
    "objectID": "post/2020-07-30-how-to-prepare-a-post-implementation-report/index.html#procedure",
    "href": "post/2020-07-30-how-to-prepare-a-post-implementation-report/index.html#procedure",
    "title": "How to Prepare a Post Implementation Report?",
    "section": "Procedure",
    "text": "Procedure\nYou are required to provide information in this PIR only on digital objects that you pledged to share in your credit request application(s). However, should you wish to share additional digital objects not originally described in your credit application but deemed valuable to the research community, please do so. PIR are submitted online via the Commons Credits Portal.\n\nTypes of Digital Objects\n\nData: Digital data contains quantitative or qualitative facts, such as numbers, words, measurements, observations, or descriptions of entities.\nApplication (or tool): A digital application has features, functions, and interfaces that constrain it to a need, context, or purpose, making it valuable. A digital tool performs a set of functions that are generally useful but not tied to any circumstance, problem, or approach. Tools typically have widespread utility.\nWorkflow (or pipeline): A digital workflow is a series of activities that are necessary to complete a task. Each step in a workflow has a specific step before it (except for the first step) and a specific step after it (except for the last step).\n\nIf you have a collection of digital objects with similar characteristics, they can be grouped into a single object of type “set”.\n\n\nWhere to store the object\nA digital object must be made available in a repository. It can be a domain-specific repository (e.g. ), a general public repository such as Figshare, a institutional repository or the native cloud environment where the object was generated.\n\n\nIndexing the object\nThe digital object must be indexed. To do this, its metadata must be submitted through CEDAR (see instructions here) Some remarks when filling in the metadata form: - Google Cloud Storage’s URI (i.e. gs://bucket/...) can be used as a digital object identifier. If uploading to Figshare, a DOI can be requested from the repository. - The fields “distribution ID” and “ID” might differ: for example, if a same object was published in different repositories, the field “distribution ID” would account for the particular. - The link to a landing page explaining the content of the object must be provided. It could be, for example, an html file uploaded to the same folder where the object is stored. - If the paper associated to the object is not published yet, the field “Title” can be filled in with “Pending”.\nWhen the form is ready to submit, set the “Ready to Index” field to True. This will tell bioCADDIE that it can start indexing the object. After a suitable time was passed (indexing can take up to two days), search the Datamed web site for your object to verify its metadata was successfully added."
  },
  {
    "objectID": "post/2021-07-26-tracking-the-traffic-to-webpages/index.html",
    "href": "post/2021-07-26-tracking-the-traffic-to-webpages/index.html",
    "title": "Tracking the traffic to webpages",
    "section": "",
    "text": "To keep track of traffic to blogdown (hugo-based) webpages in google analytics, follow the steps below\nSee Google’s instructions here. Briefly\n\nSign in to your Analytics account.\nClick Admin.\nSelect an account from the menu in the ACCOUNT column.\nSelect a property from the menu in the PROPERTY column.\nUnder PROPERTY, click Tracking Info > Tracking Code.Your Tracking ID is displayed at the top of the page.\n\nCopy the global site tag, which is several lines of code that you need to paste into each webpage you want to measure. Below is the code for tracking predictdb.org\n<!-- Global site tag (gtag.js) - Google Analytics -->\n<script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-61894206-2\"></script>\n<script>\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){dataLayer.push(arguments);}\n  gtag('js', new Date());\n\n  gtag('config', 'UA-61894206-2');\n</script>\n\n\nCopy the code above to the site’s layout head_custom.html file. For example for predictdb website, the file is found in https://github.com/hakyimlab/web-predictdb/blob/main/layouts/partials/head_custom.html.\nPaste it immediately after the <head> tag on each page of your site."
  },
  {
    "objectID": "post/2020-07-30-computing-on-anl-servers/index.html",
    "href": "post/2020-07-30-computing-on-anl-servers/index.html",
    "title": "Computing on ANL Servers",
    "section": "",
    "text": "To get access to the servers at ANL, you will need at minimum an account with Argonne’s MCS division, as well as access to specific computing groups. This process is usually handled by one of the researchers at ANL, as they need to serve as a sponsor for external users."
  },
  {
    "objectID": "post/2020-07-30-computing-on-anl-servers/index.html#washington",
    "href": "post/2020-07-30-computing-on-anl-servers/index.html#washington",
    "title": "Computing on ANL Servers",
    "section": "Washington",
    "text": "Washington\n16 CPUs, 4 GPUs (being rebuilt, currently not accessible), 1T RAM"
  },
  {
    "objectID": "post/2020-07-30-computing-on-anl-servers/index.html#nucleus",
    "href": "post/2020-07-30-computing-on-anl-servers/index.html#nucleus",
    "title": "Computing on ANL Servers",
    "section": "Nucleus",
    "text": "Nucleus\n40 CPUs, 4 GPUs, 250G RAM"
  },
  {
    "objectID": "post/2020-12-21-psychencode-models/index.html",
    "href": "post/2020-12-21-psychencode-models/index.html",
    "title": "PsychENCODE Models",
    "section": "",
    "text": "Gandal et al analyzed autism spectrum disorder, schizophrenia, and bipolar disorder across multiple levels of transcriptomic organization—gene expression, local splicing, transcript isoform expression, and coexpression networks for both protein-coding and noncoding genes to produce a quantitative, genome-wide resource. They performed TWAS based on 2,188 postmortem frontal and temporal cerebral cortex samples from 1,695 adults. RNA-sequencing reads were aligned to the GRCh37.p13 (hg19) reference genome."
  },
  {
    "objectID": "post/2020-12-21-psychencode-models/index.html#gene-expression-level-prediction-models",
    "href": "post/2020-12-21-psychencode-models/index.html#gene-expression-level-prediction-models",
    "title": "PsychENCODE Models",
    "section": "Gene expression level prediction models",
    "text": "Gene expression level prediction models\nWe extracted the elastic-net weights released provided Gandal et al. who chose (as prescribed by FUSION) the model that maximized the prediction performance among elastic net, BSLMM, lasso, top eQTL, and BLUP (best linear unbiased prediction, similar to ridge regression). Our extensive analysis had shown that this was not necessary since the very sparse architecture of gene expression traits were optimally predicted using elastic net in this paper. We further confirmed our assersion by comparing the different prediction model performance as provided by Gandal et al as shown in the figure below: the last column compares elastic net to all the other models and most points are either on or below the identity line.\n\nMore info on the study: https://science.sciencemag.org/content/362/6420/eaat8127. Their results were shared at http://resource.psychencode.org. Gandal et al’s transcriptome prediction models can be downloaded with this link PsychENCODE weights data link.\nWe used the weights they generated from the elastic net method and reformated it into a model compatible with PrediXcan software (code). We also calculated covariances between variants (code). The models can be downloaded here.\nWe validated the model by running PrediXcan on 1000G genotypes PsychENCODE, GTEx v7 Brain Cortex, and GTEx v7 Whole Blood tissue models, and then comparing the correlation between predicted gene expression and observed expression from GEUVADIS. The results can be found here.\nNext, we compared PsychENCODE S-PrediXcan association results with GTEx v7 Brain Cortex and Whole Blood tissue models from the Walters Group Schizophrenia GWAS. The results can be found here.\nThe original PsychENCODE model was defined in hg19, but we also lifted it over to hg38. We validated it by comparing S-PrediXcan associations results from the original hg19, lifted over hg38, and GTEx v8 mashr Brain Cortex models on the Schizophrenia GWAS. The steps to lift over the model and the validation results are here."
  },
  {
    "objectID": "post/2021-07-09-training-gene-expression-prediction-models/index.html",
    "href": "post/2021-07-09-training-gene-expression-prediction-models/index.html",
    "title": "Training Gene Expression Prediction Models",
    "section": "",
    "text": "PrediXcan and TWAS methods in general correlate genetically predicted levels of gene expression traits with complex traits to understand the mechanism behind GWAS loci. A key component is the training of gene expression traits. A tutorial on how to generate elastic net models can be found in this link\nElastic net is a good all purpose prediction approach for complex traits and has been shown to perform well for gene expression traits. Depending on your goals, you may want to use a different approach. For example, if the goal is to maximize the reliability (low false positive) of putatively causal genes, then we showed that a method that uses genetic variants more likely to be causal may work better. Explained in this paper. In the GTEx GWAS subgroup we chose the models that are based on fine-mapping, called mashr-based."
  },
  {
    "objectID": "post/2021-08-12-japan-biobank-data-summary/index.html",
    "href": "post/2021-08-12-japan-biobank-data-summary/index.html",
    "title": "Japan Biobank Data Summary",
    "section": "",
    "text": "We have access to dataset JGAS000114 from the Biobank Japan Project. This includes:\n\ngenotype data for 182,505 individuals (csv.gz)\nWGS (hg19) for 1,026 individuals (fastq, bam, vcf)\nreference panel (hg19) from BBJ and 1KGP WGS data (vcf.gz)\nphenotype data for 58 quantitative traits from 162,255 individuals (txt.gz)\n\nSee list of phenotypes and more details here"
  },
  {
    "objectID": "post/2020-07-30-how-to-use-aws/index.html",
    "href": "post/2020-07-30-how-to-use-aws/index.html",
    "title": "How to Use AWS",
    "section": "",
    "text": "Stop/start your EC2 instances programatically through AWS Command Line Interface (CLI).\nStop your EC2 instances to avoid incurring charges when not in use.\nYour secret key must be kept very secret. Please store it in a very safe place and don’t include it in your code."
  },
  {
    "objectID": "post/2020-07-30-how-to-use-aws/index.html#how-to-login-to-your-aws-account",
    "href": "post/2020-07-30-how-to-use-aws/index.html#how-to-login-to-your-aws-account",
    "title": "How to Use AWS",
    "section": "How to login to your AWS account?",
    "text": "How to login to your AWS account?\nyou should have gotten the credentials from the lab’s cloud administrator https://hakyimlab.signin.aws.amazon.com/console\nOnce you login to your account using temporary password, you can reset your password by the following policy: * Minimum password length (6 to 128). * Require at least one uppercase letter (A to Z). * Require at least one lowercase letter (a to z). * Require at least one number (0 to 9). * Require at least one nonalphanumeric character (! @ # $ % ^ & * ( ) _ + - = [ ] { } | ’)."
  },
  {
    "objectID": "post/2020-07-30-how-to-use-aws/index.html#how-to-manage-aws-access-and-secret-keys",
    "href": "post/2020-07-30-how-to-use-aws/index.html#how-to-manage-aws-access-and-secret-keys",
    "title": "How to Use AWS",
    "section": "How to manage AWS access and secret keys?",
    "text": "How to manage AWS access and secret keys?\n\nPlease store your access key ID (something like AKIAIOSFODNN7EXAMPLE) and a secret access key (something like wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY) in a very safe place.\nAnyone who has your credentials has the same level of access to your AWS resources (e.g. EC2, S3, RDS etc) that you do. So they must be kept very secret. Please never share them with others or include them in your code."
  },
  {
    "objectID": "post/2020-07-30-how-to-use-aws/index.html#how-to-install-and-upgrade-the-aws-cli",
    "href": "post/2020-07-30-how-to-use-aws/index.html#how-to-install-and-upgrade-the-aws-cli",
    "title": "How to Use AWS",
    "section": "How to install and upgrade the AWS CLI?",
    "text": "How to install and upgrade the AWS CLI?\nsudo easy_install pip\nsudo pip install awscli --ignore-installed six\nsudo pip install --upgrade awscli"
  },
  {
    "objectID": "post/2020-07-30-how-to-use-aws/index.html#how-to-configure-the-aws-cli",
    "href": "post/2020-07-30-how-to-use-aws/index.html#how-to-configure-the-aws-cli",
    "title": "How to Use AWS",
    "section": "How to configure the AWS CLI?",
    "text": "How to configure the AWS CLI?\naws configure\nAWS Access Key ID [None]: <YOUR AWS ACCESS KEY>\nAWS Secret Access Key [None]: <YOUR SECRET ACCESS KEY>\nDefault region name [None]: us-east-1\nDefault output format [None]: json"
  },
  {
    "objectID": "post/2020-07-30-how-to-use-aws/index.html#how-to-launch-your-ec2-instances-through-aws-console",
    "href": "post/2020-07-30-how-to-use-aws/index.html#how-to-launch-your-ec2-instances-through-aws-console",
    "title": "How to Use AWS",
    "section": "How to launch your EC2 instances through AWS console?",
    "text": "How to launch your EC2 instances through AWS console?\nAmazon hosts its resources under different regions, after you login, make sure you are in ‘N. Virginia’ region, that’s where our virtual private clouds are setup.\n\nCreate ssh key pair\n\nIn order to access EC2 resources, you should first load your ssh public key onto amazon, so that VMs will be property configured when you try to launch them. - Click’Services’ on the top nav bar,selectEC2. - Select “NETWORK&SECURITY” -> Key Pairs on the left panel - Click”create key pair” if you want to create new key pair, or load your existing one. Please name your key pair under your username\n\nLaunch a VM under public VPC\nSelect”Instances”->“LaunchInstance” in EC2 dashboard\nChoose image you want to use\nChose an instance type based on your computing needs\nConfigure instance details with those choices, use the default ones if it’s not mentioned below:\n\nNetwork: public Subnet: public subnet\nAuto-assign Public IP: enable\n\nClick “Review and Launch” ___ Security Groups -> select an existing security group -> select ‘default’ ___ Tags -> create tag for your instance name: eg: Key: Name, Value: abc\nLaunch with your ssh keypair\nLaunch a VM under private VPC\n\nIf you are going to use protected data, launch VMs under the private VPC. VMs launched under this VPC are under private network and do not have public access. This VPC has a public VM called “Login Node”. You should ssh onto that VM in order to reach VMs under the private network.\n\nContact administrator to grant you access to login node\nLauncha VM using the same instruction as for the public VPC, except for choosing those options when configuring instance details:\n\nNetwork: main\nSubnet: private\nAuto-assign Public IP: Use subnet setting (Disable) security groups: select ‘local’\n\nAfter the VM is running, ssh onto login node,then ssh on to your VM to access it"
  },
  {
    "objectID": "post/2020-07-30-how-to-use-aws/index.html#how-to-launch-your-ec2-instance-using-cloudformation-through-cli",
    "href": "post/2020-07-30-how-to-use-aws/index.html#how-to-launch-your-ec2-instance-using-cloudformation-through-cli",
    "title": "How to Use AWS",
    "section": "How to launch your EC2 instance using Cloudformation through CLI?",
    "text": "How to launch your EC2 instance using Cloudformation through CLI?\n-Download and modify the template here @https://s3.amazonaws.com/imlab-jiamaoz/LaunchEC2InstanceUsingCloudformationTemplate.json.\n\nUpload your modified template to your bucket in s3\nRun the following command in the terminal:\n\naws cloudformation update-stack --stack-name <YOUR STACK NAME> --template-url  https://s3.amazonaws.com/<YOUR BUCKET NAME>/LaunchEC2InstanceUsingCloudformationTemplate.json"
  },
  {
    "objectID": "post/2020-07-30-how-to-use-aws/index.html#how-to-associate-an-elastic-ip-address-to-your-ec2-instance",
    "href": "post/2020-07-30-how-to-use-aws/index.html#how-to-associate-an-elastic-ip-address-to-your-ec2-instance",
    "title": "How to Use AWS",
    "section": "How to associate an Elastic IP address to your EC2 instance",
    "text": "How to associate an Elastic IP address to your EC2 instance\naws ec2 associate-address --instance-id <YOUR INSTANCE ID> --public-ip <YOUR ELASTIC IP ADDRESS>"
  },
  {
    "objectID": "post/2020-07-30-how-to-use-aws/index.html#how-to-peer-aws-vpc",
    "href": "post/2020-07-30-how-to-use-aws/index.html#how-to-peer-aws-vpc",
    "title": "How to Use AWS",
    "section": "How to peer AWS VPC?",
    "text": "How to peer AWS VPC?\nPlease follow steps from An introduction to AWS VPC Peering"
  },
  {
    "objectID": "post/2020-07-30-how-to-use-aws/index.html#how-to-get-your-amazon-ec2-instance-id-using-aws-cli",
    "href": "post/2020-07-30-how-to-use-aws/index.html#how-to-get-your-amazon-ec2-instance-id-using-aws-cli",
    "title": "How to Use AWS",
    "section": "How to get your Amazon EC2 instance id using AWS CLI?",
    "text": "How to get your Amazon EC2 instance id using AWS CLI?\naws ec2 describe-instances --filters \"Name=tag:Name, Values=<YOUR INSTANCE TAG NAME>\" --query 'Reservations[0].Instances[0].InstanceId'"
  },
  {
    "objectID": "post/2020-07-30-how-to-use-aws/index.html#how-to-stop-an-ec2-instance-using-aws-cli",
    "href": "post/2020-07-30-how-to-use-aws/index.html#how-to-stop-an-ec2-instance-using-aws-cli",
    "title": "How to Use AWS",
    "section": "How to stop an EC2 instance using AWS CLI?",
    "text": "How to stop an EC2 instance using AWS CLI?\naws ec2 stop-instances --instance-ids <YOUR INSTANCE ID> --query 'StoppingInstances[0].CurrentState.Name'"
  },
  {
    "objectID": "post/2020-07-30-how-to-use-aws/index.html#how-to-start-an-ec2-instance-using-aws-cli",
    "href": "post/2020-07-30-how-to-use-aws/index.html#how-to-start-an-ec2-instance-using-aws-cli",
    "title": "How to Use AWS",
    "section": "How to start an EC2 instance using AWS CLI?",
    "text": "How to start an EC2 instance using AWS CLI?\naws ec2 start-instances --instance-ids <YOUR INSTANCE ID> --query 'StartingInstances[0].CurrentState.Name'"
  },
  {
    "objectID": "post/2020-07-30-how-to-use-aws/index.html#how-to-get-the-ip-address-of-the-running-instance-using-aws-cli",
    "href": "post/2020-07-30-how-to-use-aws/index.html#how-to-get-the-ip-address-of-the-running-instance-using-aws-cli",
    "title": "How to Use AWS",
    "section": "How to get the IP address of the running instance using AWS CLI?",
    "text": "How to get the IP address of the running instance using AWS CLI?\naws ec2 describe-instances --instance-ids <YOUR INSTANCE ID> --query 'Reservations[0].Instances[0].PublicIpAddress'"
  },
  {
    "objectID": "post/2020-07-30-how-to-use-aws/index.html#how-to-automatically-stop-the-instance",
    "href": "post/2020-07-30-how-to-use-aws/index.html#how-to-automatically-stop-the-instance",
    "title": "How to Use AWS",
    "section": "How to automatically stop the instance?",
    "text": "How to automatically stop the instance?\nIt’s highly recommended to create the CloudWatch alarm to stop your instance automatically by default.\naws cloudwatch put-metric-alarm --alarm-name XXXXXX-stop-alarm --alarm-description \"Stop the instance when it is idle for one hour\" --namespace \"AWS/EC2\" --dimensions Name=InstanceId,Value=\"i-XXXXXX\" --statistic Maximum --metric-name CPUUtilization --comparison-operator LessThanThreshold --threshold 5 --period 300 --evaluation-periods 12 --alarm-actions arn:aws:automate:us-east-1:ec2:stop --unit Percent\nAbove put-metric-alarm command should generate a good CloudWatch alarm that will automatically stop the instance if the maxium CPU utilization of your instance is less than 5% over a hour period. Please replace the values for InstanceID and alarm name with your own values.\nIf you would like to receive email notification, please use the following put-metric-alarm command which includes your SNS topic arn:aws:sns:us-east-1:215500445195:<YOUR ALARM NAME>. Please refer How to set Up Amazon Simple Notification Service? to set up your SNS topic.\naws cloudwatch put-metric-alarm --alarm-name XXXXXX-stop-alarm --alarm-description \"Stop the instance when it is idle for one hour\" --namespace \"AWS/EC2\" --dimensions Name=InstanceId,Value=\"i-XXXXXX\" --statistic Maximum --metric-name CPUUtilization --comparison-operator LessThanThreshold --threshold 5 --period 300 --evaluation-periods 12 --alarm-actions arn:aws:automate:us-east-1:ec2:stop arn:aws:sns:us-east-1:215500445195:<YOUR ALARM NAME> --unit Percent\nThere are three scenarios for stopped instance’s CloudWatch alarm .\n\nNo Data alarm status. If you restart your instance, CloudWatch will work as expected.\nALARM alarm status. If you restart your instance AND do some computational work, CloudWatch will work as expected. You will notice that ALARM alarm status will disappear and OK will appear in a few minutes.\nALARM alarm status. If you restart your instance AND don’t do any computational work, CloudWatch will NOT work as expected. The instance will not automatically stop because the instance alarm status is still ALARM, and CloudWatch alarm won’t be triggered without any alarm status changes.\n\nPlease avoid last scenario. To solve this, please either do some computational work (like scenario 2) or stop your instance and wait for at least 1 hour."
  },
  {
    "objectID": "post/2020-07-30-how-to-use-aws/index.html#how-to-set-up-amazon-simple-notification-service",
    "href": "post/2020-07-30-how-to-use-aws/index.html#how-to-set-up-amazon-simple-notification-service",
    "title": "How to Use AWS",
    "section": "How to set Up Amazon Simple Notification Service?",
    "text": "How to set Up Amazon Simple Notification Service?\n1). Create the topic using the create-topic command. You receive a topic resource name as a return value:\naws sns create-topic --name <YOUR ALARM NAME>\n2). Subscribe your email address to the topic using the subscribe command. You will receive a confirmation email message if the subscription request succeeds.\naws sns subscribe --topic-arn arn:aws:sns:us-east-1:215500445195:<YOUR ALARM NAME> --protocol email --notification-endpoint <YOUR EMAIL ADDRESS>\n3). Confirm that you intend to receive email from Amazon Simple Notification Service by clicking the confirmation link in the body of the message to complete the subscription process.  4). Publish a message directly to the topic using the publish command to ensure that the topic is properly configured.\naws sns publish --message \"Verification\" --topic arn:aws:sns:us-east-1:215500445195:<YOUR ALARM NAME>\n5). Check your email to confirm that you received the message from the topic."
  },
  {
    "objectID": "post/2022-01-02-biobank-japan-data-in-cri/index.html",
    "href": "post/2022-01-02-biobank-japan-data-in-cri/index.html",
    "title": "Biobank Japan Data in CRI",
    "section": "",
    "text": "BBJ data directory: \\gpfs/data/im-lab/nas40t2/Data/BBJ\nI first downloaded and decrypted Biobank Japan data (instructions), then organized into subdirectories BBJ-genotypes-decrypted and BBJ-phenotypes-decrypted, in their original form."
  },
  {
    "objectID": "post/2022-01-02-biobank-japan-data-in-cri/index.html#phenotypes",
    "href": "post/2022-01-02-biobank-japan-data-in-cri/index.html#phenotypes",
    "title": "Biobank Japan Data in CRI",
    "section": "Phenotypes",
    "text": "Phenotypes\nBBJ phenotypes file: gpfs/data/im-lab/nas40t2/Data/BBJ/BBJ-phenotypes.csv\nThis CSV combines all phenotype data in the BBJ-phenotypes-decrypted subdirectory into one file. The original BBJ phenotype data in BBJ-phenotypes-decrypted, was bulky and used dataset IDs instead of phenotype names. The file BBJ-phenotype-list.txt contains all the phenotypes and their folder names (download)\nI created the combined phenotype file with the following script:\npython3 process-phenotypes.py --BBJ_folder /Users/sabrinami/BBJ/BBJ-phenotypes \\\n--phenotype_mapping /Users/sabrinami/Github/analysis-sabrina/BBJ-data-processing/BBJ-phenotype-list.txt \\\n--output /Users/sabrinami/Github/analysis-sabrina/BBJ-data-processing/BBJ-phenotypes.csv"
  },
  {
    "objectID": "post/2022-01-02-biobank-japan-data-in-cri/index.html#genotypes",
    "href": "post/2022-01-02-biobank-japan-data-in-cri/index.html#genotypes",
    "title": "Biobank Japan Data in CRI",
    "section": "Genotypes",
    "text": "Genotypes\nBBJ genotypes folder: gpfs/data/im-lab/nas40t2/Data/BBJ/BBJ-genotypes-decrypted"
  },
  {
    "objectID": "post/migrating-instructions/index.html",
    "href": "post/migrating-instructions/index.html",
    "title": "Migrating",
    "section": "",
    "text": "Here will come the list of migrating instructions\n\ncreate folder under post\nname the folder year-month-date-slug (slug=summary of title of the post)"
  },
  {
    "objectID": "post/2020-07-08-using-r-markdown/index.html",
    "href": "post/2020-07-08-using-r-markdown/index.html",
    "title": "Using R_Markdown",
    "section": "",
    "text": "R Markdown is a authoring framework that allows for reproducible documentation of data science within the context of R Studio. This is an introduction designed to teach you how to use R Markdown in a few minutes. Further info and some of the examples used below can be found here: http://rmarkdown.rstudio.com\n\n\n\nR Markdown allows you to incorporate text, inline code, code chunks, and results into a single document. You can then create html pages, pdfs, word docs, and other file formats discussed below."
  },
  {
    "objectID": "post/2020-07-08-using-r-markdown/index.html#installing",
    "href": "post/2020-07-08-using-r-markdown/index.html#installing",
    "title": "Using R_Markdown",
    "section": "Installing",
    "text": "Installing\nTo install R Markdown, you can navigate to tools -> install packages -> rmarkdown in R Studio. Alternatively, you can run the console command: install.packages(“rmarkdown”)"
  },
  {
    "objectID": "post/2020-07-08-using-r-markdown/index.html#creating-a-file",
    "href": "post/2020-07-08-using-r-markdown/index.html#creating-a-file",
    "title": "Using R_Markdown",
    "section": "Creating a File",
    "text": "Creating a File\nTo create a new R Markdown file, go to file -> New File -> R Markdown. There, you can create a blank file or choose a template. R Markdown files are plain text files with the .Rmd extension. If this is your first time using R Markdown, we suggest using the .html template by entering a Title and Author Name and clicking Ok."
  },
  {
    "objectID": "post/2020-07-08-using-r-markdown/index.html#header",
    "href": "post/2020-07-08-using-r-markdown/index.html#header",
    "title": "Using R_Markdown",
    "section": "Header",
    "text": "Header\nThe first few lines of every R Markdown file make up a YAML header enclosed by three “-”s on each end. If you created a new file from above, you will see that your header contains the title, author, date, and output. Other document settings can also be modified in the header, but these will be discussed later."
  },
  {
    "objectID": "post/2020-07-08-using-r-markdown/index.html#text-formatting-tips",
    "href": "post/2020-07-08-using-r-markdown/index.html#text-formatting-tips",
    "title": "Using R_Markdown",
    "section": "Text Formatting Tips",
    "text": "Text Formatting Tips\nText written in R Markdown uses the Markdown syntax. There are a number of annotations that will result in formatted text upon file rendering. A helpful reference guide can be found here and can be accessed by going to Help -> Markdown Quick Reference\nBelow are some formatting examples that you can try. You can preview using the Knit button in the toolbar, which will be discussed in further detail below.\n*italic* or _italic_\n**bold** or __bold__\nsuperscript^2^ and subscript~2~\n~~strikethrough~~\ncode blocks can be made with “``”\nline breaks require two or more spaces at the end of the line\nlinks: [text for link](www.rstudio.com)\n# Example Header for largest header, add more # for smaller headers. Note: you must include the space after the # or it will not format correctly\nFor further syntax tips (including lists, images, etc…), take a look at the reference documents linked above."
  },
  {
    "objectID": "post/2020-07-08-using-r-markdown/index.html#code-chunks",
    "href": "post/2020-07-08-using-r-markdown/index.html#code-chunks",
    "title": "Using R_Markdown",
    "section": "Code Chunks",
    "text": "Code Chunks\nYou can incorporate code chunks into your R Markdown files using:\n* The  button in the toolbar\n* Ctrl + Alt + I (OS X: Cmd + Option +I)\n* Chunk delimiters like below"
  },
  {
    "objectID": "post/2021-04-27-querying-predictdb-sqlite-databases/index.html",
    "href": "post/2021-04-27-querying-predictdb-sqlite-databases/index.html",
    "title": "Querying PredictDB sqlite databases",
    "section": "",
    "text": "PredictDB databases are stored in simple sqlite files. You can programmatically query them via python, R, perl, etc (using appropriate libraries). Below is an example on how to query the database in R.\nEach has two tables the extra and the weights tables. - extra table contains the list of available genes and some prediction performance information (for elastic net, not for the mashr models), - weights table contains the weights for predicting the gene expression levels (or other mediating/molecular traits)\n\n\nShow the code\n## install.packages(\"RSQLite\")\nlibrary(\"RSQLite\")\nsqlite <- dbDriver(\"SQLite\")\ndbname <- \"assets/en_Adipose_Subcutaneous.db\" ## add full path if db file not in current directory\n## connect to db\ndb = dbConnect(sqlite,dbname)\n## list tables\ndbListTables(db)\n\n\n[1] \"extra\"   \"weights\"\n\n\nShow the code\ndbListFields(db, \"weights\")\n\n\n[1] \"gene\"       \"rsid\"       \"varID\"      \"ref_allele\" \"eff_allele\"\n[6] \"weight\"    \n\n\nShow the code\ndbListFields(db, \"extra\")\n\n\n [1] \"gene\"                  \"genename\"              \"gene_type\"            \n [4] \"alpha\"                 \"n_snps_in_window\"      \"n.snps.in.model\"      \n [7] \"test_R2_avg\"           \"test_R2_sd\"            \"cv_R2_avg\"            \n[10] \"cv_R2_sd\"              \"in_sample_R2\"          \"nested_cv_fisher_pval\"\n[13] \"nested_cv_converged\"   \"rho_avg\"               \"rho_se\"               \n[16] \"rho_zscore\"            \"pred.perf.R2\"          \"pred.perf.pval\"       \n[19] \"pred.perf.qval\"       \n\n\nShow the code\n## convenience query function\nquery <- function(...) dbGetQuery(db, ...)\n## example queries\nquery('select count(*) from weights')\n\n\n  count(*)\n1   249965\n\n\nShow the code\nquery('select * from weights where gene = \"GATA6\" ')\n\n\n[1] gene       rsid       varID      ref_allele eff_allele weight    \n<0 rows> (or 0-length row.names)\n\n\nShow the code\nquery('select * from weights limit 10')\n\n\n                gene       rsid                varID ref_allele eff_allele\n1  ENSG00000261456.5 rs11252127  chr10_52147_C_T_b38          C          T\n2  ENSG00000261456.5 rs11252546  chr10_58487_T_C_b38          T          C\n3  ENSG00000261456.5 rs11591988  chr10_80130_C_T_b38          C          T\n4  ENSG00000261456.5  rs4495823  chr10_97603_G_A_b38          G          A\n5  ENSG00000261456.5 rs11253478  chr10_98907_C_T_b38          C          T\n6  ENSG00000261456.5  rs7901397 chr10_102757_T_C_b38          T          C\n7  ENSG00000261456.5  rs7476951 chr10_137211_T_C_b38          T          C\n8  ENSG00000261456.5  rs3123247 chr10_264285_C_T_b38          C          T\n9  ENSG00000261456.5  rs4880567 chr10_267364_T_C_b38          T          C\n10 ENSG00000261456.5  rs4881392 chr10_519179_A_G_b38          A          G\n         weight\n1   0.052252706\n2  -0.033544959\n3   0.014296499\n4  -0.030826218\n5   0.013036311\n6  -0.118567569\n7   0.007865262\n8   0.004556608\n9   0.030123310\n10  0.057842737\n\n\nShow the code\n## how many genes are available for given tissue?\n## dbname should be the name of the sqlite database for the tissue\nquery('select count(*) from extra')\n\n\n  count(*)\n1     8650\n\n\nShow the code\n## select genes with R2>0.01 (this is cor>0.1)\n## ths won't work for the latest MASHR-based GTEx V8 models\n## this only works for models where R2 is included. \n#high.h2.genes <- query('select * from extra where R2 > 0.01')"
  },
  {
    "objectID": "post/2021-06-03-heritability-calculation/index.html",
    "href": "post/2021-06-03-heritability-calculation/index.html",
    "title": "Heritability Calculation",
    "section": "",
    "text": "How to calculate Heritability using GCTA\nIf you’re data is already in a PLINK form then it’s quite easy to use GCTA. You need to first create the genetic relatedness matrix (GRM) - make sure you’ve already done quality control on the genotype data since a high rate of missing SNPS will lead to negative eigenvalues in the GRM.\ngcta –bfile ./plink_files –maf 0.01 –make-grm –out test –thread-num 10\nThe command above will produce the GRM genomewide. The genetic relationship matrix will be saved in the files test.grm.bin, test.grm.N.bin and test.grm.id in whatever directory you’re standing in. For datasets with an extremely large number of SNPs and large sample size (e.g. 1000G imputed data) you can use the flag –chr to calculate the GRM for each autosome. If you want to remove an relatedness, you can also use the flag –grm-cutoff (make sure what your cutoff is and that its not too strict)\nFor REML analysis, you next use the command below. GCTA uses the first column in the phenotype file as the phenotype it’s testing. If you want to use another column you can use the –mpheno flag.\ngcta6 –reml –grm test –pheno test_cc.phen –out test_cc\nAbove is all for genomewide heritability. If you want to calculate cis-heritability, you need to make individual GRMs for each gene and only include the cis-snps. You would then run the above gcta command in a loop for each GRM. Here’s an example:\n\nfor(i in 1:length(genelist)){ cat(i,\"/\",length(genelist),\"\\n\") gene <- genelist[i] geneinfo <- gtf[match(gene, gtf$gene_id),] chr <- geneinfo[1] c <- chr$chr start <- geneinfo$start - 1e6 ### 1Mb lower bound for cis-eQTLS end <- geneinfo$end + 1e6 ### 1Mb upper bound for cis-eQTLs chrsnps <- subset(bim,bim[,1]==c) ### pull snps on same chr cissnps <- subset(chrsnps,chrsnps[,4]>=start & chrsnps[,4]<=end) ### pull cis-SNP info snplist <- cissnps[,2]     write.table(snplist, file= gt.dir %&% \"tmp.cis.SNPlist\",quote=F,col.names=F,row.names=F) runGCTAgrm <- \"gcta --bfile \" %&%  box.dir %&% \"tmp --make-grm-bin --extract \" %&% gt.dir %&% \"tmp.cis.SNPlist\" %&% \" --out   \" %&% grm.dir %&%  gene system(runGCTAgrm) }\n\nAnother reminder to perform quality control on your genotype data before going forth in calculating heritability. For example, here is genotype data with a high rate of missing SNPS\n0% 25% 50% 75% 100% ___ 0.07913669 0.09352518 0.10791367 0.12230216 0.71223022\nAlmost half of the samples contain only 10% of all snps. This genotype data would not be right to use for GCTA since the program will have issues calculating the GRM. It is better ot instead imput the genotype and then use that"
  },
  {
    "objectID": "post/2022-03-07-publishing-a-shinyapp/index.html",
    "href": "post/2022-03-07-publishing-a-shinyapp/index.html",
    "title": "Publishing a ShinyApp",
    "section": "",
    "text": "This was inspired by this article\n\n\n\nFirst, we need to install and load the rsconnect package.\ninstall.packages('rsconnect')\nlibrary('rsconnect')\nOnce you login to shinyapps.io, navigate to the token page.\n\n\n\nToken Example\n\n\nThen, we can access our own personal token, which we need to link to our RStudio session, by clicking show, show secret, and copying and pasting the command into RStudio.\n\n\n\nSecret Command\n\n\nThe command should take the form as follows:\nrsconnect::setAccountInfo(name='imlab',\n              token=<token>,\n              secret=<secret>)\nAt this point, if you have access to deploy the app, you can deploy it onto shinyapps.io with the command:\ndeployApp(appName='yourAppName')\nIf you are experiencing errors with the app deployment, deleting the app’s manifest.json may help resolve errors."
  },
  {
    "objectID": "post/2020-11-30-cri-gardner-upgrade-news/index.html",
    "href": "post/2020-11-30-cri-gardner-upgrade-news/index.html",
    "title": "CRI Gardner upgrade news",
    "section": "",
    "text": "Operating System Upgrade - The operating system will be upgraded from Red Hat Linux 6.7 to 7.6. This will provide a kernel that will allow for a more modern software ecosystem. For example, software such as tensorflow will not run on Red Hat 6.\nGPFS Upgrade - GPFS storage clients will be upgrade from GPFS 4.2 to GPFS 5. This will provide a performance increase for metadata operations such as creating, listing, and deleting files.\nSLURM Scheduling - The Torque/Moab scheduling on the system will be replaced with SLURM. SLURM is an open-source scheduler that has become a de facto standard across many HPC sites.\nDeep Learning capabilities - Last year, the CRI purchased two deep learning servers with 8 NVidia V100s per server. These servers have been open to users who requested the capabilites. With the upgrade, the deep learning systems will be added to the general scheduling queue.\nIncreased container capabilities - With the upgrade to Red Hat 7, we will be able to provide the ability for users to create their own singularity containers.\nAuthentication/Authorization - We are running authentication/authorization clients that are rather outdated at the moment. Upgrading those clients should provide a more stable environment when logging into the cluster and accessing files.\nUpgraded compilers - The compilers on the cluster will be upgraded to the latest version. This will be gcc-10.2.0, llvm-11.0, intel-2020.2, and nvhpc-20.9. The compilers will provide implementation of the latest standards for C, C++, and Fortran.\nEnhanced accounting - We will be provided accounting on both jobs submitted to the cluster as well as software use across the environment."
  },
  {
    "objectID": "post/2021-01-07-spredixcan-harmonization-errors/index.html",
    "href": "post/2021-01-07-spredixcan-harmonization-errors/index.html",
    "title": "SPrediXcan Harmonization Errors",
    "section": "",
    "text": "The error message– INFO - 0 % of model's snps used– can typically be traced to inconsistencies between variant IDs in prediction models and input GWAS files. Our GTEx v8 mashr and elastic net models are defined in hg38. For GWAS defined in hg19, we recommend first processing the GWAS with our harmonization tool (tutorial)\nHere, we focus on harmonization issues for prediction models and summary statistics in the same build. By harmonization, we mean that the same SNP can be identified with different names in the prediction model and GWAS, so we want to ensure they match. Our prediction models identify SNPs by their RSID and “varID”, a naming format with chromosome, position, allele, and build information. For example, the variant ‘rs12551220’ has varID ‘chr9_137032730_G_A_b38’. In our example, we focus on a GWAS that does not provide RSIDs for variants, so we requires some processing in order to run SPrediXcan.\nThe raw GWAS file:\nzless /Users/sabrinami/Desktop/psychencode_test_data/clozuk_pgc2.meta.sumstats.txt.gz | head\nSNP Freq.A1 CHR BP  A1  A2  OR  SE  P\n10:100968448:T:AA   0.3519  10  100968448   t   aa  1.0024  0.01    0.812\n10:101574552:A:ATG  0.4493  10  101574552   a   atg 0.989060.0097   0.2585\n10:10222597:AT:A    0   10  10222597    a   at  0.9997  0.01    0.9777\nOur prediction models follow the format:\nsqlite> SELECT * FROM weights LIMIT 5;\nENSG00000000457|rs6703487|chr1_169550749_C_T_b37|C|T|0.0010487355157559\nENSG00000000457|rs9332476|chr1_169556756_A_G_b37|A|G|0.0037017916840068\nENSG00000000457|rs2142759|chr1_169618820_G_A_b37|G|A|0.00806408251476846\nThe variants identified in the SNP column of the GWAS file match neither the rsid or varID columns of the model. If we run SPrediXcan without harmonizing, we get the INFO - 0 % of model's snps used error message:\npython3 $METAXCAN/SPrediXcan.py --gwas_file $DATA/clozuk_pgc2.meta.sumstats.out.txt.gz \\\n> --model_db_path $MODEL/psychencode_model/psychencode.db \\\n> --covariance $MODEL/psychencode_model/psychencode.txt.gz \\\n> --or_column OR \\\n> --pvalue_column P \\\n> --snp_column SNP \\\n> --non_effect_allele_column A2 \\\n> --effect_allele_column A1 \\\n> --throw \\\n> --output_file $RESULTS/clozuk_pgc2_imputeformat.csv\nINFO - Processing GWAS command line parameters\nINFO - Building beta for /Users/sabrinami/Desktop/psychencode_test_data/clozuk_pgc2.meta.sumstats.out.txt.gz and /Users/sabrinami/Github/psychencode/models/psychencode_model/psychencode.db\nINFO - Reading input gwas with special handling: /Users/sabrinami/Desktop/psychencode_test_data/clozuk_pgc2.meta.sumstats.out.txt.gz\nINFO - Processing input gwas\nINFO - Aligning GWAS to models\nINFO - Trimming output\nINFO - Successfully parsed input gwas in 18.136219276 seconds\nINFO - Started metaxcan process\nINFO - Loading model from: /Users/sabrinami/Github/psychencode/models/psychencode_model/psychencode.db\nINFO - Loading covariance data from: /Users/sabrinami/Github/psychencode/models/psychencode_model/psychencode.txt.gz\nINFO - Processing loaded gwas\nINFO - Started metaxcan association\nINFO - 0 % of model's snps used\nINFO - Sucessfully processed metaxcan association in 132.015579446 seconds\nTo harmonize, we add a varID column in the GWAS file with the same format of as the model using information in the CHR, BP, A1, A2 columns. The modified GWAS looks like:\nzless /Users/sabrinami/Desktop/psychencode_test_data/clozuk_pgc2.meta.sumstats.out.txt.gz | head\nSNP Freq.A1 CHR BP  A1  A2  OR  SE  P   varID   \n10:100968448:T:AA   0.3519  10  100968448   T   AA  1.0024  0.01    0.812   chr10_100968448_T_AA_b37    \n10:101574552:A:ATG  0.4493  10  101574552   A   ATG 0.989060.0097   0.2585  chr10_101574552_A_ATG_b37\n10:10222597:AT:A    0   10  10222597    A   AT  0.9997  0.01    0.9777  chr10_10222597_A_AT_b37\nWe also change our SPrediXcan parameters to indicate the GWAS SNP names follow the varID format, with the argument --model_db_snp_key varID. It runs correctly because it messages INFO - 63 % of model's snps used:\npython3 $METAXCAN/SPrediXcan.py --gwas_file $DATA/clozuk_pgc2.meta.sumstats.out.txt.gz \\\n> --model_db_path $MODEL/psychencode_model/psychencode.db \\\n> --covariance $MODEL/psychencode_model/psychencode_varID.txt.gz \\\n> --keep_non_rsid --model_db_snp_key varID \\\n> --or_column OR \\\n> --pvalue_column P \\\n> --snp_column varID \\\n> --non_effect_allele_column A2 \\\n> --effect_allele_column A1 \\\n> --throw \\\n> --output_file $RESULTS/clozuk_pgc2_psychencode.csv\nINFO - Processing GWAS command line parameters\nINFO - Building beta for /Users/sabrinami/Desktop/psychencode_test_data/clozuk_pgc2.meta.sumstats.out.txt.gz and /Users/sabrinami/Github/psychencode/models/psychencode_model/psychencode.db\nINFO - Reading input gwas with special handling: /Users/sabrinami/Desktop/psychencode_test_data/clozuk_pgc2.meta.sumstats.out.txt.gz\nINFO - Processing input gwas\nINFO - Aligning GWAS to models\nINFO - Trimming output\nINFO - Successfully parsed input gwas in 22.284876615 seconds\nINFO - Started metaxcan process\nINFO - Loading model from: /Users/sabrinami/Github/psychencode/models/psychencode_model/psychencode.db\nINFO - Loading covariance data from: /Users/sabrinami/Github/psychencode/models/psychencode_model/psychencode_varID.txt.gz\nINFO - Processing loaded gwas\nINFO - Started metaxcan association\nINFO - 10 % of model's snps found so far in the gwas study\nINFO - 20 % of model's snps found so far in the gwas study\nINFO - 30 % of model's snps found so far in the gwas study\nINFO - 40 % of model's snps found so far in the gwas study\nINFO - 50 % of model's snps found so far in the gwas study\nINFO - 60 % of model's snps found so far in the gwas study\nINFO - 63 % of model's snps used\nINFO - Sucessfully processed metaxcan association in 153.879826246 seconds\nIf we still see the INFO - 0 % of model's snps used message, there may still be inconsistencies between the GWAS and covariance file.\nIn the example, SPrediXcan runs using the varID format names from both the GWAS file and model. However, the covariance file may still identify variants by their RSID:\nzless /Users/sabrinami/Github/psychencode/models/psychencode_model/psychencode.txt.gz | head\nGENE RSID1 RSID2 VALUE\nENSG00000003249 rs140580708 rs140580708 0.013030848604677675\nENSG00000003249 rs140580708 rs164739 -0.008913106189505495\nENSG00000003249 rs140580708 rs1991508 -0.0017680061881014297\n\nCreating a new covariance file with varID format variant names replacing the RSID1 and RSID2 columns should fix the error. The prediction model file provides a dictionary to update IDs. We have example code to do so.\nThe following covariance file (with the same column names) gives the correct output:\nzless /Users/sabrinami/Github/psychencode/models/psychencode_model/psychencode_varID.txt.gz | head\nGENE RSID1 RSID2 VALUE\nENSG00000003249 chr16_89594100_C_G_b37 chr16_89594100_C_G_b37 0.0130308486046777\nENSG00000003249 chr16_89594100_C_G_b37 chr16_89667699_C_G_b37 -0.0089131061895055\nENSG00000003249 chr16_89594100_C_G_b37 chr16_89672155_C_T_b37 -0.00176800618810143"
  },
  {
    "objectID": "post/2023-03-01-gtex-sample-size-by-tissue/index.html",
    "href": "post/2023-03-01-gtex-sample-size-by-tissue/index.html",
    "title": "gtex-sample-size-by-tissue",
    "section": "",
    "text": "GTEx Sample Size by Tissue\n\n\n\n\n\n\n\n\n\n\nname\neuropean samples\nabbreviation\nexpression models\nsplicing models\n\n\n\n\nAdipose - Subcutaneous\n491\nADPSBQ\n14732\n42912\n\n\nAdipose - Visceral (Omentum)\n401\nADPVSC\n14640\n41720\n\n\nAdrenal Gland\n200\nADRNLG\n13622\n36754\n\n\nArtery - Aorta\n338\nARTAORT\n14396\n40474\n\n\nArtery - Coronary\n180\nARTCRN\n13878\n40579\n\n\nArtery - Tibial\n489\nARTTBL\n14493\n40690\n\n\nBrain - Amygdala\n119\nBRNAMY\n12814\n24236\n\n\nBrain - Anterior cingulate cortex (BA24)\n135\nBRNACC\n13528\n28806\n\n\nBrain - Caudate (basal ganglia)\n172\nBRNCDT\n14118\n32127\n\n\nBrain - Cerebellar Hemisphere\n157\nBRNCHB\n13771\n39862\n\n\nBrain - Cerebellum\n188\nBRNCHA\n13992\n40747\n\n\nBrain - Cortex\n184\nBRNCTXA\n14284\n35086\n\n\nBrain - Frontal Cortex (BA9)\n158\nBRNCTXB\n14091\n32031\n\n\nBrain - Hippocampus\n150\nBRNHPP\n13526\n27437\n\n\nBrain - Hypothalamus\n157\nBRNHPT\n13741\n30326\n\n\nBrain - Nucleus accumbens (basal ganglia)\n181\nBRNNCC\n14062\n32670\n\n\nBrain - Putamen (basal ganglia)\n153\nBRNPTM\n13694\n28461\n\n\nBrain - Spinal cord (cervical c-1)\n115\nBRNSPC\n13096\n28883\n\n\nBrain - Substantia nigra\n101\nBRNSNG\n12637\n23677\n\n\nBreast - Mammary Tissue\n337\nBREAST\n14654\n44613\n\n\nCells - Cultured fibroblasts\n417\nFIBRBLS\n13976\n36809\n\n\nCells - EBV-transformed lymphocytes\n116\nLCL\n12398\n37627\n\n\nColon - Sigmoid\n274\nCLNSGM\n14363\n41581\n\n\nColon - Transverse\n306\nCLNTRN\n14582\n41215\n\n\nEsophagus - Gastroesophageal Junction\n281\nESPGEJ\n14285\n41004\n\n\nEsophagus - Mucosa\n423\nESPMCS\n14589\n37186\n\n\nEsophagus - Muscularis\n399\nESPMSL\n14603\n40376\n\n\nHeart - Atrial Appendage\n322\nHRTAA\n14035\n36322\n\n\nHeart - Left Ventricle\n334\nHRTLV\n13200\n29470\n\n\nKidney - Cortex\n65\nKDNCTX\n11164\n24571\n\n\nLiver\n183\nLIVER\n12714\n27011\n\n\nLung\n444\nLUNG\n15058\n44346\n\n\nMinor Salivary Gland\n119\nSLVRYG\n13884\n38380\n\n\nMuscle - Skeletal\n602\nMSCLSK\n13381\n31855\n\n\nNerve - Tibial\n449\nNERVET\n15373\n45478\n\n\nOvary\n140\nOVARY\n13738\n40857\n\n\nPancreas\n253\nPNCREAS\n13695\n31203\n\n\nPituitary\n219\nPTTARY\n14647\n42343\n\n\nProstate\n186\nPRSTTE\n14450\n41991\n\n\nSkin - Not Sun Exposed (Suprapubic)\n440\nSKINNS\n14932\n42005\n\n\nSkin - Sun Exposed (Lower leg)\n517\nSKINS\n15204\n42219\n\n\nSmall Intestine - Terminal Ileum\n144\nSNTTRM\n14065\n39864\n\n\nSpleen\n186\nSPLEEN\n14073\n40290\n\n\nStomach\n269\nSTMACH\n14102\n36624\n\n\nTestis\n277\nTESTIS\n17867\n67784\n\n\nThyroid\n494\nTHYROID\n15303\n45217\n\n\nUterus\n108\nUTERUS\n13199\n39485\n\n\nVagina\n122\nVAGINA\n12969\n36931\n\n\nWhole Blood\n573\nWHLBLD\n12623\n24568"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html",
    "href": "post/2022-06-10-qgt-training/index.html",
    "title": "Transcriptome QGT Training 2023",
    "section": "",
    "text": "Predict whole blood expression from genotype\nCheck how well the prediction works with GEUVADIS expression data\nRun association between predicted expression and a phenotype\nCalculate association between expression levels and coronary artery disease risk using s-predixcan\nFine-map the coronary artery disease gwas results using torus\nCalculate colocalization probability using fastenloc\nRun cTWAS (fine-map SNPs and genes jointly)"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#questionnaire-01",
    "href": "post/2022-06-10-qgt-training/index.html#questionnaire-01",
    "title": "Transcriptome QGT Training 2023",
    "section": "Questionnaire 01",
    "text": "Questionnaire 01\n\nOpen and start filling questionnaire 01 Preliminary questionnaire https://forms.gle/fhNJAyjx7MJTy3yt8\nInstall packages as needed\n\n\n# List of packages you want to install\npackages <- c(\"tidyverse\", \"data.table\", \"BEDMatrix\", \"Rfast\", \"susieR\", \"coloc\")\n\n# Function to check and install any missing packages\ncheck_and_install <- function(pkg){\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg, dependencies = TRUE)\n    library(pkg, character.only = TRUE)\n  }\n}\n\n# Use the function to check and install packages\nsapply(packages, check_and_install)\n\n\nLoad Rstudio Libraries\n\n\nlibrary(tidyverse)\n\n## packages needed for susie+coloc\nlibrary(data.table)\nlibrary(BEDMatrix)\nlibrary(Rfast)\nlibrary(susieR)\nlibrary(coloc)\n##library(tidyverse)\n\n\nNavigate to starting directory\n\n\ncd \"/cloud/project/\"\n\n\nactivate the the imlabtools environment, which will make sure that the right version of python modules are available\n\n\nconda activate imlabtools\n\n\nTo define some variables to access the data more easily within the R session, run the following r chunk\n\n\nprint(getwd())\n\nlab=\"/cloud/project/QGT-Columbia-HKI-repo/\"\nCODE=glue::glue(\"{lab}/code\")\nsource(glue::glue(\"{CODE}/load_data_functions.R\"))\nsource(glue::glue(\"{CODE}/plotting_utils_functions.R\"))\n\nPRE=\"/cloud/project/QGT-Columbia-HKI-repo/box_files\"\nMODEL=glue::glue(\"{PRE}/models\")\nDATA=glue::glue(\"{PRE}/data\")\nRESULTS=glue::glue(\"{PRE}/results\")\nMETAXCAN=glue::glue(\"{PRE}/repos/MetaXcan-master/software\")\nFASTENLOC=glue::glue(\"{PRE}/repos/fastenloc-master\")\n\n# This is a reference table we'll use a lot throughout the lab. It contains information about the genes.\ngencode_df = load_gencode_df()\n\n\ncheck the values of the variables you just defined in R\n\n\nMODEL\n\nDATA\n\n\ndefine some variables to access the data more easily in the terminal. Remember we are running R code in the R console and command line code in the terminal.\n\n\nexport PRE=\"/cloud/project/QGT-Columbia-HKI-repo/box_files\"\nexport LAB=\"/cloud/project/QGT-Columbia-HKI-repo/\"\nexport CODE=$LAB/code\nexport DATA=$PRE/data\nexport MODEL=$PRE/models\nexport RESULTS=$PRE/results\nexport METAXCAN=$PRE/repos/MetaXcan-master/software\n\n\ncheck the values of the variables you just defined\n\n\n\necho $CODE\necho $RESULTS"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#questionnaire-02",
    "href": "post/2022-06-10-qgt-training/index.html#questionnaire-02",
    "title": "Transcriptome QGT Training 2023",
    "section": "Questionnaire 02",
    "text": "Questionnaire 02\n\nOpen and start filling questionnaire 02 Prediction https://forms.gle/T6kAHvFTxYfcQguW7"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#optional-assess-actual-prediction-performance",
    "href": "post/2022-06-10-qgt-training/index.html#optional-assess-actual-prediction-performance",
    "title": "Transcriptome QGT Training 2023",
    "section": "(Optional) Assess Actual Prediction Performance",
    "text": "(Optional) Assess Actual Prediction Performance\n\n## download and read observed expression data from GEUVADIS \n## from https://uchicago.box.com/s/4y7xle5l0pnq9d1fwmthe2ewhogrnlrv\n\nobs_exp<- read_csv(glue::glue(\"{DATA}/predixcan/GEUVADIS.observed_df.csv.gz\"))\n\n## Note that the version of the ensemble id of the gene was removed\nhead(predicted_expression)\n\n## Q: how many genes were predicted?\nlength(unique(predicted_expression$gene_id))\n\n## inner join predicted expression with observed expression data (by IID and gene)\n## common errors occur when ensemble id's have versions in one set and not the other set\nfullset=inner_join(predicted_expression, obs_exp, by = c(\"gene_id\",\"IID\"))\n\n## calculate spearman correlation for all genes\ngenelist = unique(predicted_expression$gene_id)\ncorvec = rep(NA,length(genelist))\nnames(corvec) = genelist\nfor(gg in 1:length(genelist))\n{\n  ind = fullset$gene_id==genelist[gg]\n  corvec[gg] = cor(fullset$predicted_expression[ind], fullset$observed_expression[ind])\n}\n\n## what's the best performing gene?\n\n## plot the histogram of the prediction performance\nhist(corvec)\n\n## list the top 10 best performing genes\nhead(sort(corvec,decreasing = TRUE),2)\ntail(sort(corvec,decreasing = TRUE),2)\n\n## plot the correlation of the top 2 best performing genes bottom 2\n\ngeneid = \"ENSG00000100376\"\ngenename = gencode_df %>% filter(gene_id==geneid) %>% .[[\"gene_name\"]]\nfullset %>% filter(gene_id==geneid) %>% ggplot(aes(observed_expression, predicted_expression))+geom_point()+ggtitle(paste(genename, \"-\",geneid))\n\ngeneid = \"ENSG00000075234\"\ngenename = gencode_df %>% filter(gene_id==geneid) %>% .[[\"gene_name\"]]\nfullset %>% filter(gene_id==geneid) %>% ggplot(aes(observed_expression, predicted_expression))+geom_point()+ggtitle(paste(genename, \"-\", geneid))\n\ngeneid = \"ENSG00000070371\"\ngenename = gencode_df %>% filter(gene_id==geneid) %>% .[[\"gene_name\"]]\nfullset %>% filter(gene_id==geneid) %>% ggplot(aes(observed_expression, predicted_expression))+geom_point()+ggtitle(paste(genename, \"-\", geneid))\n\ngeneid = \"ENSG00000184164\"\ngenename = gencode_df %>% filter(gene_id==geneid) %>% .[[\"gene_name\"]]\nfullset %>% filter(gene_id==geneid) %>% ggplot(aes(observed_expression, predicted_expression))+geom_point()+ggtitle(paste(genename, \"-\", geneid))"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#questionnaire-03",
    "href": "post/2022-06-10-qgt-training/index.html#questionnaire-03",
    "title": "Transcriptome QGT Training 2023",
    "section": "Questionnaire 03",
    "text": "Questionnaire 03\n\nOpen and start filling questionnaire 03 PrediXcan https://forms.gle/3H319knWbLgnynNs9\n\nWe are going to use a simulated phenotype for which only UPK3A has an effect on the phenotype (\\(\\beta=-0.9887378\\))\n\\(Y = \\sum_k T_k \\beta_k + \\epsilon\\)\nwith random effects \\(\\beta_k \\sim (1-\\pi)\\cdot \\delta_0 + \\pi\\cdot N(0,1)\\)\n\n\nexport PHENO=\"sim.spike_n_slab_0.01_pve0.1\"\n\nprintf \"association\\n\\n\"\npython3 $METAXCAN/PrediXcanAssociation.py \\\n--expression_file $RESULTS/predixcan/Whole_Blood__predict.txt \\\n--input_phenos_file $DATA/predixcan/phenotype/$PHENO.txt \\\n--input_phenos_column pheno \\\n--output $RESULTS/predixcan/$PHENO/Whole_Blood__association.txt \\\n--verbosity 9 \\\n--throw\n\nMore predicted phenotypes can be found in $DATA/predixcan/phenotype/. The naming of the phenotypes provides information about the genetic architecture: the number after pve is the proportion of variance of Y explained by the genetic component of expression. The number after spike_n_slab represents the probability that a gene is causal π (i.e. prob β≠0)"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#looking-at-association-results",
    "href": "post/2022-06-10-qgt-training/index.html#looking-at-association-results",
    "title": "Transcriptome QGT Training 2023",
    "section": "Looking at Association Results",
    "text": "Looking at Association Results\n\n## read association results\nPHENO=\"sim.spike_n_slab_0.01_pve0.1\"\n\npredixcan_association = load_predixcan_association(glue::glue(\"{RESULTS}/predixcan/{PHENO}/Whole_Blood__association.txt\"), gencode_df)\n\n## take a look at the results\ndim(predixcan_association)\npredixcan_association %>% arrange(pvalue) %>% select(gene_name,effect,se,pvalue,gene) %>% head\npredixcan_association %>% arrange(pvalue) %>% ggplot(aes(pvalue)) + geom_histogram(bins=10)\n## compare distribution against the null (uniform)\ngg_qqplot(predixcan_association$pvalue, max_yval = 40)"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#comparing-the-estimated-effect-size-with-true-effect-size",
    "href": "post/2022-06-10-qgt-training/index.html#comparing-the-estimated-effect-size-with-true-effect-size",
    "title": "Transcriptome QGT Training 2023",
    "section": "Comparing the Estimated Effect Size with True Effect Size",
    "text": "Comparing the Estimated Effect Size with True Effect Size\n\ntruebetas = load_truebetas(glue::glue(\"{DATA}/predixcan/phenotype/gene-effects/{PHENO}.txt\"), gencode_df)\nbetas = (predixcan_association %>% \n               inner_join(truebetas,by=c(\"gene\"=\"gene_id\")) %>%\n               select(c('estimated_beta'='effect', \n                        'true_beta'='effect_size',\n                        'pvalue', \n                        'gene_id'='gene', \n                        'gene_name'='gene_name.x', \n                        'region_id'='region_id.x')))\nbetas %>% arrange(pvalue) %>% select(gene_name,estimated_beta,true_beta,pvalue) %>% head\n## do you see examples of potential LD contamination?\nbetas %>% mutate(causal= true_beta!=0) %>% ggplot(aes(estimated_beta, true_beta,col=causal))+geom_point(alpha=0.6,size=5)+geom_abline()+theme_bw()\n\n\nUPK3A is the causal gene and has the most significant pvalue. RIBC2 is also significantly associated but has no causal role (we know because we simulated the phenotype that way). Why?\n\nHint: correlation between the genes"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#questionnaire-04",
    "href": "post/2022-06-10-qgt-training/index.html#questionnaire-04",
    "title": "Transcriptome QGT Training 2023",
    "section": "Questionnaire 04",
    "text": "Questionnaire 04\n\nOpen and start filling questionnaire 04 S-PrediXcan https://forms.gle/xJs2U66cnrqdb5cj6"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#running-s-predixcan",
    "href": "post/2022-06-10-qgt-training/index.html#running-s-predixcan",
    "title": "Transcriptome QGT Training 2023",
    "section": "Running S-PrediXcan",
    "text": "Running S-PrediXcan\n\n\npython $METAXCAN/SPrediXcan.py \\\n--gwas_file  $DATA/spredixcan/imputed_CARDIoGRAM_C4D_CAD_ADDITIVE.txt.gz \\\n--snp_column panel_variant_id \\\n--effect_allele_column effect_allele \\\n--non_effect_allele_column non_effect_allele \\\n--zscore_column zscore \\\n--model_db_path $MODEL/gtex_v8_mashr/mashr_Whole_Blood.db \\\n--covariance $MODEL/gtex_v8_mashr/mashr_Whole_Blood.txt.gz \\\n--keep_non_rsid \\\n--additional_output \\\n--model_db_snp_key varID \\\n--throw \\\n--output_file $RESULTS/spredixcan/eqtl/CARDIoGRAM_C4D_CAD_ADDITIVE__PM__Whole_Blood.csv\n\n\nWe can run the full genome because the summary statistics based PrediXcan is much faster than individual level one."
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#plot-and-interpret-results",
    "href": "post/2022-06-10-qgt-training/index.html#plot-and-interpret-results",
    "title": "Transcriptome QGT Training 2023",
    "section": "Plot and Interpret Results",
    "text": "Plot and Interpret Results\n\nspredixcan_association = load_spredixcan_association(glue::glue(\"{RESULTS}/spredixcan/eqtl/CARDIoGRAM_C4D_CAD_ADDITIVE__PM__Whole_Blood.csv\"), gencode_df)\ndim(spredixcan_association)\nspredixcan_association %>% arrange(pvalue) %>% head\nspredixcan_association %>% arrange(pvalue) %>% ggplot(aes(pvalue)) + geom_histogram(bins=20)\n\ngg_qqplot(spredixcan_association$pvalue)\n\n\nQuestion: SORT1, considered to be a causal gene for LDL cholesterol and as a consequence of coronary artery disease, is not found here. Why?\ncheck whether SORT1 is expressed in whole blood GTEx portal\ncheck whether SORT1 has eQTL in whole blood GTEx portal"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#run-s-predixcan-using-gene-expression-predicted-in-liver",
    "href": "post/2022-06-10-qgt-training/index.html#run-s-predixcan-using-gene-expression-predicted-in-liver",
    "title": "Transcriptome QGT Training 2023",
    "section": "Run S-PrediXcan using gene expression predicted in liver",
    "text": "Run S-PrediXcan using gene expression predicted in liver\n-[] Run s-predixcan with liver model, do you find SORT1? Is it significant?\n\n#loction Liver models \n#/cloud/project/QGT-Columbia-HKI-repo/box_files/models/gtex_v8_mashr/\npython $METAXCAN/SPrediXcan.py \\\n--gwas_file  $DATA/spredixcan/imputed_CARDIoGRAM_C4D_CAD_ADDITIVE.txt.gz \\\n--snp_column panel_variant_id \\\n--effect_allele_column effect_allele \\\n--non_effect_allele_column non_effect_allele \\\n--zscore_column zscore \\\n--model_db_path $MODEL/gtex_v8_mashr/mashr_Liver.db \\\n--covariance $MODEL/gtex_v8_mashr/mashr_Liver.txt.gz \\\n--keep_non_rsid \\\n--additional_output \\\n--model_db_snp_key varID \\\n--throw \\\n--output_file $RESULTS/spredixcan/eqtl/CARDIoGRAM_C4D_CAD_ADDITIVE__PM__Liver.csv\n\n\nspredixcan_association_L= load_spredixcan_association(glue::glue(\"{RESULTS}/spredixcan/eqtl/CARDIoGRAM_C4D_CAD_ADDITIVE__PM__Liver.csv\"), gencode_df)\ndim(spredixcan_association_L)\nspredixcan_association_L %>% arrange(pvalue) %>% head\nspredixcan_association_L %>% arrange(pvalue) %>% ggplot(aes(pvalue)) + geom_histogram(bins=20)\n\ngg_qqplot(spredixcan_association_L$pvalue)\ncol_order= c(\"gene_name\",\"gene\",\"zscore\",\"effect_size\",\"pvalue\",\"var_g\",\"pred_perf_r2\", \"pred_perf_pval\",\"pred_perf_qval\", \"n_snps_used\", \"n_snps_in_cov\", \"n_snps_in_model\",\"best_gwas_p\",\"largest_weight\")\nspredixcan_association_L <- spredixcan_association_L[, col_order]\nfilter(spredixcan_association_L, gene_name==\"SORT1\")"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#optional-compare-zscores-in-liver-and-whole-blood.",
    "href": "post/2022-06-10-qgt-training/index.html#optional-compare-zscores-in-liver-and-whole-blood.",
    "title": "Transcriptome QGT Training 2023",
    "section": "(Optional) Compare zscores in liver and whole blood.",
    "text": "(Optional) Compare zscores in liver and whole blood.\n\nRecall that zscore is the effect size divided by the standard error\n\n\nspredixcan_association_L=rename(spredixcan_association_L, zscore_liver = \"zscore\")\nhead(spredixcan_association_L)\ntest=left_join(spredixcan_association, spredixcan_association_L, by=\"gene_name\")\ntest=select(test,\"gene_name\",\"zscore\",\"zscore_liver\")\ntest %>% arrange(zscore_liver) %>% head\n\ntest %>% mutate(zscore_WB=zscore) %>% ggplot(aes(zscore_WB,zscore_liver)) + geom_point(size=3,alpha=.6) + geom_abline()\n\n## S-PrediXcan association in liver and whole blood are significantly correlated\ncor.test(test$zscore,test$zscore_liver)"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#optional-run-multixcan",
    "href": "post/2022-06-10-qgt-training/index.html#optional-run-multixcan",
    "title": "Transcriptome QGT Training 2023",
    "section": "(Optional) Run MultiXcan",
    "text": "(Optional) Run MultiXcan\n\nmultixcan aggregates information across multiple tissues to boost the power to detect association. It was developed movivated by the fact that eQTLs are shared across multiple tissues, i.e. many genetic variants that regulate expression are common across tissues.\nbefore you run multixcan ensure you have run s-predixcan for all the tissues you want to multixcan. In this tutorial we have two tissues (liver and whole blood), ensure you have run s-predixcan with the two tissues before running multixcan.\nOne thing to note is to ensure similar naming pattern for the output files. This is to ensure the files are captured correctly when running multixcan’s filter.\n\n\n\npython $METAXCAN/SMulTiXcan.py \\\n--models_folder $MODEL/gtex_v8_mashr \\\n--models_name_pattern \"mashr_(.*).db\" \\\n--snp_covariance $MODEL/gtex_v8_expression_mashr_snp_smultixcan_covariance.txt.gz \\\n--metaxcan_folder $RESULTS/spredixcan/eqtl/ \\\n--metaxcan_filter \"CARDIoGRAM_C4D_CAD_ADDITIVE__PM__(.*).csv\" \\\n--metaxcan_file_name_parse_pattern \"(.*)__PM__(.*).csv\" \\\n--gwas_file $DATA/spredixcan/imputed_CARDIoGRAM_C4D_CAD_ADDITIVE.txt.gz \\\n--snp_column panel_variant_id --effect_allele_column effect_allele --non_effect_allele_column non_effect_allele --zscore_column zscore --keep_non_rsid --model_db_snp_key varID \\\n--cutoff_condition_number 30 \\\n--verbosity 9 \\\n--throw \\\n--output $RESULTS/smultixcan/eqtl/CARDIoGRAM_C4D_CAD_ADDITIVE_smultixcan.txt"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#questionnaire-5",
    "href": "post/2022-06-10-qgt-training/index.html#questionnaire-5",
    "title": "Transcriptome QGT Training 2023",
    "section": "Questionnaire 5",
    "text": "Questionnaire 5\n\nOpen and start filling questionnaire 05 Colocalization https://forms.gle/NfH2MSdy4UyJzGAp7"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#run-colocalization",
    "href": "post/2022-06-10-qgt-training/index.html#run-colocalization",
    "title": "Transcriptome QGT Training 2023",
    "section": "Run colocalization",
    "text": "Run colocalization\nWhen you use coloc on your own data, you may want to check out coloc’s documentation, with good advice and tips on avoiding common mistakes https://cran.r-project.org/web/packages/coloc/vignettes\nDue to time constraints, we will run one region and one gene only\n\nFinemap GWAS of CAD\nFinemap eQTL of SORT1\n\nFor finemapping, we need the summary statistics (effect size, standard errors, etc) and the correlation between SNPs (LD matrix)"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#load-the-genotype-to-calculate-the-ld-matrix",
    "href": "post/2022-06-10-qgt-training/index.html#load-the-genotype-to-calculate-the-ld-matrix",
    "title": "Transcriptome QGT Training 2023",
    "section": "load the genotype to calculate the ld matrix",
    "text": "load the genotype to calculate the ld matrix\n\n# load the genotype to calculate the ld matrix\nX_mat <- BEDMatrix(glue::glue(\"{DATA}/colocalization/geuvadis_chr1\"))\ncolnames(X_mat) <- gsub(\"\\\\_.*\", \"\",colnames(X_mat))\ncolnames(X_mat) <- str_replace_all(colnames(X_mat),\":\",\"_\")\nsnp_info <- fread(glue::glue(\"{DATA}/colocalization/geuvadis_chr1.bim\")) %>% \n  setnames(., colnames(.), c(\"chr\", \"snp\", \"cm\", \"pos\", \"alt\", \"ref\")) \n\nsnp_info$snp <- str_replace_all(snp_info$snp,\":\",\"_\")"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#load-the-eqtl-and-gwas-effect-size-files",
    "href": "post/2022-06-10-qgt-training/index.html#load-the-eqtl-and-gwas-effect-size-files",
    "title": "Transcriptome QGT Training 2023",
    "section": "Load the eqtl and gwas effect size files",
    "text": "Load the eqtl and gwas effect size files\n\n# load the eqtl effect sizes\ngene_ss <- fread(glue::glue(\"{DATA}/colocalization/Liver_chr1.txt\"))\n\n# load gwas effect sizes\ngwas <- data.table::fread(glue::glue(\"{DATA}/spredixcan/imputed_CARDIoGRAM_C4D_CAD_ADDITIVE.txt.gz\"))\n\n# filter to select genome wide significant snps at 5 × 10−8\nfiltered_regions <- gwas %>% dplyr::filter(pvalue < 5e-8)\n\n# load the ld block\nldblocks <-read_tsv(glue::glue(\"{DATA}/spredixcan/eur_ld.hg38.txt.gz\"))"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#find-regions-with-the-strongest-signal-in-the-gwas",
    "href": "post/2022-06-10-qgt-training/index.html#find-regions-with-the-strongest-signal-in-the-gwas",
    "title": "Transcriptome QGT Training 2023",
    "section": "Find regions with the strongest signal in the gwas",
    "text": "Find regions with the strongest signal in the gwas\n\n# get the loci where the significant snps are located\nfor (n in 1:nrow(filtered_regions)){\n  # extract genename, start and end\n  variant_id <- as.character(filtered_regions[n,\"variant_id\"])\n  variant_chr <- as.character(filtered_regions[n,\"chromosome\"])\n  variant_pos <- as.numeric(filtered_regions[n,\"position\"])\n  #gene_end <- as.numeric(genes[n,\"end\"])\n\n  locus <- ldblocks %>%\n    dplyr::filter(chr == variant_chr) %>%\n    filter(variant_pos >= start & variant_pos < stop) %>%\n    mutate(locus_name = paste0(chr,\"_\",start,\"_\",stop)) %>%\n    dplyr::rename(locus_start=start,locus_end=stop) %>%\n    mutate(variant_id = variant_id, position=variant_pos)\n\n  # create a data frame with info\n  if (exists('all_loci') && is.data.frame(get('all_loci'))) {\n    all_loci <- rbind(all_loci,locus)\n  } else {\n    all_loci <- locus\n  }\n}\n\n# select uniq loci\nd_loci <- all_loci %>%\n  dplyr::select(locus_name,chr,locus_start,locus_end) %>%\n  dplyr::distinct()"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#select-a-region-to-run-coloc",
    "href": "post/2022-06-10-qgt-training/index.html#select-a-region-to-run-coloc",
    "title": "Transcriptome QGT Training 2023",
    "section": "select a region to run coloc",
    "text": "select a region to run coloc\n\n# select regions to fine map. we are going to use regions in chromosome 1\nuniq_loci <- d_loci %>% dplyr::filter(\"chr1\" == chr)\nn = 3 # chr1_107867043_109761309 region\n\n# extract information\nl_chr = as.numeric(str_remove(uniq_loci[n,\"chr\"],\"chr\"))\ns_chr = uniq_loci[n,]$chr\nl_start = uniq_loci[n,]$locus_start\nl_stop = uniq_loci[n,]$locus_end\nl_name = uniq_loci[n,]$locus_name"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#prepare-gwas-data-for-coloc",
    "href": "post/2022-06-10-qgt-training/index.html#prepare-gwas-data-for-coloc",
    "title": "Transcriptome QGT Training 2023",
    "section": "Prepare gwas data for coloc",
    "text": "Prepare gwas data for coloc\n\n# select snps for the region from the summary stats\nss <- gwas %>% \n  dplyr::filter(chromosome == s_chr) %>%\n  dplyr::filter(position >= l_start & position <= l_stop) %>% \n  dplyr::filter(! is.na(effect_size))\n\n# find the snps in the genotype to calculate the correlation\ng.snps <- ss %>% inner_join(snp_info %>% mutate(chr = glue::glue(\"chr{chr}\")), \n                            by=c(\"chromosome\" = \"chr\",\"panel_variant_id\" = \"snp\"))\n\n\n# select genotype to calculate correlation\n#f_mat <- X_mat[,g.snps$snp]\nf_mat <- X_mat[,g.snps$panel_variant_id]\n\n# calculate corr\nR = cora(f_mat) # the package is for speed\n\n## clean up\nrm(f_mat)\n\nff <- g.snps %>% dplyr::filter(! is.na(effect_size)) %>% #select(-snp) %>% \n  dplyr::rename(snp=panel_variant_id,beta=effect_size) %>% \n  mutate(varbeta = standard_error^2) %>% \n  dplyr::select(beta,varbeta,snp,position) %>% as.list()\n\nff$type <- \"cc\"\nff$sdY <- 1\n\nff$LD = R\nff$N = 184305\n\n## check the data (NULL means it's fine)\ncheck_dataset(ff)"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#prepare-eqtl-data-for-coloc",
    "href": "post/2022-06-10-qgt-training/index.html#prepare-eqtl-data-for-coloc",
    "title": "Transcriptome QGT Training 2023",
    "section": "Prepare eqtl data for coloc",
    "text": "Prepare eqtl data for coloc\n\n#Using SORT1 gene and liver tissue\ngene <- gene_ss %>% dplyr::filter(gene_id == \"ENSG00000134243.11\") %>% \n  dplyr::rename(snp = variant_id,beta = slope, MAF = maf,\n                pvalue = pval_nominal) %>% \n  mutate(varbeta = slope_se^2, name = snp) %>% \n  filter(! is.na(varbeta)) %>% \n  separate(name, into = c(\"chr\", \"position\",\"ref\",\"alt\",\"build\"),sep = \"_\")\n\n## calculate the ld matrix\n### get the snps\ngene.snps <- gene %>% mutate(position = as.integer(position)) %>% \n  inner_join(snp_info %>% mutate(chr = glue::glue(\"chr{chr}\")), \n                              by=c(\"chr\" = \"chr\",\"snp\" = \"snp\"))\n\n# select genotype to calculate correlation\ng_mat <- X_mat[,gene.snps$snp]\n# calculate corr\ng.R = cora(g_mat)\n\n# clean up\nrm(g_mat)\n\n# format data for coloc\ngg <- gene %>% dplyr::filter(snp %in% gene.snps$snp) %>%\n  mutate(position = as.integer(position)) %>% \n  dplyr::select(beta,varbeta,snp,position,MAF, pvalue) %>% as.list()\n\ngg$type <- \"quant\"\ngg$LD = g.R\ngg$N = 208 # 670 for blood\n\n## check the data\ncheck_dataset(gg)"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#run-older-version-of-coloc-which-assumes-single-causal-variant",
    "href": "post/2022-06-10-qgt-training/index.html#run-older-version-of-coloc-which-assumes-single-causal-variant",
    "title": "Transcriptome QGT Training 2023",
    "section": "run older version of coloc which assumes single causal variant",
    "text": "run older version of coloc which assumes single causal variant\ncoloc.abf makes the simplifying assumption that each trait has at most one causal variant in the region under consideration\n\nmy.res <- coloc.abf(dataset1=ff, dataset2=gg)\nsensitivity(my.res,\"H4 > 0.9\")"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#run-coloc-allowing-multiple-causal-variants",
    "href": "post/2022-06-10-qgt-training/index.html#run-coloc-allowing-multiple-causal-variants",
    "title": "Transcriptome QGT Training 2023",
    "section": "run coloc allowing multiple causal variants",
    "text": "run coloc allowing multiple causal variants\nMultiple causal variants, using SuSiE to separate the signals\n\n# Run susie fine maooing\nS3 = runsusie(ff)\nS4 = runsusie(gg)\n#summary(S3)\n\n# Run coloc\nsusie.res=coloc.susie(S3,S4)\nprint(susie.res$summary)\n\nSuSiE can take a while to run on larger datasets, so it is best to run once per dataset with the =runsusie= function, store the results and feed those into subsequent analyses.\nplot the coloc result with the sensitivity function because weird effects are much easier to understand visually\n\nsensitivity(susie.res,\"H4 > 0.9\",row=1,dataset1=ff,dataset2=gg,)"
  },
  {
    "objectID": "post/2022-06-10-qgt-training/index.html#questionnaire-06",
    "href": "post/2022-06-10-qgt-training/index.html#questionnaire-06",
    "title": "Transcriptome QGT Training 2023",
    "section": "Questionnaire 06",
    "text": "Questionnaire 06\n\nOpen and start filling out questionnaire 06 cTWAS https://forms.gle/A4evWkbhR7cXLy36A\n\n\n#install.packages(\"R.utils\")\n\n#install.packages(\"remotes\")\n#remotes::install_github(\"simingz/ctwas\", ref = \"develop\")\n\nlibrary(ctwas)\n\n#get positions for region of interest (SORT1/PSRC1 locus)\nregion <- unlist(strsplit(spredixcan_association$region_id[spredixcan_association$gene_name==\"PSRC1\"], \"_\"))\nchr <- region[2]\nstart <- as.numeric(region[3])\nend <- as.numeric(region[4])\n\n#format summary statistics (and subset to variants in region to save memory)\nz_snp <- data.table::fread(glue::glue(\"{DATA}/spredixcan/imputed_CARDIoGRAM_C4D_CAD_ADDITIVE.txt.gz\"), select=c(\"chromosome\", \"position\", \"variant_id\", \"effect_allele\", \"non_effect_allele\", \"zscore\", \"sample_size\"))\nz_snp <- z_snp[z_snp$chromosome==chr & z_snp$position >= start & z_snp$position <= end,]\nz_snp <- z_snp[!is.na(z_snp$variant_id),-(1:2)]\ncolnames(z_snp) <- c(\"id\", \"A1\", \"A2\", \"z\", \"ss\")\n\n#specify directories for LD matrices and weights\nld_R_dir <- glue::glue(\"{DATA}/cTWAS/LD_matrices\")\nweight <-  glue::glue(\"{MODEL}/gtex_v8_mashr/mashr_Liver.db\")\n\n#specify output locations and names for cTWAS\noutputdir <- glue::glue(\"{DATA}/cTWAS/results/\")\noutname.e <- \"CARDIoGRAM_Liver_expr\"\noutname <- \"CARDIoGRAM_Liver_ctwas\"\n\n#impute gene z scores using cTWAS and save the results\n##################################\n# NOTE: we are skipping this step and using the precomputed values \n## takes ~10 minutes\n##################################\n# ctwas_imputation <- impute_expr_z(z_snp=z_snp, weight=weight, ld_R_dir=ld_R_dir, outputdir=outputdir, outname=outname.e, harmonize_z=F, harmonize_wgt=F)\n# save(ctwas_imputation, file = paste0(outputdir, outname.e, \"_output.Rd\"))\nload(paste0(outputdir, outname.e, \"_output.Rd\"))\n\nz_gene <- ctwas_imputation$z_gene\nld_exprfs <- ctwas_imputation$ld_exprfs\nz_snp <- ctwas_imputation$z_snp\n\n#make custom region file for single region\nld_regions_custom <- data.frame(\"chr\" = chr, \"start\" = start, \"stop\" = end)\n\nwrite.table(ld_regions_custom, \n            file= paste0(outputdir, \"ld_regions_custom.txt\"),\n            row.names=F, col.names=T, sep=\"\\t\", quote = F)\n    \nld_regions_custom <- paste0(outputdir, \"ld_regions_custom.txt\")\n\n#run cTWAS with pre-specified prior parameters at a single locus\n#estimating prior requires genome-wide data, too slow for demonstration\n#prior is 1% inclusion for genes and is 100x more likely than SNPs\n#prior assumes genes have larger effect size than SNPs, in reasonable range for data we've looked at\nctwas_rss(z_gene=z_gene, z_snp=z_snp, ld_exprfs=ld_exprfs, ld_R_dir = ld_R_dir, ld_regions_custom=ld_regions_custom, outputdir = outputdir, outname = outname, thin = 0.01,\n          estimate_group_prior = F,\n          estimate_group_prior_var = F,\n          group_prior=c(0.01, 0.0001),\n          group_prior_var=c(50, 25))\n\n#load results\nctwas_results <- data.table::fread(paste0(outputdir,outname,\".susieIrss.txt\"))\n\n#merge gene names into the results\nsqlite <- RSQLite::dbDriver(\"SQLite\")\ndb = RSQLite::dbConnect(sqlite, weight)\nquery <- function(...) RSQLite::dbGetQuery(db, ...)\nextra_table <- query(\"select * from extra\")\nRSQLite::dbDisconnect(db)\n\nctwas_results$genename <- extra_table$genename[match(ctwas_results$id, extra_table$gene)]\n\n#show results with highest PIPs\ncol_order_2= c(\"genename\", \"chrom\", \"id\", \"pos\", \"type\", \"region_tag1\", \"region_tag2\", \"cs_index\", \"susie_pip\" , \"mu2\")\nctwas_results <- ctwas_results[, ..col_order_2]\nhead(ctwas_results[order(-ctwas_results$susie_pip),])"
  },
  {
    "objectID": "post/2020-07-30-how-to-submit-to-zenodo/index.html",
    "href": "post/2020-07-30-how-to-submit-to-zenodo/index.html",
    "title": "How to Submit to Zenodo",
    "section": "",
    "text": "How to submit data to Zenodo"
  },
  {
    "objectID": "post/2020-07-30-how-to-submit-to-zenodo/index.html#using-the-web-inferface",
    "href": "post/2020-07-30-how-to-submit-to-zenodo/index.html#using-the-web-inferface",
    "title": "How to Submit to Zenodo",
    "section": "using the web inferface",
    "text": "using the web inferface\n\nlog in with Github or ORDID to zenodo.org\nupload files, fill out the form with authors, affiliations, ORCID (optional) and upload files\npublish"
  },
  {
    "objectID": "post/2020-07-30-how-to-submit-to-zenodo/index.html#programmatic-upload",
    "href": "post/2020-07-30-how-to-submit-to-zenodo/index.html#programmatic-upload",
    "title": "How to Submit to Zenodo",
    "section": "programmatic upload",
    "text": "programmatic upload\nSee Yanyu’s instructions https://github.com/hakyimlab/Yanyus-misc-tools/tree/master/zenodo_upload"
  },
  {
    "objectID": "post/2023-02-21-power-calculator-for-mol-qtls/index.html",
    "href": "post/2023-02-21-power-calculator-for-mol-qtls/index.html",
    "title": "Power calculator for mol QTLs",
    "section": "",
    "text": "The success of the prediction training depends mostly on whether the corresponding QTL study will be powered. Thus, we provide here the power to detect molecular QTLs for a range of sample sizes, effect sizes, and minor allele frequencies."
  },
  {
    "objectID": "post/2023-02-21-power-calculator-for-mol-qtls/index.html#function-r2-from-beta-assuming-variance-of-y-1",
    "href": "post/2023-02-21-power-calculator-for-mol-qtls/index.html#function-r2-from-beta-assuming-variance-of-y-1",
    "title": "Power calculator for mol QTLs",
    "section": "function: R2 from beta assuming variance of y = 1",
    "text": "function: R2 from beta assuming variance of y = 1\n\\[y = \\delta \\cdot x + \\epsilon\\]\n\\[r^2 = \\delta^2 \\cdot \\text{var}(x) = \\delta^2 \\cdot 2 \\cdot \\text{maf} \\cdot (1-\\text{maf})\\]\n\n\nShow the code\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(knitr))\n## install.packages(\"pwr\")\n#if (!(\"pwr\" %in% installed.packages()[, 1])) {\n#  install.packages(\"pwr\")\n#}"
  },
  {
    "objectID": "post/2023-02-21-power-calculator-for-mol-qtls/index.html#define-ranges-of-maf-eff-sample-sizes",
    "href": "post/2023-02-21-power-calculator-for-mol-qtls/index.html#define-ranges-of-maf-eff-sample-sizes",
    "title": "Power calculator for mol QTLs",
    "section": "define ranges of maf, eff, sample sizes",
    "text": "define ranges of maf, eff, sample sizes\n\n\nShow the code\nmafvec = c(0.05, 0.10, 0.30) \neffvec = c(0.40, 0.60, 0.80) \nnvec = c(200,350,500) \nnsnps = 1000\nalpha = 0.05/nsnps"
  },
  {
    "objectID": "post/2023-02-21-power-calculator-for-mol-qtls/index.html#create-data-frame-with-all-combinations",
    "href": "post/2023-02-21-power-calculator-for-mol-qtls/index.html#create-data-frame-with-all-combinations",
    "title": "Power calculator for mol QTLs",
    "section": "create data frame with all combinations",
    "text": "create data frame with all combinations\n\n\nShow the code\n# mat = matrix(NA,length(mafvec)*length(effvec)*length(nvec),4)\n# colnames(mat) = c(\"maf\",\"eff\",\"nsam\",\"power\")\n# cont = 1\n# for(maf in mafvec)\n# {\n#   for(nn in nvec)\n#   {\n#     for(eff in effvec)\n#     {\n#       r2 = eff^2 * 2 * maf * (1-maf)\n#       rr = sqrt(r2)\n#       pp = pwr::pwr.r.test(n = nn, r= rr , sig.level = alpha)\n#       mat[cont,] = c(maf, eff, nn, pp$power)\n#       cont = cont + 1\n#     }\n#   }\n# }\n# \n# mat %>% data.frame() %>% \n#   pivot_wider(names_from = nsam, values_from = power) %>% \n#   mutate(across(3:ncol(.), ~sprintf(\"%.1f%%\", . * 100))) %>% \n#   kable(format = \"markdown\", align = c(\"l\", \"l\", \"c\", \"c\", \"c\"), \n#         caption = \"Power by sample size\")"
  },
  {
    "objectID": "post/2023-02-21-power-calculator-for-mol-qtls/index.html#calculate-detectable-effect-sizes",
    "href": "post/2023-02-21-power-calculator-for-mol-qtls/index.html#calculate-detectable-effect-sizes",
    "title": "Power calculator for mol QTLs",
    "section": "calculate detectable effect sizes",
    "text": "calculate detectable effect sizes\nThe following table shows the detectable effect sizes at 80% power with a significance level of 0.05/1000. Variance of Y is standardized to 1.\n\n\nShow the code\ncalc_mateff = function(mafvec,nvec,outmat=FALSE)\n{\n  mateff = matrix(NA,length(mafvec)*length(nvec),3)\ncolnames(mateff) = c(\"maf\",\"nsam\",\"power\")\ncont = 1\nfor(maf in mafvec)\n{\n  for(nn in nvec)\n  {\n      # r2 = eff^2 * 2 * maf * (1-maf)\n      # rr = sqrt(r2)\n      pp = pwr::pwr.r.test(n = nn, power=0.80 , sig.level = alpha)\n      eff = pp$r/sqrt(2*maf*(1-maf))\n      mateff[cont,] = c(maf,  nn, eff)\n      cont = cont + 1\n  }\n}\n  mateff = mateff %>% data.frame() %>% \n  pivot_wider(names_from = nsam, values_from = power) %>% \n  mutate_at(vars(-c(1)), ~round(., 2)) \n  print(mateff)\n  if(outmat) mateff\n} \n\nmafvec = c(0.05, 0.10, 0.30) \nnvec = c(100,500,1000,7000) \nnsnps = 1000\nalpha = 0.05/nsnps\n## ---\nmat = calc_mateff(mafvec,nvec,outmat=TRUE)\n\n\n# A tibble: 3 × 5\n    maf `100` `500` `1000` `7000`\n  <dbl> <dbl> <dbl>  <dbl>  <dbl>\n1  0.05  1.5   0.7    0.5    0.19\n2  0.1   1.09  0.51   0.36   0.14\n3  0.3   0.71  0.33   0.24   0.09\n\n\nShow the code\nmat %>%  kable(format = \"markdown\", align = c(\"l\", \"c\", \"c\", \"c\", \"c\"), caption = \"Detectable effects w/1000 SNPs\",label=NA) \n\n\n\nDetectable effects w/1000 SNPs\n\n\nmaf\n100\n500\n1000\n7000\n\n\n\n\n0.05\n1.50\n0.70\n0.50\n0.19\n\n\n0.10\n1.09\n0.51\n0.36\n0.14\n\n\n0.30\n0.71\n0.33\n0.24\n0.09\n\n\n\n\n\nShow the code\nmafvec = c(0.05, 0.10, 0.30) \nnvec = c(1000,6000,10000,100000) \nnsnps = 1e6\nalpha = 0.05/nsnps\n## ---\nmat = calc_mateff(mafvec,nvec,outmat=TRUE)\n\n\n# A tibble: 3 × 5\n    maf `1000` `6000` `10000` `1e+05`\n  <dbl>  <dbl>  <dbl>   <dbl>   <dbl>\n1  0.05   0.64   0.26    0.2     0.06\n2  0.1    0.46   0.19    0.15    0.05\n3  0.3    0.3    0.13    0.1     0.03\n\n\nShow the code\nmat %>%  kable(format = \"markdown\", align = c(\"l\", \"c\", \"c\", \"c\", \"c\"), caption = \"Detectable effects w/1e6 SNPs\",label=NA) \n\n\n\nDetectable effects w/1e6 SNPs\n\n\nmaf\n1000\n6000\n10000\n1e+05\n\n\n\n\n0.05\n0.64\n0.26\n0.20\n0.06\n\n\n0.10\n0.46\n0.19\n0.15\n0.05\n\n\n0.30\n0.30\n0.13\n0.10\n0.03"
  },
  {
    "objectID": "post/2023-02-21-power-calculator-for-mol-qtls/index.html#power-for-xcan-association-continuous-x",
    "href": "post/2023-02-21-power-calculator-for-mol-qtls/index.html#power-for-xcan-association-continuous-x",
    "title": "Power calculator for mol QTLs",
    "section": "power for Xcan association (continuous X)",
    "text": "power for Xcan association (continuous X)\n\n\nShow the code\nnvec = c(10000,100000,500000) \nntests = 10000\nalpha = 0.05/ntests\n\ncalc_matr = function(nvec,outmat=FALSE)\n{\n  mateff = matrix(NA,length(nvec),2)\n  colnames(mateff) = c(\"nsam\",\"r\")\n  cont = 1\n  for(nn in nvec)\n  {\n    pp = pwr::pwr.r.test(n = nn, power=0.80 , sig.level = alpha)\n    mateff[cont,] = c(nn, pp$r)\n    cont = cont + 1\n  }\n  mateff = mateff %>% data.frame() %>%  mutate(r2 = r^2) %>% mutate_at(vars(-c(1)), ~signif(., 2)) \n  print(mateff)\n  if(outmat) mateff\n} \n\ncalc_matr(nvec,outmat=TRUE) %>% knitr::kable()\n\n\n   nsam      r      r2\n1 1e+04 0.0540 2.9e-03\n2 1e+05 0.0170 2.9e-04\n3 5e+05 0.0077 5.9e-05\n\n\n\n\n\nnsam\nr\nr2\n\n\n\n\n1e+04\n0.0540\n2.9e-03\n\n\n1e+05\n0.0170\n2.9e-04\n\n\n5e+05\n0.0077\n5.9e-05\n\n\n\n\n\nFor the *-Xcan analysis, sample sizes ranging from 10,000 to 500,000 will detect omic traits explaining 0.29% to 0.0059% of the total variation of the phenotype with a power of 80%."
  },
  {
    "objectID": "post/2023-03-28-cistromedb-data/index.html",
    "href": "post/2023-03-28-cistromedb-data/index.html",
    "title": "Cistrome DB data",
    "section": "",
    "text": "suppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\n##PRE=\"/Users/margaretperry/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data \"\n##PRE=\"/Users/temi/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data\"\n## COPY THE DATE AND SLUG fields FROM THE HEADER\nSLUG=\"cistromedb-data\" ## copy the slug from the header\nbDATE='2023-03-28' ## copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n\n## move data to DATA\n#tempodata=(\"~/Downloads/tempo/gwas_catalog_v1.0.2-associations_e105_r2022-04-07.tsv\")\n#system(glue::glue(\"cp {tempodata} {DATA}/\"))\n\n## system(glue(\"open {DATA}\")) ## this will open the folder \n\n\ndata = read_tsv(glue(\"{DATA}/human_factor_full_QC.txt\"))\n\nRows: 11348 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (6): Species, GSMID, Factor, Cell_line, Cell_type, Tissue_type\ndbl (7): DCid, FastQC, UniquelyMappedRatio, PBC, PeaksFoldChangeAbove10, FRi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnames(data)\n\n [1] \"DCid\"                   \"Species\"                \"GSMID\"                 \n [4] \"Factor\"                 \"Cell_line\"              \"Cell_type\"             \n [7] \"Tissue_type\"            \"FastQC\"                 \"UniquelyMappedRatio\"   \n[10] \"PBC\"                    \"PeaksFoldChangeAbove10\" \"FRiP\"                  \n[13] \"PeaksUnionDHSRatio\"    \n\ndata %>% select(Factor,Cell_line,Cell_type,Tissue_type) %>% unique() %>% dim()\n\n[1] 4426    4\n\ndata %>% count(Factor,Cell_line,Cell_type,Tissue_type) %>% arrange(desc(n))\n\n# A tibble: 4,426 × 5\n   Factor Cell_line Cell_type  Tissue_type     n\n   <chr>  <chr>     <chr>      <chr>       <int>\n 1 ESR1   MCF-7     Epithelium Breast        213\n 2 AR     LNCaP     Epithelium Prostate      143\n 3 POLR2A HeLa      Epithelium Cervix         76\n 4 AR     VCaP      Epithelium Prostate       64\n 5 POLR2A MCF-7     Epithelium Breast         64\n 6 NR3C1  A549      Epithelium Lung           46\n 7 POLR2A HCT-116   None       HCT116         46\n 8 CTCF   MCF-7     Epithelium Breast         45\n 9 FOXA1  LNCaP     Epithelium Prostate       45\n10 ESR1   None      None       Breast         42\n# ℹ 4,416 more rows\n\ndata %>% count(Factor,Cell_line,Cell_type,Tissue_type) %>% .[[\"n\"]] %>% table()\n\n.\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n2223 1196  299  282   89  106   36   43   18   28   12   17    4    8    7    6 \n  17   18   19   20   22   23   24   25   26   28   29   30   31   32   33   34 \n   1    8    3    2    1    1    2    3    2    1    1    3    2    2    3    3 \n  37   38   40   42   45   46   64   76  143  213 \n   2    1    1    1    2    2    2    1    1    1 \n\n## are cell line==None non tumor?\ndata %>% filter(Cell_line==\"None\") %>% dim()\n\n[1] 1817   13\n\n## how many unique cell lines?\ndata %>% count(Cell_line) %>% dim()\n\n[1] 520   2\n\n## how many unique cell types?\ndata %>% count(Cell_type) %>% dim()\n\n[1] 153   2\n\n## how many unique tissue types?\ndata %>% count(Tissue_type) %>% dim()\n\n[1] 84  2"
  },
  {
    "objectID": "post/2022-06-14-transcriptome-qgt-lab-2022-setup/index.html",
    "href": "post/2022-06-14-transcriptome-qgt-lab-2022-setup/index.html",
    "title": "Transcriptome QGT Lab 2022 Setup",
    "section": "",
    "text": "The instructions in this blog were written to set up the lab in Rstudio cloud"
  },
  {
    "objectID": "post/2022-06-14-transcriptome-qgt-lab-2022-setup/index.html#setting-up-your-own-system",
    "href": "post/2022-06-14-transcriptome-qgt-lab-2022-setup/index.html#setting-up-your-own-system",
    "title": "Transcriptome QGT Lab 2022 Setup",
    "section": "Setting up your own system",
    "text": "Setting up your own system\nLinux is the operating system of choice to run bioinformatics software. You will need either a computer running linux or or mac os, which has a linux-like environment.\n\ninstall anaconda/miniconda\ndefine imlabtools conda environment how to here, which will install all the python modules needed for this analysis session\ndownload data and software from Box. This will have copies of all the software repositories and the models\ndownload software\ndownload metaxcan repo\n\ndownload prediction models from predictdb.org\ninstall R/RStudio/tidyverse package\ninstall R packages\ngit clone https://github.com/hakyimlab/QGT-Columbia-HKI.git\nstart Rstudio (if you installed workflowr, you can just open the QGT-Columbia-HKI.Rproj)"
  },
  {
    "objectID": "post/2020-11-13-creating-environments-in-cri/index.html",
    "href": "post/2020-11-13-creating-environments-in-cri/index.html",
    "title": "Creating Environments in CRI",
    "section": "",
    "text": "Create the new environment in the lab share\n\nconda create --prefix /gpfs/data/im-lab/nas40t2/bin/envs/env_name \n\nActivate the new environment\n\nconda activate env_name\n\nInstall whatever you want in your new environment. Also because the environment is in the labshare, anyone can use it just by calling\n\nconda install <software>\nconda activate /gpfs/data/im-lab/nas40t2/bin/envs/env_name"
  },
  {
    "objectID": "post/2020-11-17-installing-running-tensorqtl-on-cri/index.html",
    "href": "post/2020-11-17-installing-running-tensorqtl-on-cri/index.html",
    "title": "Installing/Running Tensorqtl on CRI",
    "section": "",
    "text": "Note: You can download tensorqtl using pip install. However, there seems to be a bug that makes tensorqtl incompatible with pandas plink 2.2.2. If you want to download tensorqtl using pip, then you have to downgrade pandas plink to 2.1.x so that it matches the version of pandas 1.1.x\nI installed tensorqtl straight from the Github repo https://github.com/broadinstitute/tensorqtl\n\nActivate the environment\n\n\n\nShow the code\nconda activate /gpfs/data/im-lab/nas40t2/bin/envs/tensorqtl\n\n\n\nClone the repo into your own directory in the labshare\n\n\n\nShow the code\ngit clone git@github.com:broadinstitute/tensorqtl.git\ncd tensorqtl\n\n\n\nInstall tensorqtl and its requirements\n\n\n\nShow the code\npip install -r install/requirements.txt\n\n\n\nRun Tensorqtl\n\nRequirements: Tensorqtl requires a genotype, phenotype and covariate file. The genotype files must be in plink format. The phenotype file must be in a .bed.gz format and follow the UCSC bed formate (http://fastqtl.sourceforge.net) Finally, the covariate file must be in a .txt format and is in the setup covariates x samples.\nAlso make sure to set column names true for both the phenotype and covariate files. (Row names must also be present only for the covariate file)\nIt’s also helpful to look at the example data provided by the repo.\nThis is the command I used to run tensorqtl. I ran a trans-qtl so start and end positions on the genes were not significant.\n\n\nShow the code\npython3 -m tensorqtl /gpfs/data/im-lab/nas40t2/Data/GTEx/V8/genotype/plink_files/GTEX_tensorqtl /gpfs/data/im-lab/nas40t2/natasha/tensorqtl/pheno-tensorqtl.bed.gz /gpfs/data/im-lab/nas40t2/natasha/GTEX_tensorqtl \\\n--covariates /gpfs/data/im-lab/nas40t2/natasha/tensorqtl/covariates.txt  \\\n--mode trans \n\n\nThe command above will generate a parquet file in wherever you set the prefix to. The file can be read using pandas\n\n\nShow the code\nimport pandas as pd\ndf = pd.read_parquet(\"<path_to_filename>\")"
  },
  {
    "objectID": "post/2021-07-12-map-cri-storage-to-your-computer/index.html",
    "href": "post/2021-07-12-map-cri-storage-to-your-computer/index.html",
    "title": "Mount Gardner file stystem to your computer",
    "section": "",
    "text": "This page contains description on how to map CRI storage to your computer. Depending on the operating system you are using there are different approaches. Which are shown below;"
  },
  {
    "objectID": "post/2021-07-12-map-cri-storage-to-your-computer/index.html#mac-os-users",
    "href": "post/2021-07-12-map-cri-storage-to-your-computer/index.html#mac-os-users",
    "title": "Mount Gardner file stystem to your computer",
    "section": "Mac os users",
    "text": "Mac os users\n\nfrom Finder, click ‘Go’\nthen click ‘connect to server’\nthen connect to smb://prfs.cri.uchicago.edu/im-lab"
  },
  {
    "objectID": "post/2021-07-12-map-cri-storage-to-your-computer/index.html#linux-users",
    "href": "post/2021-07-12-map-cri-storage-to-your-computer/index.html#linux-users",
    "title": "Mount Gardner file stystem to your computer",
    "section": "Linux users",
    "text": "Linux users\n\nInstall sshfs\n\nsudo apt-get install sshfs \n\nCreate the mount point\n\n# Create the mountpoint\n[festus@ubuntu ~ ]$ mkdir ~/im-lab\n\nInvoke sshfs with your credentials\n\n# Invoke SSHFS with your SSH credentials and the remote location to mount\n[festus@ubuntu ~ ]$ sshfs t.cri.fnyasimi@gardner.cri.uchicago.edu:/gpfs/data/im-lab ~/im-lab\n\n# If you have set up the ssh config you can use it to mount;\n[festus@ubuntu ~ ]$ sshfs gardner:/gpfs/data/im-lab ~/im-lab\n\n# Access the mounted filesystem\n[festus@ubuntu ~ ]$ ls ~/im-lab/\nnas40t2\n\nUnmount the filesystem\n\n# Unmount the remote FS\n[festus@ubuntu ~ ]$ fusermount -u ~/im-lab\nNB: sshfs doesn’t expand ~ on a remote machine to the user’s home directory."
  },
  {
    "objectID": "post/2021-07-12-map-cri-storage-to-your-computer/index.html#windows-users",
    "href": "post/2021-07-12-map-cri-storage-to-your-computer/index.html#windows-users",
    "title": "Mount Gardner file stystem to your computer",
    "section": "Windows users",
    "text": "Windows users\nYou need to install two programmes, After the installation, you can mount/map directories from any servers that providing SSH connection service to Windows as a network drive.\n\nInstall two programs - WinFsp and SSHFS-Win.\nClicks mouse right button on “This PC” in file explorer, selects “Map network drive…” on pop menu.\nSelect drive letter and replaces username and server host name to yours.\nInput your authoritative credentials\nIf success, you will see the drive letter mapped to the folder of your remote ssh server. You’re ready to access and manipulate remote files and folders now."
  },
  {
    "objectID": "post/2021-07-12-map-cri-storage-to-your-computer/index.html#note",
    "href": "post/2021-07-12-map-cri-storage-to-your-computer/index.html#note",
    "title": "Mount Gardner file stystem to your computer",
    "section": "Note",
    "text": "Note\nThe sshfs system can also be used on Mac os and windows os."
  },
  {
    "objectID": "post/2020-07-30-how-to-use-workflowr/index.html",
    "href": "post/2020-07-30-how-to-use-workflowr/index.html",
    "title": "How to Use Workflowr",
    "section": "",
    "text": "#Open RStudio\n#Type the following:\nnameproject = “PNPO-Alcohol-Epilepsy” githubuser = “hakyimlab”\nlibrary(workflowr) setwd(“~/Github”)\nwflow_git_config(user.name = “Your Name”, user.email = “email@domain”)\nwflow_start(nameproject) wflow_build() wflow_view() wflow_status() wflow_publish(c(“analysis/index.Rmd”, “analysis/about.Rmd”, “analysis/license.Rmd”), “Publish the initial files for myproject”)\n#Create a GitHub repository with the same name as the nameproject\nwflow_use_github(githubuser, nameproject)\n#select 2 since the repo has already been created\n#push the content of the new repo to GitHub\ngit push –set-upstream origin master"
  },
  {
    "objectID": "post/2021-06-10-how-to-use-cri-cluster/index.html",
    "href": "post/2021-06-10-how-to-use-cri-cluster/index.html",
    "title": "How to use CRI cluster",
    "section": "",
    "text": "Type ssh username@gardner.cri.uchicago.edu\nType in your password when prompted\nType yes if you are prompted to accept a key\n\nNote: A shortcut to login your tarbell account, please follow the following instruction:  How-to-set-up-SSH-keys-and-configure-custom-connection-options-for-your-SSH-Client"
  },
  {
    "objectID": "post/2021-06-10-how-to-use-cri-cluster/index.html#accessing-lab-share-resources",
    "href": "post/2021-06-10-how-to-use-cri-cluster/index.html#accessing-lab-share-resources",
    "title": "How to use CRI cluster",
    "section": "Accessing lab-share resources",
    "text": "Accessing lab-share resources\nPlease follow the instructions to access /gpfs/data/im-lab/ from the University of Chicago’s Network: - Open Finder - Go to menu option Go -> Connect to Server - Type smb://bulkstorage.uchicago.edu/im-lab - Enter your BSD username and password. Make sure to prefix your username with ADLOCAL\\ if from off-campus"
  },
  {
    "objectID": "post/2021-06-10-how-to-use-cri-cluster/index.html#storing-your-data",
    "href": "post/2021-06-10-how-to-use-cri-cluster/index.html#storing-your-data",
    "title": "How to use CRI cluster",
    "section": "Storing your data",
    "text": "Storing your data\nHome directory/Quota: /home/<userID> (10G) - use for temporary personal files such scripts to execute analysis, source code, etc. Data stored here is not backed up. Move important data to lab share.\nFast Scratch Space: Temporary storage. /scratch/im-lab directory on the HPC is used to stage input that is used in analysis jobs as well as temp files from job execution. Scratch data older than 14 days is automatically purged and outputs should be copied out to your lab share if you intend to keep job output.\nLab-Share: /group/im-lab Long term storage. Use to store lab data or persistent job outputs. Copy data you intend to keep out of scratch into this space. Do not run jobs out of here. It is not optimized for job execution. Please perform regular maintenance on the lab share by removing unneeded data. If you no longer needs access to the Lab Share, please fill  the revoke resource access form \n\nThe /scratch area is used like any other area on the filesystem. It differs by lab shares in the following ways:\n\nLab shares (/group/im-lab) are stored on an Isilon cluster and the data is backed up to tape on a daily schedule.\nLab shares have a cost associated if you provision a certain amount of space.\nLab shares are more of a general purpose filesystem which is not tuned to specifically handle HPC workflows.\nThere is no quota on /scratch or cost associated with it.\nThe data on /scratch is not backed up and should only be stored there on a temporary basis.\nScratch is tuned to handle large files, but will perform poorly if you have a lot of small files.\n\nHow I would use scratch space is that I would stage large files there before running my jobs through the scheduler and I would also set up a working directory there while the job is executing.\nPlease contact storage@rt.cri.uchicago.edu to get your own scratch area in tarbell."
  },
  {
    "objectID": "post/2021-06-10-how-to-use-cri-cluster/index.html#common-module-commands",
    "href": "post/2021-06-10-how-to-use-cri-cluster/index.html#common-module-commands",
    "title": "How to use CRI cluster",
    "section": "Common module commands",
    "text": "Common module commands\n\nmodule avail\n\nmodule list\n\nmodule load python/2.7.9 (first load module load gcc/7.3.0)\nmodule spider python (to search all modules)\nmodule clear"
  },
  {
    "objectID": "post/2021-06-10-how-to-use-cri-cluster/index.html#create-job-submission-script",
    "href": "post/2021-06-10-how-to-use-cri-cluster/index.html#create-job-submission-script",
    "title": "How to use CRI cluster",
    "section": "Create job submission script",
    "text": "Create job submission script\n\nemacs run_metaXcan.pbs\n\n#!/bin/bash \n#################################\n## Resource Manager Directives ##\n#################################\n### Set the name of the job\n#PBS -N job_0_5_1\n### Select the shell you would like to script to execute within.\n#PBS -S /bin/bash\n### walltime=HH:MM:SS\n#PBS -l walltime=1:00:00\n### Inform the scheduler of the number of CPU cores for your job.\n#PBS -l nodes=1:ppn=1\n### Inform the scheduler of the amount of memory you expect \n#PBS -l mem=4gb\n### Set the destination for your program's output: stdout and stderr. \n#PBS -o logs_0_5/${PBS_JOBNAME}.o${PBS_JOBID}.log\n#PBS -e logs_0_5/${PBS_JOBNAME}.e${PBS_JOBID}.err\n\n###################\n## Job Execution ##\n###################\n# The program to be executed \n\n./MetaXcan.py \\\n--beta_folder intermediate/beta \\\n--weight_db_path data/DGN-WB_0.5.db \\\n--covariance intermediate/cov/covariance.txt.gz \\\n--gwas_folder data/GWAS \\\n--gwas_file_pattern \".*gz\" \\\n--compressed \\\n--beta_column BETA \\\n--pvalue_column P \\\n--output_file results/test.csv"
  },
  {
    "objectID": "post/2021-06-10-how-to-use-cri-cluster/index.html#submit-jobs",
    "href": "post/2021-06-10-how-to-use-cri-cluster/index.html#submit-jobs",
    "title": "How to use CRI cluster",
    "section": "Submit jobs",
    "text": "Submit jobs\n\nqsub run_metaXcan.pbs"
  },
  {
    "objectID": "post/2021-06-10-how-to-use-cri-cluster/index.html#monitor-job-status",
    "href": "post/2021-06-10-how-to-use-cri-cluster/index.html#monitor-job-status",
    "title": "How to use CRI cluster",
    "section": "Monitor job status",
    "text": "Monitor job status\n\nqstat\nshowq"
  },
  {
    "objectID": "post/2021-06-10-how-to-use-cri-cluster/index.html#interrupt-running-jobs-after-submission",
    "href": "post/2021-06-10-how-to-use-cri-cluster/index.html#interrupt-running-jobs-after-submission",
    "title": "How to use CRI cluster",
    "section": "Interrupt running jobs after submission",
    "text": "Interrupt running jobs after submission\n\nqdel job_id\nqdel all"
  },
  {
    "objectID": "post/2021-06-10-how-to-use-cri-cluster/index.html#batch-job-submissions",
    "href": "post/2021-06-10-how-to-use-cri-cluster/index.html#batch-job-submissions",
    "title": "How to use CRI cluster",
    "section": "Batch job submissions",
    "text": "Batch job submissions\n\nPrepare batch job submission scripts as follows: ``` #!/bin/bash\nqsub run_metaXcan2.pbs\nsleep 2\nqsub run_metaXcan2.pbs\n… ```\nThen run sh submit_jobs.sh"
  },
  {
    "objectID": "post/2021-06-10-how-to-use-cri-cluster/index.html#running-interactive-jobs",
    "href": "post/2021-06-10-how-to-use-cri-cluster/index.html#running-interactive-jobs",
    "title": "How to use CRI cluster",
    "section": "Running interactive jobs",
    "text": "Running interactive jobs\n\nTo run an interactive shell, issue: qsub -I"
  },
  {
    "objectID": "post/2021-06-10-how-to-use-cri-cluster/index.html#running-r",
    "href": "post/2021-06-10-how-to-use-cri-cluster/index.html#running-r",
    "title": "How to use CRI cluster",
    "section": "Running R",
    "text": "Running R\nTo run R in cri you need to load the following\nmodule load gcc/6.2.0  \nmodule load R/4.0.3\n\nFor more detail, please read  A detailed user guide on how to use our new cluster"
  },
  {
    "objectID": "post/2021-06-10-how-to-use-cri-cluster/index.html#cri-support-contact",
    "href": "post/2021-06-10-how-to-use-cri-cluster/index.html#cri-support-contact",
    "title": "How to use CRI cluster",
    "section": "CRI Support contact",
    "text": "CRI Support contact\n\nShould you have any problems, please submit  a support ticket \n\n\nOr please contact our customer service line at (773) 834-8475 or at support@rt.cri.uchicago.edu."
  },
  {
    "objectID": "post/2020-08-10-windows-wsl/index.html",
    "href": "post/2020-08-10-windows-wsl/index.html",
    "title": "Configuring Windows Subsystem for Linux",
    "section": "",
    "text": "This is a guide to configuring a Windows system to utilize many of the tools that the Im Lab uses."
  },
  {
    "objectID": "post/2020-08-10-windows-wsl/index.html#windows-subsystem-for-linux",
    "href": "post/2020-08-10-windows-wsl/index.html#windows-subsystem-for-linux",
    "title": "Configuring Windows Subsystem for Linux",
    "section": "Windows Subsystem for Linux",
    "text": "Windows Subsystem for Linux\nThe native command line options in Windows are not well-integrated for many bioinformatic tools, so an alternative solution is to use Windows Subsystem for Linux (WSL). This allows your windows machine to run a Linux environment. These instructions were written on Windows 10 Version 10.0.19041 Build 19041. You can find further info here.\n\nSetup\nIn order to get started with WSL, you first need to enable it.\n\nTo do so, search for “features” in your search bar and locate “Turn Windows features on or off.”\n\nFind the Windows Subsystem for Linux box and check it. Click OK.\n\nAlternatively, you can activate WSL by opening Windows PowerShell as Administrator and running: dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\n\n\nThere are two WSL versions as of this document’s creation. WSL 2 has some performance improvements and other changes that are discussed here. If you want to use WSL 1 (not recommended, unless you have a specific reason to or your computer does not support WSL 2), restart now. Otherwise, continue to Step 4.\n\nIn order to run WSL 2, your Windows version must be 2004, Build 19041 or higher. You can check your version by using Windows key + R then winver. If your version does not support WSL 2, update to the latest Windows version.\n\n\nOnce you have enabled the WSL feature in Step 2, enable the ‘Virtual Machine Platform’ by opening PowerShell and running dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart\n\nRestart your computer.\n\nTo set WSL 2 as your default version, enter the following into PowerShell wsl --set-default-version 2\n\n\n\nDownloading Linux Distribution\n\nIn the Microsoft Store, you can search for various Linux distributions. I use the Ubuntu app.\n\nInstall your distribution of choice.\nLaunch the Linux distribution (it will have to decompress some files the first time you download and then you will be asked to make an account)\n\n\n\nSetting WSL Distribution Version\n\nCheck the WSL version associated with your Linux Distribution by entering the following in PowerShell wsl -l -v\n\nIf you see 1 for the WSL version, you can change it using PowerShell wsl --set-version <distribution name> <versionNumber>, where  is the name you see for the distribution when you use wsl -l -v and  is 1 or 2. E.g. wsl --set-version Ubuntu 2\n\nTroubleshooting options and further details can be found in the Microsoft WSL documentation here.\n\n\nUsing the Ubuntu App\nYou can use Linux commands in the Ubuntu App and this set-up (or another WSL distro) will allow you to use the various Im Lab tools that require Bash.\nYour home directory can be accessed in Widows Explorer by entering \\\\wsl$\\ in the address bar. Your personal files are stored at \\wsl$<yourname>. You can change this using a .bashrc file, but it is recommended to keep your projects in the linux directory for faster access.\nTo refer to files within the Windows Filesystem, use “/mnt/c/” in place of “C:” in the filepath.\nIf you need to regularly access a folder in the Windows filesystem, you can use ln -s /mnt/c/<rest of filepath> to link the folder to your home directory in the Linux system. Be aware that access will be slower this way than if you just move that folder into the WSL Linux home folder.\nRemember to use Linux versions for programs that you are downloading to run through WSL (e.g. Plink, miniconda)!"
  },
  {
    "objectID": "post/2020-08-10-windows-wsl/index.html#r-studio",
    "href": "post/2020-08-10-windows-wsl/index.html#r-studio",
    "title": "Configuring Windows Subsystem for Linux",
    "section": "R Studio",
    "text": "R Studio\nR Studio is a platform that provides a useful interface to run R commands, packages, etc…\n\nInstallation\nTo install R Studio, follow the instructions here.\n\n\nIntegrating WSL into R Studio Terminal\nOne last step to get you set up with WSL is to integrate your Linux Distribution into the Terminal in R Studio, so that you can run things in Linux instead of the default Windows options. To do so, open R Studio > Tools > Terminal > Terminal Options > change New Terminals Open With to ‘Bash (Windows Subsystem for Linux)’.\nNow you should be ready to go and able to download and utilize packages and tools used in the Im Lab."
  },
  {
    "objectID": "post/2020-11-23-working-with-large-files/index.html",
    "href": "post/2020-11-23-working-with-large-files/index.html",
    "title": "Working with Large Files",
    "section": "",
    "text": "When working with large datasets, only the files with code should be pushed to Github repositories, not the data itself. The raw data inputs or analysis output should either be kept in a local directory that is never committed, or for best practices, they should be stored in Box (download).\nOnce installed, you can navigate Box folders from Mac Finder or Windows Explorer. Files in Box Drive are also identified by a local path.\nFirst, check that you have access to the imlab-data folder on Box. It should also appear in the Box Drive folder in Finder or Explorer, but you may need to first request access permission from Haky.\nNext, create a directory in the data-Github folder, Box/imlab-data/data-Github, with the same name as the Github repository for the analysis. Copy all input files into the data-Github folder and create an output folder.\nFor example, I made a folder in data-Github, rat-genomic-analysis, and added the genotype data and model. Then I ran PrediXcan and output the results directly to /Users/sabrinami/Box/imlab-data/data-Github/rat-genomic-analysis/Results/PrediXcan"
  },
  {
    "objectID": "post/2020-11-23-working-with-large-files/index.html#here-is-a-semi-automated-way-to-generate-and-open-the-data-folder-in-box",
    "href": "post/2020-11-23-working-with-large-files/index.html#here-is-a-semi-automated-way-to-generate-and-open-the-data-folder-in-box",
    "title": "Working with Large Files",
    "section": "here is a semi-automated way to generate and open the data folder in box",
    "text": "here is a semi-automated way to generate and open the data folder in box\nCopy pasted it to the beginning of your Rmd file\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\n##PRE=\"/Users/temi/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data\"\n## COPY THE DATE AND SLUG fields FROM THE HEADER\nSLUG=\"correlation-between-ptrs-and-rat-height-bmi\" ## copy the slug from the header\nbDATE='2022-07-07' ## copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n\n## move data to DATA\n#tempodata=(\"~/Downloads/tempo/gwas_catalog_v1.0.2-associations_e105_r2022-04-07.tsv\")\n#system(glue::glue(\"cp {tempodata} {DATA}/\"))\nsystem(glue(\"open {DATA}\")) ## this will open the folder"
  },
  {
    "objectID": "post/2022-10-20-parg-fall-social-2022/index.html",
    "href": "post/2022-10-20-parg-fall-social-2022/index.html",
    "title": "PARG Fall Social 2022",
    "section": "",
    "text": "Show the code\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\n##PRE=\"/Users/margaretperry/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data \"\n##PRE=\"/Users/temi/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data\"\n## COPY THE DATE AND SLUG fields FROM THE HEADER\nSLUG=\"parg-fall-social-2022\" ## copy the slug from the header\nbDATE='2022-10-20' ## copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n\n## move data to DATA\n#tempodata=(\"~/Downloads/tempo/gwas_catalog_v1.0.2-associations_e105_r2022-04-07.tsv\")\n#system(glue::glue(\"cp {tempodata} {DATA}/\"))\n##system(glue(\"open {DATA}\")) ## this will open the folder \n\n\n\n\nShow the code\ndf <- readxl::read_excel(glue(\"{WORK}/report-2022-10-20T1314.xlsx\"))\n\n\nNew names:\n• `Please explain` -> `Please explain...26`\n• `Please explain` -> `Please explain...27`\n\n\nShow the code\nnames(df)\n\n\n [1] \"Order #\"                                                                         \n [2] \"Order Date\"                                                                      \n [3] \"First Name\"                                                                      \n [4] \"Last Name\"                                                                       \n [5] \"Email\"                                                                           \n [6] \"Quantity\"                                                                        \n [7] \"Price Tier\"                                                                      \n [8] \"Ticket Type\"                                                                     \n [9] \"Attendee #\"                                                                      \n[10] \"Group\"                                                                           \n[11] \"Order Type\"                                                                      \n[12] \"Currency\"                                                                        \n[13] \"Total Paid\"                                                                      \n[14] \"Fees Paid\"                                                                       \n[15] \"Eventbrite Fees\"                                                                 \n[16] \"Eventbrite Payment Processing\"                                                   \n[17] \"Attendee Status\"                                                                 \n[18] \"Home Address 1\"                                                                  \n[19] \"Home Address 2\"                                                                  \n[20] \"Home City\"                                                                       \n[21] \"Home State\"                                                                      \n[22] \"Home Zip\"                                                                        \n[23] \"Home Country\"                                                                    \n[24] \"Will you be attending the event?\"                                                \n[25] \"Do you have any allergies or dietary requirements?\"                              \n[26] \"Please explain...26\"                                                             \n[27] \"Please explain...27\"                                                             \n[28] \"Are you interested in participating in the talent show?\"                         \n[29] \"What equipment would you need?\"                                                  \n[30] \"Campus affiliation  (dept/div) and role (undergrad, grad, staff, faculty, other)\"\n[31] \"Is this your first time attending a Pan-Asian Resource Group event?\"             \n[32] \"How did you hear about us?\"                                                      \n\n\nShow the code\ndf %>% count(`Campus affiliation  (dept/div) and role (undergrad, grad, staff, faculty, other)`) %>% arrange(desc(n))\n\n\n# A tibble: 59 × 2\n   Campus affiliation  (dept/div) and role (undergrad, grad, staff, facu…¹     n\n   <chr>                                                                   <int>\n 1 Grad                                                                       19\n 2 grad                                                                        7\n 3 Harris                                                                      6\n 4 BSD                                                                         5\n 5 Grad student                                                                3\n 6 Harris student                                                              3\n 7 PSD, grad                                                                   3\n 8 Harris MPP                                                                  2\n 9 PSD grad                                                                    2\n10 BSD grad student                                                            1\n# ℹ 49 more rows\n# ℹ abbreviated name:\n#   ¹​`Campus affiliation  (dept/div) and role (undergrad, grad, staff, faculty, other)`\n\n\nnot very helpful, better to have list to select from rather than free text next time\n\n\nShow the code\ndf %>% count(`How did you hear about us?`) %>% arrange(desc(n)) \n\n\n# A tibble: 5 × 2\n  `How did you hear about us?`                                                 n\n  <chr>                                                                    <int>\n1 International House newsletter                                              40\n2 Through a friend                                                            30\n3 Through the Pan Asian or the Pan Asian Resource Group list serves           17\n4 Divisional/departmental events                                              12\n5 International House newsletter | Through the Pan Asian or the Pan Asian…     1\n\n\nI-house newsletter was the most effective dissemination"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/delete/index.html",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/delete/index.html",
    "title": "ARIC PWAS models",
    "section": "",
    "text": "The ARIC model for MetaXcan is in Box: https://uchicago.box.com/s/3sf4y4gv6c7zam0l5fxicpcd3zji5wzc"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/delete/index.html#aric-pwas",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/delete/index.html#aric-pwas",
    "title": "ARIC PWAS models",
    "section": "ARIC PWAS",
    "text": "ARIC PWAS\nThe Atherosclerosis Risk in Communities Study (ARIC) generated genotype and proteomic data from a total of 9,084 participants (7,213 European Americans and 1,871 African Americans). The relative conectrations of plasma proteins or protein complexes was measured from blood samples using an aptamer-based approach. Genotyping of blood samples was imputed to the TOPMed reference panel (GRCh38).\nNilan Chatterjee et al analyzed cis-genetic regulation of the plasma proteome, generating PWAS through TWAS/Fusion pipeline. They study involved 4,665 SOMAmers measuring 4,491 unique plasma proteins or protein complexes encoded by 4,445 autosomal genes.\nFor details on the paper: https://www.biorxiv.org/content/10.1101/2021.03.15.435533v1.full"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/delete/index.html#generating-the-model",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/delete/index.html#generating-the-model",
    "title": "ARIC PWAS models",
    "section": "Generating the model",
    "text": "Generating the model\nWe created a prediction model compatible with MetaXcan software from the weights generated by Nilan Chatterjee et al’s PWAS study. The steps are documented below:\n\nCreate prediction weights file\nCalculate covariances"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/delete/index.html#validation",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/delete/index.html#validation",
    "title": "ARIC PWAS models",
    "section": "Validation",
    "text": "Validation\nWe validated the model by running SPrediXcan on height and coronary artery disease GWAS, then comparing the results to association found from Whole Blood mashr models.\n\nGIANT height\nCARDIoGRAM+C4D CAD"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/ARIC_EA_hg38_validation.html",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/ARIC_EA_hg38_validation.html",
    "title": "ARIC EA hg38 validation",
    "section": "",
    "text": "::: {.container-fluid .main-container} ::: {#header .fluid-row} # psychencode_hg38_validation {#psychencode_hg38_validation .title .toc-ignore}"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/ARIC_EA_hg38_validation.html#definitions",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/ARIC_EA_hg38_validation.html#definitions",
    "title": "ARIC EA hg38 validation",
    "section": "Definitions",
    "text": "Definitions\nconda activate imlabtools\nMETAXCAN=/Users/sabrinami/Github/MetaXcan/software\nDATA=/Users/sabrinami/Github/ARIC/test_data/GWAS\nRESULTS=/Users/sabrinami/Github/ARIC/results/SPrediXcan\nMODEL=/Users/sabrinami/Github/ARIC/models"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/ARIC_EA_hg38_validation.html#definitions-1",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/ARIC_EA_hg38_validation.html#definitions-1",
    "title": "ARIC EA hg38 validation",
    "section": "Definitions",
    "text": "Definitions\n.\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(qqman))\nsuppressPackageStartupMessages(library(data.table))\nsuppressPackageStartupMessages(library(RSQLite))\nDATA=\"/Users/sabrinami/Github/ARIC/test_data/GWAS\"\nRESULTS=\"/Users/sabrinami/Github/ARIC/results/SPrediXcan\"\nMODEL=\"/Users/sabrinami/Github/ARIC/models\"\nCODE=\"/Users/sabrinami/Github/ARIC/code\"\nsource(glue::glue(\"{CODE}/load_data_functions.R\"))\nsource(glue::glue(\"{CODE}/plotting_utils_functions.R\"))\n\ngencode_df = load_gencode_df()"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/ARIC_EA_hg38_validation.html#run-s-predixcan",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/ARIC_EA_hg38_validation.html#run-s-predixcan",
    "title": "ARIC EA hg38 validation",
    "section": "Run S-PrediXcan",
    "text": "Run S-PrediXcan\nRun S-PrediXcan with the ARIC model.\npython $METAXCAN/SPrediXcan.py \\\n--gwas_file  $DATA/imputed_CARDIoGRAM_C4D_CAD_ADDITIVE.txt.gz \\\n--snp_column panel_variant_id --effect_allele_column effect_allele --non_effect_allele_column non_effect_allele --zscore_column zscore \\\n--model_db_path $MODEL/ARIC_EA_hg38.db \\\n--covariance $MODEL/ARIC_EA_hg38.txt.gz \\\n--keep_non_rsid --additional_output --model_db_snp_key varID \\\n--throw \\\n--output_file $RESULTS/CAD_ARIC_hg38.csv\nAnd the mashr model.\npython $METAXCAN/SPrediXcan.py \\\n--gwas_file  $DATA/imputed_CARDIoGRAM_C4D_CAD_ADDITIVE.txt.gz \\\n--snp_column panel_variant_id --effect_allele_column effect_allele --non_effect_allele_column non_effect_allele --zscore_column zscore \\\n--model_db_path $MODEL/mashr_Whole_Blood.db \\\n--covariance $MODEL/mashr_Whole_Blood.txt.gz \\\n--keep_non_rsid --additional_output --model_db_snp_key varID \\\n--throw \\\n--output_file $RESULTS/CAD_mashr_Whole_Blood.csv"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/ARIC_EA_hg38_validation.html#compare-association-results",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/ARIC_EA_hg38_validation.html#compare-association-results",
    "title": "ARIC EA hg38 validation",
    "section": "Compare Association Results",
    "text": "Compare Association Results\nspredixcan_association_ARIC = load_spredixcan_association(glue::glue(\"{RESULTS}/CAD_ARIC_hg38.csv\"), gencode_df)\ndim(spredixcan_association_ARIC)\n[1] 1318   16\nsignificant_genes_ARIC <- spredixcan_association_ARIC %>% filter(pvalue < 0.05/nrow(spredixcan_association_ARIC)) %>% arrange(pvalue)\nspredixcan_association_Whole_Blood = load_spredixcan_association(glue::glue(\"{RESULTS}/CAD_mashr_Whole_Blood.csv\"), gencode_df)\ndim(spredixcan_association_Whole_Blood)\n[1] 12587    16\nsignificant_genes_Whole_Blood <- spredixcan_association_Whole_Blood %>% filter(pvalue < 0.05/nrow(spredixcan_association_Whole_Blood)) %>% arrange(pvalue)\nThen compare ARIC and Whole Blood z-scores.\nzscores = inner_join(spredixcan_association_Whole_Blood, spredixcan_association_ARIC, by=c(\"gene\"))\ndim(zscores)\n[1] 815  31\nzscores %>% ggplot(aes(zscore.x, zscore.y)) + geom_point() + ggtitle(\"S-PrediXcan z-score\") + xlab(\"mashr Whole Blood\") + ylab(\"ARIC\") + geom_abline(intercept = 0, slope = 1)\nWarning: Removed 1 rows containing missing values (geom_point).\n\nWe can compare the significant genes found with the ARIC and mashr Whole Blood models.\nsignificant_genes_ARIC[, c(1,2)]\n             gene    zscore\n1 ENSG00000186063 -6.914873\n2 ENSG00000160712 -5.813218\n3 ENSG00000169174  5.326004\n4 ENSG00000133789 -5.085480\n5 ENSG00000107562  4.976651\n6 ENSG00000158710  4.567579\n7 ENSG00000158710  4.567579\n8 ENSG00000175573  4.125269\nsignificant_genes_Whole_Blood[, c(1,2)]\n              gene    zscore\n1  ENSG00000134222 -9.263287\n2  ENSG00000107798  7.525837\n3  ENSG00000163596  7.210367\n4  ENSG00000138380 -5.982736\n5  ENSG00000160712  5.744854\n6  ENSG00000127616  5.703439\n7  ENSG00000183431 -5.382606\n8  ENSG00000182511 -5.373519\n9  ENSG00000115486  5.258467\n10 ENSG00000084093 -5.204898\n11 ENSG00000143498  4.920139\n12 ENSG00000031698 -4.747462\n13 ENSG00000168906 -4.698991\n14 ENSG00000130475 -4.683521\n15 ENSG00000119718  4.681344\nintersect(significant_genes_ARIC$gene,significant_genes_Whole_Blood$gene)\n[1] \"ENSG00000160712\"\nThere is only one gene found significant in both, ENSG00000160712\n\n\nSession information\n\nsessionInfo()\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] RSQLite_2.2.1     data.table_1.13.2 qqman_0.1.4       forcats_0.5.0    \n [5] stringr_1.4.0     dplyr_1.0.2       purrr_0.3.4       readr_1.4.0      \n [9] tidyr_1.1.2       tibble_3.0.4      ggplot2_3.3.2     tidyverse_1.3.0  \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.5       lubridate_1.7.9  assertthat_0.2.1 rprojroot_1.3-2 \n [5] digest_0.6.27    R6_2.4.1         cellranger_1.1.0 backports_1.1.10\n [9] reprex_0.3.0     evaluate_0.14    httr_1.4.2       highr_0.8       \n[13] pillar_1.4.6     rlang_0.4.8      readxl_1.3.1     rstudioapi_0.11 \n[17] blob_1.2.1       rmarkdown_2.5    labeling_0.4.2   bit_4.0.4       \n[21] munsell_0.5.0    broom_0.7.2      compiler_4.0.3   httpuv_1.5.4    \n[25] modelr_0.1.8     xfun_0.18        pkgconfig_2.0.3  htmltools_0.5.0 \n[29] tidyselect_1.1.0 workflowr_1.6.2  fansi_0.4.1      calibrate_1.7.7 \n[33] crayon_1.3.4     dbplyr_1.4.4     withr_2.3.0      later_1.1.0.1   \n[37] MASS_7.3-53      grid_4.0.3       jsonlite_1.7.1   gtable_0.3.0    \n[41] lifecycle_0.2.0  DBI_1.1.0        git2r_0.27.1     magrittr_1.5    \n[45] scales_1.1.1     cli_2.1.0        stringi_1.5.3    farver_2.0.3    \n[49] fs_1.5.0         promises_1.1.1   xml2_1.3.2       ellipsis_0.3.1  \n[53] generics_0.0.2   vctrs_0.3.4      tools_4.0.3      bit64_4.0.5     \n[57] glue_1.4.2       hms_0.5.3        yaml_2.2.1       colorspace_1.4-1\n[61] rvest_0.3.6      memoise_1.1.0    knitr_1.30       haven_2.3.1"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/ARIC_EA_hg38_validation_height.html",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/ARIC_EA_hg38_validation_height.html",
    "title": "ARIC EA hg38 validation height",
    "section": "",
    "text": "::: {.container-fluid .main-container} ::: {#header .fluid-row} # ARIC EA validation {#aric-ea-validation .title .toc-ignore}"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/ARIC_EA_hg38_validation_height.html#definitions",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/ARIC_EA_hg38_validation_height.html#definitions",
    "title": "ARIC EA hg38 validation height",
    "section": "Definitions",
    "text": "Definitions\nconda activate imlabtools\nMETAXCAN=/Users/sabrinami/Github/MetaXcan/software\nDATA=/Users/sabrinami/Github/ARIC/test_data/GWAS\nRESULTS=/Users/sabrinami/Github/ARIC/results/SPrediXcan\nMODEL=/Users/sabrinami/Github/ARIC/models\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(qqman))\nsuppressPackageStartupMessages(library(data.table))\nsuppressPackageStartupMessages(library(RSQLite))\nsuppressPackageStartupMessages(library(UpSetR))\nDATA=\"/Users/sabrinami/Github/ARIC/test_data/GWAS\"\nRESULTS=\"/Users/sabrinami/Github/ARIC/results/SPrediXcan\"\nMODEL=\"/Users/sabrinami/Github/ARIC/models\"\nCODE=\"/Users/sabrinami/Github/ARIC/code\"\nsource(glue::glue(\"{CODE}/load_data_functions.R\"))\nsource(glue::glue(\"{CODE}/plotting_utils_functions.R\"))\n\ngencode_df = load_gencode_df()"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/ARIC_EA_hg38_validation_height.html#run-s-predixcan",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/ARIC_EA_hg38_validation_height.html#run-s-predixcan",
    "title": "ARIC EA hg38 validation height",
    "section": "Run S-PrediXcan",
    "text": "Run S-PrediXcan\nRun S-PrediXcan with the ARIC model.\npython $METAXCAN/SPrediXcan.py \\\n--gwas_file  $DATA/imputed_GIANT_HEIGHT.txt.gz \\\n--snp_column panel_variant_id --effect_allele_column effect_allele --non_effect_allele_column non_effect_allele --zscore_column zscore \\\n--model_db_path $MODEL/ARIC_EA_hg38.db \\\n--covariance $MODEL/ARIC_EA_hg38.txt.gz \\\n--keep_non_rsid --additional_output --model_db_snp_key varID \\\n--throw \\\n--output_file $RESULTS/GIANT_HEIGHT_ARIC_hg38.csv\nAnd the mashr model.\npython $METAXCAN/SPrediXcan.py \\\n--gwas_file  $DATA/imputed_GIANT_HEIGHT.txt.gz \\\n--snp_column panel_variant_id --effect_allele_column effect_allele --non_effect_allele_column non_effect_allele --zscore_column zscore \\\n--model_db_path $MODEL/mashr_Whole_Blood.db \\\n--covariance $MODEL/mashr_Whole_Blood.txt.gz \\\n--keep_non_rsid --additional_output --model_db_snp_key varID \\\n--throw \\\n--output_file $RESULTS/GIANT_HEIGHT_mashr_Whole_Blood.csv"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/ARIC_EA_hg38_validation_height.html#compare-association-results",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/ARIC_EA_hg38_validation_height.html#compare-association-results",
    "title": "ARIC EA hg38 validation height",
    "section": "Compare Association Results",
    "text": "Compare Association Results\nspredixcan_association_ARIC = load_spredixcan_association(glue::glue(\"{RESULTS}/GIANT_HEIGHT_ARIC_hg38.csv\"), gencode_df)\ndim(spredixcan_association_ARIC)\n[1] 1318   16\nsignificant_genes_ARIC <- spredixcan_association_ARIC %>% filter(pvalue < 0.05/nrow(spredixcan_association_ARIC)) %>% arrange(pvalue)\nspredixcan_association_Whole_Blood = load_spredixcan_association(glue::glue(\"{RESULTS}/GIANT_HEIGHT_mashr_Whole_Blood.csv\"), gencode_df)\ndim(spredixcan_association_Whole_Blood)\n[1] 12557    16\nsignificant_genes_Whole_Blood <- spredixcan_association_Whole_Blood %>% filter(pvalue < 0.05/nrow(spredixcan_association_Whole_Blood)) %>% arrange(pvalue)\nThen compare ARIC and Whole Blood z-scores.\nzscores = inner_join(spredixcan_association_Whole_Blood, spredixcan_association_ARIC, by=c(\"gene\"))\ndim(zscores)\n[1] 814  31\nzscores %>% ggplot(aes(zscore.x, zscore.y)) + geom_point() + ggtitle(\"S-PrediXcan z-score\") + xlab(\"mashr Whole Blood\") + ylab(\"ARIC\") + geom_abline(intercept = 0, slope = 1)\nWarning: Removed 1 rows containing missing values (geom_point).\n We can also look at correlation between z-scores from the ARIC and mashr models.\ncor(zscores[c(2,17)], use = \"complete.obs\", method = \"spearman\")[1,2]\n[1] 0.2915486\n\n\nSession information\n\nsessionInfo()\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] UpSetR_1.4.0      RSQLite_2.2.1     data.table_1.13.2 qqman_0.1.4      \n [5] forcats_0.5.0     stringr_1.4.0     dplyr_1.0.2       purrr_0.3.4      \n [9] readr_1.4.0       tidyr_1.1.2       tibble_3.0.4      ggplot2_3.3.2    \n[13] tidyverse_1.3.0  \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.5       lubridate_1.7.9  assertthat_0.2.1 rprojroot_1.3-2 \n [5] digest_0.6.27    plyr_1.8.6       R6_2.4.1         cellranger_1.1.0\n [9] backports_1.1.10 reprex_0.3.0     evaluate_0.14    httr_1.4.2      \n[13] highr_0.8        pillar_1.4.6     rlang_0.4.8      readxl_1.3.1    \n[17] rstudioapi_0.11  blob_1.2.1       rmarkdown_2.5    labeling_0.4.2  \n[21] bit_4.0.4        munsell_0.5.0    broom_0.7.2      compiler_4.0.3  \n[25] httpuv_1.5.4     modelr_0.1.8     xfun_0.18        pkgconfig_2.0.3 \n[29] htmltools_0.5.0  tidyselect_1.1.0 gridExtra_2.3    workflowr_1.6.2 \n[33] fansi_0.4.1      calibrate_1.7.7  crayon_1.3.4     dbplyr_1.4.4    \n[37] withr_2.3.0      later_1.1.0.1    MASS_7.3-53      grid_4.0.3      \n[41] jsonlite_1.7.1   gtable_0.3.0     lifecycle_0.2.0  DBI_1.1.0       \n[45] git2r_0.27.1     magrittr_1.5     scales_1.1.1     cli_2.1.0       \n[49] stringi_1.5.3    farver_2.0.3     fs_1.5.0         promises_1.1.1  \n[53] xml2_1.3.2       ellipsis_0.3.1   generics_0.0.2   vctrs_0.3.4     \n[57] tools_4.0.3      bit64_4.0.5      glue_1.4.2       hms_0.5.3       \n[61] yaml_2.2.1       colorspace_1.4-1 rvest_0.3.6      memoise_1.1.0   \n[65] knitr_1.30       haven_2.3.1"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/index.html",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/index.html",
    "title": "ARIC PWAS models",
    "section": "",
    "text": "The ARIC model for MetaXcan is in Box: https://uchicago.box.com/s/3sf4y4gv6c7zam0l5fxicpcd3zji5wzc"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/index.html#aric-pwas",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/index.html#aric-pwas",
    "title": "ARIC PWAS models",
    "section": "ARIC PWAS",
    "text": "ARIC PWAS\nThe Atherosclerosis Risk in Communities Study (ARIC) generated genotype and proteomic data from a total of 9,084 participants (7,213 European Americans and 1,871 African Americans). The relative conectrations of plasma proteins or protein complexes was measured from blood samples using an aptamer-based approach. Genotyping of blood samples was imputed to the TOPMed reference panel (GRCh38).\nNilan Chatterjee et al analyzed cis-genetic regulation of the plasma proteome, generating PWAS through TWAS/Fusion pipeline. They study involved 4,665 SOMAmers measuring 4,491 unique plasma proteins or protein complexes encoded by 4,445 autosomal genes.\nFor details on the paper: https://www.biorxiv.org/content/10.1101/2021.03.15.435533v1.full"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/index.html#generating-the-model",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/index.html#generating-the-model",
    "title": "ARIC PWAS models",
    "section": "Generating the model",
    "text": "Generating the model\nWe created a prediction model compatible with MetaXcan software from the weights generated by Nilan Chatterjee et al’s PWAS study. The steps are documented below:\n\nCreate prediction weights file\nCalculate covariances"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/index.html#validation",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/index.html#validation",
    "title": "ARIC PWAS models",
    "section": "Validation",
    "text": "Validation\nWe validated the model by running SPrediXcan on height and coronary artery disease GWAS, then comparing the results to association found from Whole Blood mashr models.\n\nGIANT height\nCARDIoGRAM+C4D CAD"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/covariances_EA_hg38.html",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/covariances_EA_hg38.html",
    "title": "Covariances EA hg38",
    "section": "",
    "text": "::: {.container-fluid .main-container} ::: {#header .fluid-row} # covariances_hg38 {#covariances_hg38 .title .toc-ignore}\n\n2021-06-04\n:::\n workflowr \n\n\nSummary\nChecks \nPast versions\n\n::: tab-content ::: {#summary .tab-pane .fade .in .active} Last updated: 2021-09-08\nChecks:  6  1\nKnit directory: ~/Github/ARIC/ \nThis reproducible R Markdown analysis was created with workflowr (version 1.6.2). The Checks tab describes the reproducibility checks that were applied when the results were created. The Past versions tab lists the development history.\n\n\n\n\n\n\n R Markdown file: uncommitted changes\n\n\n\nThe R Markdown is untracked by Git. To know which version of the R Markdown file created these results, you’ll want to first commit it to the Git repo. If you’re still working on the analysis, you can ignore this warning. When you’re finished, you can run wflow_publish to commit the R Markdown file and build the HTML.\n\n\n\n\n\n Environment: empty\n\n\n\nGreat job! The global environment was empty. Objects defined in the global environment can affect the analysis in your R Markdown file in unknown ways. For reproduciblity it’s best to always run the code in an empty environment.\n\n\n\n\n\n Seed: set.seed(12345)\n\n\n\nThe command set.seed(12345) was run prior to running the code in the R Markdown file. Setting a seed ensures that any results that rely on randomness, e.g. subsampling or permutations, are reproducible.\n\n\n\n\n\n Session information: recorded\n\n\n\nGreat job! Recording the operating system, R version, and package versions is critical for reproducibility.\n\n\n\n\n\n Cache: none\n\n\n\nNice! There were no cached chunks for this analysis, so you can be confident that you successfully produced the results during this run.\n\n\n\n\n\n File paths: relative\n\n\n\nGreat job! Using relative paths to the files within your workflowr project makes it easier to run your code on other machines.\n\n\n\n\n\n Repository version: No commits yet\n\n\n\nGreat! You are using Git for version control. Tracking code development and connecting the code version to the results is critical for reproducibility.\nNote that you need to be careful to ensure that all relevant files for the analysis have been committed to Git prior to generating the results (you can use wflow_publish or wflow_git_commit). workflowr only checks the R Markdown file, but you know if there are other scripts or data files that it depends on. Below is the status of the Git repository when the results were generated:\nUntracked files:\n    Untracked:  .DS_Store\n    Untracked:  .Rhistory\n    Untracked:  ARIC_EA_hg38_validation.Rmd\n    Untracked:  ARIC_EA_hg38_validation.html\n    Untracked:  ARIC_EA_hg38_validation_height.Rmd\n    Untracked:  ARIC_EA_hg38_validation_height.html\n    Untracked:  PWAS/\n    Untracked:  code/\n    Untracked:  covariances_EA_hg38.Rmd\n    Untracked:  figure/\n    Untracked:  models/\n    Untracked:  results/\n    Untracked:  test_data/\n    Untracked:  weights_EA.Rmd\n    Untracked:  weights_EA.html\nNote that any generated files, e.g. HTML, png, CSS, etc., are not included in this status report because it is ok for generated content to have uncommitted changes.\n\n\n\n\n\n\n\nThere are no past versions. Publish this analysis with wflow_publish() to start tracking its development.\n\n\n\n\n\n\n:::\n\n\n\n\ntitle: Covariances EA hg38 author: Sabrina Mi date: ‘2021-09-08’\n\n\n\n\n\n\ncovariances_hg38\n\n2021-06-04\n\n\n\n workflowr \n\n\nSummary\nChecks \nPast versions\n\n\n\nLast updated: 2021-09-08\nChecks:  6  1\nKnit directory: ~/Github/ARIC/ \nThis reproducible R Markdown analysis was created with workflowr (version 1.6.2). The Checks tab describes the reproducibility checks that were applied when the results were created. The Past versions tab lists the development history.\n\n\n\n\n\n\n R Markdown file: uncommitted changes\n\n\n\nThe R Markdown is untracked by Git. To know which version of the R Markdown file created these results, you’ll want to first commit it to the Git repo. If you’re still working on the analysis, you can ignore this warning. When you’re finished, you can run wflow_publish to commit the R Markdown file and build the HTML.\n\n\n\n\n\n Environment: empty\n\n\n\nGreat job! The global environment was empty. Objects defined in the global environment can affect the analysis in your R Markdown file in unknown ways. For reproduciblity it’s best to always run the code in an empty environment.\n\n\n\n\n\n Seed: set.seed(12345)\n\n\n\nThe command set.seed(12345) was run prior to running the code in the R Markdown file. Setting a seed ensures that any results that rely on randomness, e.g. subsampling or permutations, are reproducible.\n\n\n\n\n\n Session information: recorded\n\n\n\nGreat job! Recording the operating system, R version, and package versions is critical for reproducibility.\n\n\n\n\n\n Cache: none\n\n\n\nNice! There were no cached chunks for this analysis, so you can be confident that you successfully produced the results during this run.\n\n\n\n\n\n File paths: relative\n\n\n\nGreat job! Using relative paths to the files within your workflowr project makes it easier to run your code on other machines.\n\n\n\n\n\n Repository version: No commits yet\n\n\n\nGreat! You are using Git for version control. Tracking code development and connecting the code version to the results is critical for reproducibility.\nNote that you need to be careful to ensure that all relevant files for the analysis have been committed to Git prior to generating the results (you can use wflow_publish or wflow_git_commit). workflowr only checks the R Markdown file, but you know if there are other scripts or data files that it depends on. Below is the status of the Git repository when the results were generated:\nUntracked files:\n    Untracked:  .DS_Store\n    Untracked:  .Rhistory\n    Untracked:  ARIC_EA_hg38_validation.Rmd\n    Untracked:  ARIC_EA_hg38_validation.html\n    Untracked:  ARIC_EA_hg38_validation_height.Rmd\n    Untracked:  ARIC_EA_hg38_validation_height.html\n    Untracked:  PWAS/\n    Untracked:  code/\n    Untracked:  covariances_EA_hg38.Rmd\n    Untracked:  figure/\n    Untracked:  models/\n    Untracked:  results/\n    Untracked:  test_data/\n    Untracked:  weights_EA.Rmd\n    Untracked:  weights_EA.html\nNote that any generated files, e.g. HTML, png, CSS, etc., are not included in this status report because it is ok for generated content to have uncommitted changes.\n\n\n\n\n\n\n\nThere are no past versions. Publish this analysis with wflow_publish() to start tracking its development.\n\n\n\n\n\nDownload Data\ncovariance_for_model.py takes genotypes in parquet format. Run git clone https://github.com/hakyimlab/summary-gwas-imputation.git. The data can be downloaded: https://zenodo.org/record/3569954#.XyRiqChKiUk. Or in CRI: /gpfs/data/im-lab/nas40t2/Data/1000G_hg38_EUR_maf0.01_parquet\n\n\nCalculating Covariance\nCODE=/Users/t.med.scmi/Github/summary-gwas-imputation\nDATA=/gpfs/data/im-lab/nas40t2/Data/1000G_hg38_EUR_maf0.01_parquet\nMODEL=/gpfs/data/im-lab/nas40t2/sabrina/ARIC\nparquet_genotype_pattern helps identify genotype files by chromosome. ARIC_EA_hg38.db is a PrediXcan format prediction model defined in hg38. The script can also be submitted as a job in CRI: /gpfs/data/im-lab/nas40t2/sabrina/ARIC/covariances_1000G_hg38.sh\npython $CODE/covariance_for_model.py \\\n-parquet_genotype_folder $DATA \\\n-parquet_genotype_pattern \"chr(.*).variants.parquet\" \\\n-model_db $MODEL/ARIC_EA_hg38.db \\\n-output $MODEL/ARIC_EA_hg38.txt.gz \\\n-parsimony 1\n\n\n Session information\n\nsessionInfo()\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.5       rstudioapi_0.11  knitr_1.30       magrittr_1.5    \n [5] workflowr_1.6.2  R6_2.4.1         rlang_0.4.8      stringr_1.4.0   \n [9] tools_4.0.3      xfun_0.18        git2r_0.27.1     htmltools_0.5.0 \n[13] ellipsis_0.3.1   yaml_2.2.1       digest_0.6.27    rprojroot_1.3-2 \n[17] tibble_3.0.4     lifecycle_0.2.0  crayon_1.3.4     later_1.1.0.1   \n[21] vctrs_0.3.4      promises_1.1.1   fs_1.5.0         glue_1.4.2      \n[25] evaluate_0.14    rmarkdown_2.5    stringi_1.5.3    compiler_4.0.3  \n[29] pillar_1.4.6     backports_1.1.10 httpuv_1.5.4     pkgconfig_2.0.3 \n\n\n\n```````````````````"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/weights_AA.html",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/weights_AA.html",
    "title": "Weights AA",
    "section": "",
    "text": "::: {.container-fluid .main-container} ::: {#header .fluid-row} # generate weights {#generate-weights .title .toc-ignore}\n\nsabrina-mi\n\n\n2020-06-22\n:::\nworkflowr\n\n\nSummary\nChecks\nPast versions\n\n::: tab-content ::: {#summary .tab-pane .fade .in .active} Last updated: 2021-10-01\nChecks: 5 2\nKnit directory: ~/Github/ARIC/\nThis reproducible R Markdown analysis was created with workflowr (version 1.6.2). The Checks tab describes the reproducibility checks that were applied when the results were created. The Past versions tab lists the development history.\n\n\n\n\n\n\nR Markdown file: uncommitted changes\n\n\n\nThe R Markdown is untracked by Git. To know which version of the R Markdown file created these results, you’ll want to first commit it to the Git repo. If you’re still working on the analysis, you can ignore this warning. When you’re finished, you can run wflow_publish to commit the R Markdown file and build the HTML.\n\n\n\n\n\nEnvironment: empty\n\n\n\nGreat job! The global environment was empty. Objects defined in the global environment can affect the analysis in your R Markdown file in unknown ways. For reproduciblity it’s best to always run the code in an empty environment.\n\n\n\n\n\nSeed: set.seed(12345)\n\n\n\nThe command set.seed(12345) was run prior to running the code in the R Markdown file. Setting a seed ensures that any results that rely on randomness, e.g. subsampling or permutations, are reproducible.\n\n\n\n\n\nSession information: recorded\n\n\n\nGreat job! Recording the operating system, R version, and package versions is critical for reproducibility.\n\n\n\n\n\nCache: none\n\n\n\nNice! There were no cached chunks for this analysis, so you can be confident that you successfully produced the results during this run.\n\n\n\n\n\nFile paths: absolute\n\n\n\nUsing absolute paths to the files within your workflowr project makes it difficult for you and others to run your code on a different machine. Change the absolute path(s) below to the suggested relative path(s) to make your code more reproducible.\n\n\n\n\n\n\n\nabsolute\nrelative\n\n\n\n\n/Users/sabrinami/Github/ARIC/PWAS/PWAS_AA/Plasma_Protein_AA_hg38.pos\nPWAS/PWAS_AA/Plasma_Protein_AA_hg38.pos\n\n\n/Users/sabrinami/Github/ARIC/PWAS/PWAS_AA/Plasma_Protein_weights_AA\nPWAS/PWAS_AA/Plasma_Protein_weights_AA\n\n\n/Users/sabrinami/Github/ARIC/models/ARIC_AA_hg38.db\nmodels/ARIC_AA_hg38.db\n\n\n\n\n\n\n\n\nRepository version: No commits yet\n\n\n\nGreat! You are using Git for version control. Tracking code development and connecting the code version to the results is critical for reproducibility.\nNote that you need to be careful to ensure that all relevant files for the analysis have been committed to Git prior to generating the results (you can use wflow_publish or wflow_git_commit). workflowr only checks the R Markdown file, but you know if there are other scripts or data files that it depends on. Below is the status of the Git repository when the results were generated:\nUntracked files:\n    Untracked:  .DS_Store\n    Untracked:  .Rhistory\n    Untracked:  1000G_AFR_individuals.csv\n    Untracked:  ARIC_AA_hg38_validation_CAD.Rmd\n    Untracked:  ARIC_AA_hg38_validation_CAD.html\n    Untracked:  ARIC_EA_hg38_validation.Rmd\n    Untracked:  ARIC_EA_hg38_validation.html\n    Untracked:  ARIC_EA_hg38_validation_height.Rmd\n    Untracked:  ARIC_EA_hg38_validation_height.html\n    Untracked:  PWAS/\n    Untracked:  code/\n    Untracked:  covariances_EA_hg38.Rmd\n    Untracked:  covariances_EA_hg38.html\n    Untracked:  dosage_to_parquet.py\n    Untracked:  figure/\n    Untracked:  models/\n    Untracked:  results/\n    Untracked:  subset_parquet.py\n    Untracked:  test_data/\n    Untracked:  vcf_to_parquet.py\n    Untracked:  vcf_to_parquet.sh\n    Untracked:  weights_AA.Rmd\n    Untracked:  weights_EA.Rmd\n    Untracked:  weights_EA.html\nNote that any generated files, e.g. HTML, png, CSS, etc., are not included in this status report because it is ok for generated content to have uncommitted changes.\n\n\n\n\n\n\n\nThere are no past versions. Publish this analysis with wflow_publish() to start tracking its development.\n\n\n\n\n\n\n:::\n\n\n\n\ntitle: Weights AA author: Sabrina Mi date: ‘2021-09-08’\n\n\n\n\n\n\ngenerate weights\n\nsabrina-mi\n\n\n2020-06-22\n\n\n\nworkflowr\n\n\nSummary\nChecks\nPast versions\n\n\n\nLast updated: 2021-10-01\nChecks: 5 2\nKnit directory: ~/Github/ARIC/\nThis reproducible R Markdown analysis was created with workflowr (version 1.6.2). The Checks tab describes the reproducibility checks that were applied when the results were created. The Past versions tab lists the development history.\n\n\n\n\n\n\nR Markdown file: uncommitted changes\n\n\n\nThe R Markdown is untracked by Git. To know which version of the R Markdown file created these results, you’ll want to first commit it to the Git repo. If you’re still working on the analysis, you can ignore this warning. When you’re finished, you can run wflow_publish to commit the R Markdown file and build the HTML.\n\n\n\n\n\nEnvironment: empty\n\n\n\nGreat job! The global environment was empty. Objects defined in the global environment can affect the analysis in your R Markdown file in unknown ways. For reproduciblity it’s best to always run the code in an empty environment.\n\n\n\n\n\nSeed: set.seed(12345)\n\n\n\nThe command set.seed(12345) was run prior to running the code in the R Markdown file. Setting a seed ensures that any results that rely on randomness, e.g. subsampling or permutations, are reproducible.\n\n\n\n\n\nSession information: recorded\n\n\n\nGreat job! Recording the operating system, R version, and package versions is critical for reproducibility.\n\n\n\n\n\nCache: none\n\n\n\nNice! There were no cached chunks for this analysis, so you can be confident that you successfully produced the results during this run.\n\n\n\n\n\nFile paths: absolute\n\n\n\nUsing absolute paths to the files within your workflowr project makes it difficult for you and others to run your code on a different machine. Change the absolute path(s) below to the suggested relative path(s) to make your code more reproducible.\n\n\n\n\n\n\n\nabsolute\nrelative\n\n\n\n\n/Users/sabrinami/Github/ARIC/PWAS/PWAS_AA/Plasma_Protein_AA_hg38.pos\nPWAS/PWAS_AA/Plasma_Protein_AA_hg38.pos\n\n\n/Users/sabrinami/Github/ARIC/PWAS/PWAS_AA/Plasma_Protein_weights_AA\nPWAS/PWAS_AA/Plasma_Protein_weights_AA\n\n\n/Users/sabrinami/Github/ARIC/models/ARIC_AA_hg38.db\nmodels/ARIC_AA_hg38.db\n\n\n\n\n\n\n\n\nRepository version: No commits yet\n\n\n\nGreat! You are using Git for version control. Tracking code development and connecting the code version to the results is critical for reproducibility.\nNote that you need to be careful to ensure that all relevant files for the analysis have been committed to Git prior to generating the results (you can use wflow_publish or wflow_git_commit). workflowr only checks the R Markdown file, but you know if there are other scripts or data files that it depends on. Below is the status of the Git repository when the results were generated:\nUntracked files:\n    Untracked:  .DS_Store\n    Untracked:  .Rhistory\n    Untracked:  1000G_AFR_individuals.csv\n    Untracked:  ARIC_AA_hg38_validation_CAD.Rmd\n    Untracked:  ARIC_AA_hg38_validation_CAD.html\n    Untracked:  ARIC_EA_hg38_validation.Rmd\n    Untracked:  ARIC_EA_hg38_validation.html\n    Untracked:  ARIC_EA_hg38_validation_height.Rmd\n    Untracked:  ARIC_EA_hg38_validation_height.html\n    Untracked:  PWAS/\n    Untracked:  code/\n    Untracked:  covariances_EA_hg38.Rmd\n    Untracked:  covariances_EA_hg38.html\n    Untracked:  dosage_to_parquet.py\n    Untracked:  figure/\n    Untracked:  models/\n    Untracked:  results/\n    Untracked:  subset_parquet.py\n    Untracked:  test_data/\n    Untracked:  vcf_to_parquet.py\n    Untracked:  vcf_to_parquet.sh\n    Untracked:  weights_AA.Rmd\n    Untracked:  weights_EA.Rmd\n    Untracked:  weights_EA.html\nNote that any generated files, e.g. HTML, png, CSS, etc., are not included in this status report because it is ok for generated content to have uncommitted changes.\n\n\n\n\n\n\n\nThere are no past versions. Publish this analysis with wflow_publish() to start tracking its development.\n\n\n\n\n\nPWAS Schema\nPlasma_Protein_EA_hg38.pos is a text file with proteins (ID) and locations on their encoding genes, (CHR, P0, P1), as well as pointers to their weights files (WGT).\nThe weights file for each protein is in Plasma_Protein_weights_EA, in TWAS/Fusion format. When loaded, each .RDat file contains snps (snp info), wgt.matrix (weights), and cv.performance (cross validation) data. The columns of the snps table are chromosome (V1), rsid (V2), position (V4), effect allele (V5) and reference allele (V6). In the wgt.matrix table, the rownames are the rsids, and the columns are the weights derived from elastic net and top1 methods for each snp.\n\n\nLoad Libraries\nRun in R:\nsuppressPackageStartupMessages(library(RSQLite))\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(data.table))\nsuppressPackageStartupMessages(library(biomaRt))\n\n\nInitialize Extra Table\nThe file Plasma_Protein_EA_hg38.pos points to the TWAS/FUSION files that contains the weights for each gene. We create a dictionary file that joins gene name, TWAS/FUSION file name, and Ensembl ID.\nensembl = useEnsembl(biomart = \"genes\", dataset = \"hsapiens_gene_ensembl\")\nEnsembl site unresponsive, trying useast mirror\ngene_annot = getBM( attributes=\n                    c(\"ensembl_gene_id\",\n                      \"hgnc_symbol\"),\n                  values =TRUE,\n                  mart = ensembl)\ngene_annot = gene_annot[!duplicated(gene_annot$hgnc_symbol),]\ngene_list = read.table(\"/Users/sabrinami/Github/ARIC/PWAS/PWAS_AA/Plasma_Protein_AA_hg38.pos\", head=TRUE)\ngene_list = gene_list[2:6]\nSome of the genes in the PWAS do not have an Ensembl ID annotation, so we use HGNC symbol in its place.\nextra = left_join(gene_list, gene_annot, by=c(\"ID\"=\"hgnc_symbol\"))\nextra = extra %>% mutate(ensembl_gene_id = coalesce(ensembl_gene_id,ID))\nThen add columns to match PrediXcan format.\nextra$pred.perf.R2 <- NA\nextra$pred.perf.pval <- NA\nextra$pred.perf.qval <- NA\n\n\nConvert File to Dataframe\nmake_df will load a file and store its data as a dataframe. This is only for a single gene, so later will be repeated for all genes. The input is the name of the .RDat file, and it returns returns dataframe with gene, position, chromosome, ref allele, eff allele, and non-zero enet weights.\nmake_df <- function(file) {\n  if (file %in% extra$WGT) {\n    load(file)  \n    weights <- data.frame(wgt.matrix) \n    snps <- data.frame(snps) \n    \n    index = which(extra$WGT == file)\n    \n    weights$gene <- extra$ensembl_gene_id[index]\n    weights$rsid <- rownames(weights)\n    weights$varID <- paste(\"chr\",paste(snps$V1,snps$V4,snps$V6,snps$V5,\"b38\", sep=\"_\"), sep=\"\")\n    weights$ref_allele <- snps$V6\n    weights$eff_allele <- snps$V5\n    weights = filter(weights, enet != 0)[,c(3,4,5,6,7,1)]\n    rownames(weights) <- c()\n    weights = rename(weights, c(\"weight\"=\"enet\"))\n    \n    rsq = cv.performance[1,1]\n    pval = cv.performance[2,1]\n    extra$pred.perf.R2[index] = rsq\n    extra$pred.perf.pval[index] = pval\n    assign('extra',extra,envir=.GlobalEnv)\n    return(weights)\n  }\n}\n\n\nMake Weights Table\nFirst, combine .RDat file names in a vector. Then convert each of them to a dataframe in PrediXcan format, appending them in a weights table.\nsetwd(\"/Users/sabrinami/Github/ARIC/PWAS/PWAS_AA/Plasma_Protein_weights_AA\")\nfiles <- list.files(pattern = \"\\\\.RDat\")\n\nweights = data.frame()\nfor(i in 1:length(files)) {\n  weights <- rbind(weights, make_df(files[i]))\n}\n\n\nMake Extra Table\nGenerate number of snps for each gene from the weights table.\nextra = rename(extra, c(\"genename\"=\"ID\", \"gene\"=\"ensembl_gene_id\"))\nn.snps = weights %>% group_by(gene) %>% summarise(n.snps.in.model = n())\n`summarise()` ungrouping output (override with `.groups` argument)\nextra = inner_join(extra, n.snps)\nJoining, by = \"gene\"\nextra <- extra[,c(6,2,10,7,8,9)]\n\n\nWrite to SQLite Database\nCreate database connection, and write the weights and extra tables to database.\nmodel_db = \"/Users/sabrinami/Github/ARIC/models/ARIC_AA_hg38.db\"\nconn <- dbConnect(RSQLite::SQLite(), model_db)\ndbWriteTable(conn, \"weights\", weights, overwrite=TRUE)\ndbWriteTable(conn, \"extra\", extra, overwrite=TRUE)\nTo double check, confirm there is a weights and extra table, and show their contents.\ndbListTables(conn)\n[1] \"extra\"   \"weights\"\ndbGetQuery(conn, 'SELECT * FROM weights') %>% head\n             gene       rsid                  varID ref_allele eff_allele\n1 ENSG00000254521  rs3752135 chr19_51497370_T_G_b38          T          G\n2 ENSG00000254521  rs3826667 chr19_51500820_C_T_b38          C          T\n3 ENSG00000254521  rs3810114 chr19_51503097_T_C_b38          T          C\n4 ENSG00000254521 rs10418495 chr19_51503534_G_A_b38          G          A\n5 ENSG00000254521  rs8113048 chr19_51503888_T_G_b38          T          G\n6 ENSG00000254521  rs8112579 chr19_51504162_C_T_b38          C          T\n        weight\n1 -0.083643830\n2 -0.076623212\n3 -0.031837335\n4 -0.008567506\n5 -0.031837782\n6 -0.008003640\ndbGetQuery(conn, 'SELECT * FROM extra') %>% head\n             gene genename n.snps.in.model  pred.perf.R2 pred.perf.pval\n1 ENSG00000254521 SIGLEC12               8  0.1066109691   6.020845e-48\n2 ENSG00000111361   EIF2B1              20  0.0473916727   9.995404e-22\n3 ENSG00000198931     APRT              54  0.1955522022   1.229389e-90\n4 ENSG00000111674     ENO2             103  0.0253777708   2.520872e-12\n5 ENSG00000168610    STAT3              23  0.0901864931   1.617896e-40\n6 ENSG00000089127     OAS1              14 -0.0005060883   8.161129e-01\n  pred.perf.qval\n1             NA\n2             NA\n3             NA\n4             NA\n5             NA\n6             NA\nLastly, disconnect from database connection\ndbDisconnect(conn)\n\n\nSession information\n\nsessionInfo()\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] biomaRt_2.44.4    data.table_1.13.2 forcats_0.5.0     stringr_1.4.0    \n [5] dplyr_1.0.2       purrr_0.3.4       readr_1.4.0       tidyr_1.1.2      \n [9] tibble_3.0.4      ggplot2_3.3.2     tidyverse_1.3.0   RSQLite_2.2.1    \n\nloaded via a namespace (and not attached):\n [1] Biobase_2.48.0       httr_1.4.2           bit64_4.0.5         \n [4] jsonlite_1.7.1       modelr_0.1.8         assertthat_0.2.1    \n [7] askpass_1.1          BiocFileCache_1.12.1 highr_0.8           \n[10] stats4_4.0.3         blob_1.2.1           cellranger_1.1.0    \n[13] yaml_2.2.1           progress_1.2.2       pillar_1.4.6        \n[16] backports_1.1.10     glue_1.4.2           digest_0.6.27       \n[19] promises_1.1.1       rvest_0.3.6          colorspace_1.4-1    \n[22] htmltools_0.5.0      httpuv_1.5.4         XML_3.99-0.5        \n[25] pkgconfig_2.0.3      broom_0.7.2          haven_2.3.1         \n[28] scales_1.1.1         later_1.1.0.1        git2r_0.27.1        \n[31] openssl_1.4.3        generics_0.0.2       IRanges_2.22.2      \n[34] ellipsis_0.3.1       withr_2.3.0          BiocGenerics_0.34.0 \n[37] cli_2.1.0            magrittr_1.5         crayon_1.3.4        \n[40] readxl_1.3.1         memoise_1.1.0        evaluate_0.14       \n[43] fs_1.5.0             fansi_0.4.1          xml2_1.3.2          \n[46] tools_4.0.3          prettyunits_1.1.1    hms_0.5.3           \n[49] lifecycle_0.2.0      S4Vectors_0.26.1     munsell_0.5.0       \n[52] reprex_0.3.0         AnnotationDbi_1.50.3 compiler_4.0.3      \n[55] rlang_0.4.8          grid_4.0.3           rstudioapi_0.11     \n[58] rappdirs_0.3.1       rmarkdown_2.5        gtable_0.3.0        \n[61] curl_4.3             DBI_1.1.0            R6_2.4.1            \n[64] lubridate_1.7.9      knitr_1.30           bit_4.0.4           \n[67] workflowr_1.6.2      rprojroot_1.3-2      stringi_1.5.3       \n[70] parallel_4.0.3       Rcpp_1.0.5           vctrs_0.3.4         \n[73] dbplyr_1.4.4         tidyselect_1.1.0     xfun_0.18           \n\n\n\n```````````````````"
  },
  {
    "objectID": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/weights_EA.html",
    "href": "post/2021-09-08-generating-metaxcan-prediction-model-from-aric-pwas/weights_EA.html",
    "title": "Weights EA",
    "section": "",
    "text": "::: {.container-fluid .main-container} ::: {#header .fluid-row} # generate weights {#generate-weights .title .toc-ignore}\n\nsabrina-mi\n\n\n2020-06-22\n:::\nworkflowr\n\n\nSummary\nChecks\nPast versions\n\n::: tab-content ::: {#summary .tab-pane .fade .in .active} Last updated: 2021-09-08\nChecks: 5 2\nKnit directory: ~/Github/ARIC/\nThis reproducible R Markdown analysis was created with workflowr (version 1.6.2). The Checks tab describes the reproducibility checks that were applied when the results were created. The Past versions tab lists the development history.\n\n\n\n\n\n\nR Markdown file: uncommitted changes\n\n\n\nThe R Markdown is untracked by Git. To know which version of the R Markdown file created these results, you’ll want to first commit it to the Git repo. If you’re still working on the analysis, you can ignore this warning. When you’re finished, you can run wflow_publish to commit the R Markdown file and build the HTML.\n\n\n\n\n\nEnvironment: empty\n\n\n\nGreat job! The global environment was empty. Objects defined in the global environment can affect the analysis in your R Markdown file in unknown ways. For reproduciblity it’s best to always run the code in an empty environment.\n\n\n\n\n\nSeed: set.seed(12345)\n\n\n\nThe command set.seed(12345) was run prior to running the code in the R Markdown file. Setting a seed ensures that any results that rely on randomness, e.g. subsampling or permutations, are reproducible.\n\n\n\n\n\nSession information: recorded\n\n\n\nGreat job! Recording the operating system, R version, and package versions is critical for reproducibility.\n\n\n\n\n\nCache: none\n\n\n\nNice! There were no cached chunks for this analysis, so you can be confident that you successfully produced the results during this run.\n\n\n\n\n\nFile paths: absolute\n\n\n\nUsing absolute paths to the files within your workflowr project makes it difficult for you and others to run your code on a different machine. Change the absolute path(s) below to the suggested relative path(s) to make your code more reproducible.\n\n\n\n\n\n\n\nabsolute\nrelative\n\n\n\n\n/Users/sabrinami/Github/ARIC/PWAS/PWAS_EA/Plasma_Protein_EA_hg38.pos\nPWAS/PWAS_EA/Plasma_Protein_EA_hg38.pos\n\n\n/Users/sabrinami/Github/ARIC/PWAS/PWAS_EA/Plasma_Protein_weights_EA\nPWAS/PWAS_EA/Plasma_Protein_weights_EA\n\n\n/Users/sabrinami/Github/ARIC/models/ARIC_EA_hg38.db\nmodels/ARIC_EA_hg38.db\n\n\n\n\n\n\n\n\nRepository version: No commits yet\n\n\n\nGreat! You are using Git for version control. Tracking code development and connecting the code version to the results is critical for reproducibility.\nNote that you need to be careful to ensure that all relevant files for the analysis have been committed to Git prior to generating the results (you can use wflow_publish or wflow_git_commit). workflowr only checks the R Markdown file, but you know if there are other scripts or data files that it depends on. Below is the status of the Git repository when the results were generated:\nUntracked files:\n    Untracked:  .DS_Store\n    Untracked:  .Rhistory\n    Untracked:  ARIC_EA_hg38_validation.Rmd\n    Untracked:  ARIC_EA_hg38_validation.html\n    Untracked:  ARIC_EA_hg38_validation_height.Rmd\n    Untracked:  ARIC_EA_hg38_validation_height.html\n    Untracked:  PWAS/\n    Untracked:  code/\n    Untracked:  covariances_EA_hg38.Rmd\n    Untracked:  covariances_EA_hg38.html\n    Untracked:  figure/\n    Untracked:  models/\n    Untracked:  results/\n    Untracked:  test_data/\n    Untracked:  weights_EA.Rmd\n    Untracked:  weights_EA.html\nNote that any generated files, e.g. HTML, png, CSS, etc., are not included in this status report because it is ok for generated content to have uncommitted changes.\n\n\n\n\n\n\n\nThere are no past versions. Publish this analysis with wflow_publish() to start tracking its development.\n\n\n\n\n\n\n:::\n\n\n\n\ntitle: Weights EA author: Sabrina Mi date: ‘2021-09-08’\n\n\n\n\n\n\ngenerate weights\n\nsabrina-mi\n\n\n2020-06-22\n\n\n\nworkflowr\n\n\nSummary\nChecks\nPast versions\n\n\n\nLast updated: 2021-09-08\nChecks: 5 2\nKnit directory: ~/Github/ARIC/\nThis reproducible R Markdown analysis was created with workflowr (version 1.6.2). The Checks tab describes the reproducibility checks that were applied when the results were created. The Past versions tab lists the development history.\n\n\n\n\n\n\nR Markdown file: uncommitted changes\n\n\n\nThe R Markdown is untracked by Git. To know which version of the R Markdown file created these results, you’ll want to first commit it to the Git repo. If you’re still working on the analysis, you can ignore this warning. When you’re finished, you can run wflow_publish to commit the R Markdown file and build the HTML.\n\n\n\n\n\nEnvironment: empty\n\n\n\nGreat job! The global environment was empty. Objects defined in the global environment can affect the analysis in your R Markdown file in unknown ways. For reproduciblity it’s best to always run the code in an empty environment.\n\n\n\n\n\nSeed: set.seed(12345)\n\n\n\nThe command set.seed(12345) was run prior to running the code in the R Markdown file. Setting a seed ensures that any results that rely on randomness, e.g. subsampling or permutations, are reproducible.\n\n\n\n\n\nSession information: recorded\n\n\n\nGreat job! Recording the operating system, R version, and package versions is critical for reproducibility.\n\n\n\n\n\nCache: none\n\n\n\nNice! There were no cached chunks for this analysis, so you can be confident that you successfully produced the results during this run.\n\n\n\n\n\nFile paths: absolute\n\n\n\nUsing absolute paths to the files within your workflowr project makes it difficult for you and others to run your code on a different machine. Change the absolute path(s) below to the suggested relative path(s) to make your code more reproducible.\n\n\n\n\n\n\n\nabsolute\nrelative\n\n\n\n\n/Users/sabrinami/Github/ARIC/PWAS/PWAS_EA/Plasma_Protein_EA_hg38.pos\nPWAS/PWAS_EA/Plasma_Protein_EA_hg38.pos\n\n\n/Users/sabrinami/Github/ARIC/PWAS/PWAS_EA/Plasma_Protein_weights_EA\nPWAS/PWAS_EA/Plasma_Protein_weights_EA\n\n\n/Users/sabrinami/Github/ARIC/models/ARIC_EA_hg38.db\nmodels/ARIC_EA_hg38.db\n\n\n\n\n\n\n\n\nRepository version: No commits yet\n\n\n\nGreat! You are using Git for version control. Tracking code development and connecting the code version to the results is critical for reproducibility.\nNote that you need to be careful to ensure that all relevant files for the analysis have been committed to Git prior to generating the results (you can use wflow_publish or wflow_git_commit). workflowr only checks the R Markdown file, but you know if there are other scripts or data files that it depends on. Below is the status of the Git repository when the results were generated:\nUntracked files:\n    Untracked:  .DS_Store\n    Untracked:  .Rhistory\n    Untracked:  ARIC_EA_hg38_validation.Rmd\n    Untracked:  ARIC_EA_hg38_validation.html\n    Untracked:  ARIC_EA_hg38_validation_height.Rmd\n    Untracked:  ARIC_EA_hg38_validation_height.html\n    Untracked:  PWAS/\n    Untracked:  code/\n    Untracked:  covariances_EA_hg38.Rmd\n    Untracked:  covariances_EA_hg38.html\n    Untracked:  figure/\n    Untracked:  models/\n    Untracked:  results/\n    Untracked:  test_data/\n    Untracked:  weights_EA.Rmd\n    Untracked:  weights_EA.html\nNote that any generated files, e.g. HTML, png, CSS, etc., are not included in this status report because it is ok for generated content to have uncommitted changes.\n\n\n\n\n\n\n\nThere are no past versions. Publish this analysis with wflow_publish() to start tracking its development.\n\n\n\n\n\nPWAS Schema\nPlasma_Protein_EA_hg38.pos is a text file with proteins (ID) and locations on their encoding genes, (CHR, P0, P1), as well as pointers to their weights files (WGT).\nThe weights file for each protein is in Plasma_Protein_weights_EA, in TWAS/Fusion format. When loaded, each .RDat file contains snps (snp info), wgt.matrix (weights), and cv.performance (cross validation) data. The columns of the snps table are chromosome (V1), rsid (V2), position (V4), effect allele (V5) and reference allele (V6). In the wgt.matrix table, the rownames are the rsids, and the columns are the weights derived from elastic net and top1 methods for each snp.\n\n\nLoad Libraries\nRun in R:\nsuppressPackageStartupMessages(library(RSQLite))\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(data.table))\nsuppressPackageStartupMessages(library(biomaRt))\n\n\nInitialize Extra Table\nThe file Plasma_Protein_EA_hg38.pos points to the TWAS/FUSION files that contains the weights for each gene. We create a dictionary file that joins gene name, TWAS/FUSION file name, and Ensembl ID.\nensembl = useEnsembl(biomart = \"genes\", dataset = \"hsapiens_gene_ensembl\")\ngene_annot = getBM( attributes=\n                    c(\"ensembl_gene_id\",\n                      \"hgnc_symbol\"),\n                  values =TRUE,\n                  mart = ensembl)\ngene_annot = gene_annot[!duplicated(gene_annot$hgnc_symbol),]\ngene_list = read.table(\"/Users/sabrinami/Github/ARIC/PWAS/PWAS_EA/Plasma_Protein_EA_hg38.pos\", head=TRUE)\ngene_list = gene_list[2:6]\nSome of the genes in the PWAS do not have an Ensembl ID annotation, so we use HGNC symbol in its place.\nextra = left_join(gene_list, gene_annot, by=c(\"ID\"=\"hgnc_symbol\"))\nextra = extra %>% mutate(ensembl_gene_id = coalesce(ensembl_gene_id,ID))\nThen add columns to match PrediXcan format.\nextra$pred.perf.R2 <- NA\nextra$pred.perf.pval <- NA\nextra$pred.perf.qval <- NA\n\n\nConvert File to Dataframe\nmake_df will load a file and store its data as a dataframe. This is only for a single gene, so later will be repeated for all genes. The input is the name of the .RDat file, and it returns returns dataframe with gene, position, chromosome, ref allele, eff allele, and non-zero enet weights.\nmake_df <- function(file) {\n  if (file %in% extra$WGT) {\n    load(file)  \n    weights <- data.frame(wgt.matrix) \n    snps <- data.frame(snps) \n    \n    index = which(extra$WGT == file)\n    \n    weights$gene <- extra$ensembl_gene_id[index]\n    weights$rsid <- rownames(weights)\n    weights$varID <- paste(\"chr\",paste(snps$V1,snps$V4,snps$V6,snps$V5,\"b38\", sep=\"_\"), sep=\"\")\n    weights$ref_allele <- snps$V6\n    weights$eff_allele <- snps$V5\n    weights = filter(weights, enet != 0)[,c(3,4,5,6,7,1)]\n    rownames(weights) <- c()\n    weights = rename(weights, c(\"weight\"=\"enet\"))\n    \n    rsq = cv.performance[1,1]\n    pval = cv.performance[2,1]\n    extra$pred.perf.R2[index] = rsq\n    extra$pred.perf.pval[index] = pval\n    assign('extra',extra,envir=.GlobalEnv)\n    return(weights)\n  }\n}\n\n\nMake Weights Table\nFirst, combine .RDat file names in a vector. Then convert each of them to a dataframe in PrediXcan format, appending them in a weights table.\nsetwd(\"/Users/sabrinami/Github/ARIC/PWAS/PWAS_EA/Plasma_Protein_weights_EA\")\nfiles <- list.files(pattern = \"\\\\.RDat\")\n\nweights = data.frame()\nfor(i in 1:length(files)) {\n  weights <- rbind(weights, make_df(files[i]))\n}\n\n\nMake Extra Table\nGenerate number of snps for each gene from the weights table.\nextra = rename(extra, c(\"genename\"=\"ID\", \"gene\"=\"ensembl_gene_id\"))\nn.snps = weights %>% group_by(gene) %>% summarise(n.snps.in.model = n())\n`summarise()` ungrouping output (override with `.groups` argument)\nextra = inner_join(extra, n.snps)\nJoining, by = \"gene\"\nextra <- extra[,c(6,2,10,7,8,9)]\n\n\nWrite to SQLite Database\nCreate database connection, and write the weights and extra tables to database.\nmodel_db = \"/Users/sabrinami/Github/ARIC/models/ARIC_EA_hg38.db\"\nconn <- dbConnect(RSQLite::SQLite(), model_db)\ndbWriteTable(conn, \"weights\", weights, overwrite=TRUE)\ndbWriteTable(conn, \"extra\", extra, overwrite=TRUE)\nTo double check, confirm there is a weights and extra table, and show their contents.\ndbListTables(conn)\n[1] \"extra\"   \"weights\"\ndbGetQuery(conn, 'SELECT * FROM weights') %>% head\n             gene        rsid                  varID ref_allele eff_allele\n1 ENSG00000254521 rs528743654 chr19_51442871_G_A_b38          G          A\n2 ENSG00000254521 rs150832888 chr19_51472073_G_A_b38          G          A\n3 ENSG00000254521   rs3752135 chr19_51497370_T_G_b38          T          G\n4 ENSG00000254521   rs3826667 chr19_51500820_C_T_b38          C          T\n5 ENSG00000254521   rs3810109 chr19_51501297_C_T_b38          C          T\n6 ENSG00000254521   rs3810114 chr19_51503097_T_C_b38          T          C\n        weight\n1  0.002895501\n2  0.010318968\n3 -0.126489008\n4 -0.130910477\n5 -0.039422600\n6 -0.022390375\ndbGetQuery(conn, 'SELECT * FROM extra') %>% head\n             gene genename n.snps.in.model pred.perf.R2 pred.perf.pval\n1 ENSG00000254521 SIGLEC12              14  0.315496028   0.000000e+00\n2 ENSG00000197943    PLCG2              68  0.051123646   1.694797e-84\n3 ENSG00000230124    ACBD6              11  0.007350099   1.816883e-13\n4 ENSG00000198931     APRT              51  0.013318563   4.921077e-23\n5 ENSG00000111674     ENO2              11  0.012191463   3.150181e-21\n6 ENSG00000168610    STAT3              32  0.085432419  2.835291e-142\n  pred.perf.qval\n1             NA\n2             NA\n3             NA\n4             NA\n5             NA\n6             NA\nLastly, disconnect from database connection\ndbDisconnect(conn)\n\n\nSession information\n\nsessionInfo()\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] biomaRt_2.44.4    data.table_1.13.2 forcats_0.5.0     stringr_1.4.0    \n [5] dplyr_1.0.2       purrr_0.3.4       readr_1.4.0       tidyr_1.1.2      \n [9] tibble_3.0.4      ggplot2_3.3.2     tidyverse_1.3.0   RSQLite_2.2.1    \n\nloaded via a namespace (and not attached):\n [1] Biobase_2.48.0       httr_1.4.2           bit64_4.0.5         \n [4] jsonlite_1.7.1       modelr_0.1.8         assertthat_0.2.1    \n [7] askpass_1.1          BiocFileCache_1.12.1 highr_0.8           \n[10] stats4_4.0.3         blob_1.2.1           cellranger_1.1.0    \n[13] yaml_2.2.1           progress_1.2.2       pillar_1.4.6        \n[16] backports_1.1.10     glue_1.4.2           digest_0.6.27       \n[19] promises_1.1.1       rvest_0.3.6          colorspace_1.4-1    \n[22] htmltools_0.5.0      httpuv_1.5.4         XML_3.99-0.5        \n[25] pkgconfig_2.0.3      broom_0.7.2          haven_2.3.1         \n[28] scales_1.1.1         later_1.1.0.1        git2r_0.27.1        \n[31] openssl_1.4.3        generics_0.0.2       IRanges_2.22.2      \n[34] ellipsis_0.3.1       withr_2.3.0          BiocGenerics_0.34.0 \n[37] cli_2.1.0            magrittr_1.5         crayon_1.3.4        \n[40] readxl_1.3.1         memoise_1.1.0        evaluate_0.14       \n[43] fs_1.5.0             fansi_0.4.1          xml2_1.3.2          \n[46] tools_4.0.3          prettyunits_1.1.1    hms_0.5.3           \n[49] lifecycle_0.2.0      S4Vectors_0.26.1     munsell_0.5.0       \n[52] reprex_0.3.0         AnnotationDbi_1.50.3 compiler_4.0.3      \n[55] rlang_0.4.8          grid_4.0.3           rstudioapi_0.11     \n[58] rappdirs_0.3.1       rmarkdown_2.5        gtable_0.3.0        \n[61] curl_4.3             DBI_1.1.0            R6_2.4.1            \n[64] lubridate_1.7.9      knitr_1.30           bit_4.0.4           \n[67] workflowr_1.6.2      rprojroot_1.3-2      stringi_1.5.3       \n[70] parallel_4.0.3       Rcpp_1.0.5           vctrs_0.3.4         \n[73] dbplyr_1.4.4         tidyselect_1.1.0     xfun_0.18           \n\n\n\n```````````````````"
  },
  {
    "objectID": "post/2020-11-11-installing-tensorqtl-module/index.html",
    "href": "post/2020-11-11-installing-tensorqtl-module/index.html",
    "title": "Installing tensorqtl module",
    "section": "",
    "text": "Installing tensorqtl requires pytorch which is based on gpus but there is also a cpu based version.\nCRI has set up pytorch for cpus as in a conda environment and that is what I am going to use to set up tensorqtl.\nI will install the tensorqtl in im-lab share space for lab use."
  },
  {
    "objectID": "post/2020-11-11-installing-tensorqtl-module/index.html#steps-for-installation",
    "href": "post/2020-11-11-installing-tensorqtl-module/index.html#steps-for-installation",
    "title": "Installing tensorqtl module",
    "section": "Steps for installation",
    "text": "Steps for installation\n\nCreate a directory for the environment\nmkdir -p /gpfs/data/im-lab/nas40t2/bin/envs\ncd /gpfs/data/im-lab/nas40t2/bin/envs\nCopy the pytorch environment into this new directory and name it tensorqtl\ncp -r /apps/software/gcc-6.2.0/miniconda3/4.7.10/envs/pytorch-1.4.0-cpu_py37 tensorqtl\nNow we have the pytorch setup environment next we are going to set up tensorqtl\nChecking if requirements are available\nActivate the conda environment\nconda activate /gpfs/data/im-lab/nas40t2/bin/envs/tensorqtl\nIn this environment when you test the pip command its not executable because the environment has python2. We need to upgrade the environment to use python3.\nTest the availability of pip and python3 using the following commands\npython3 --version\npip3 --version\npip --version\nIf you get error then you definitely need to set up these tools\nSet up python3\nInstall python3 which works with the set up pip\nconda install python==3.8.0\nInstall tensorqtl\nTensorqtl is available from pip\npip install tensorqtl\nOnce installation is successful install the dependecies\nInstall the rpy2 dependency\nconda install rpy2\nTest tensorqtl\n python3 -m tensorqtl --help\nClean up\nConda caches all these packages which consume a lot of disk space. The need to be removed;\n conda clean --all\n\nNB: This environment is available for lab use. To activate the the environment for use just activate it\n  conda activate /gpfs/data/im-lab/nas40t2/bin/envs/tensorqtl\nHappy QTL mapping!!!"
  },
  {
    "objectID": "post/2020-06-16-introstatgen-r-studio-servers-using-google-cloud/index.html",
    "href": "post/2020-06-16-introstatgen-r-studio-servers-using-google-cloud/index.html",
    "title": "IntroStatGen R Studio Servers using Google Cloud",
    "section": "",
    "text": "For the one-day seminar, we had a hands-on lab where we decided we needed to set up R Studio Servers. The servers needed pre-loaded data, access to a terminal, pre-compiled binaries for torus and fastenloc, and the correct python/R/Linux environments to run all of our analyses. Here’s a guide about how we set up that server.\n\n\nTo set everything up, we had a basic workflow: 1. Create a new VM. 1. Configure the VM as an RStudio server with everything installed, downloaded, etc. 1. Take a snapshot of this VM. 1. Spin up a bunch of new VMs from this snapshot.\nMost of the time (and therefore most of this document) was spent on step 2. Installing, compiling, configuring, uploading, and permissions-ing was the long part. Anyway, you won’t have to do all of that if you want to use the most current snapshot. It’s called rstudio-final-2020-06-12.\nTo spin up multiple VMs, we used Google’s command line tools. Most commands in the Google Cloud Console can be replicated in the command line, and just before creating a VM or Snapshot in the Console, you can find a link which gives you the analogous command. Here is what we used to spin up an array:\n$ cat gcloud_init.sh\ngcloud compute --project \"introstatgen\" disks create \"qgt-${1}\" \\\n    --size \"50\" --zone \"us-central1-a\" \\\n    --source-snapshot \"rstudio-final-2020-06-12\" --type \"pd-standard\"\n\ngcloud beta compute --project=introstatgen instances create qgt-${1} \\\n    --zone=us-central1-a --machine-type=custom-1-6656 --subnet=default \\\n    --network-tier=PREMIUM --maintenance-policy=MIGRATE \\\n    --service-account=10877517008-compute@developer.gserviceaccount.com \\\n    --scopes=https://www.googleapis.com/auth/devstorage.read_only,https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring.write,https://www.googleapis.com/auth/servicecontrol,https://www.googleapis.com/auth/service.management.readonly,https://www.googleapis.com/auth/trace.append \\\n     --tags=http-server --disk=name=qgt-${1},device-name=qgt-${1},mode=rw,boot=yes,auto-delete=yes --reservation-affinity=any\n$ for i in {0..35} ; do bash gcloud_init.sh $i ; done\nWhich made 36 different servers, each with RStudio available on port 8787. To get a sense of the size of the servers, they were initiated with 50GB of disk space, 6.5 GB of RAM, and 1 processor. When we used Google Cloud’s smallest VM (which only had ~3GB RAM) there was a memory error when running S-MultiXcan.\n\n\n\nThis tutorial link was pretty helpful. One thing which took some figuring out was that the command listed under the Install R on your VM heading. The command listed downloaded a version of R incompatible with the Ubuntu version running on the VM. But this link had useful information as well as a directory of the R/Ubuntu versions for download.\nThe download and installation process for R Studio Server was documented very well on their website. R Studio ran automatically after the download, so I didn’t even need a startup script for the VM clones. It just runs automatically, I guess."
  },
  {
    "objectID": "post/2020-06-16-introstatgen-r-studio-servers-using-google-cloud/index.html#add-python-environment",
    "href": "post/2020-06-16-introstatgen-r-studio-servers-using-google-cloud/index.html#add-python-environment",
    "title": "IntroStatGen R Studio Servers using Google Cloud",
    "section": "Add Python Environment",
    "text": "Add Python Environment\nWe used Anaconda. The installation was performed when logged in as the student user. Doing it as my own user and then changing permissions was a nightmare. The installer script can be downloaded using something like curl -O https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh. Check here for the latest version.\nA conda environment was defined using a yaml environment file link"
  },
  {
    "objectID": "post/2020-06-16-introstatgen-r-studio-servers-using-google-cloud/index.html#add-fastenloc-and-torus",
    "href": "post/2020-06-16-introstatgen-r-studio-servers-using-google-cloud/index.html#add-fastenloc-and-torus",
    "title": "IntroStatGen R Studio Servers using Google Cloud",
    "section": "Add fastenloc and torus",
    "text": "Add fastenloc and torus\nFastenloc and torus can be compiled pretty easily on Ubuntu. One may need to install a few libraries using apt. Make sure to compile static versions, because these binaries should end up in a folder at /home/student/bin/, and the student user may not have the necessary permissions to find linked libraries.\nAfter compiling static versions, move them to /home/student/bin/; make sure to change owner to student and make them executable by student. It is also good to automatically add /home/student/bin to the PATH variable, which can be achieved by modifying the file at /home/student/.bashrc."
  },
  {
    "objectID": "post/2020-06-16-introstatgen-r-studio-servers-using-google-cloud/index.html#add-data",
    "href": "post/2020-06-16-introstatgen-r-studio-servers-using-google-cloud/index.html#add-data",
    "title": "IntroStatGen R Studio Servers using Google Cloud",
    "section": "Add Data",
    "text": "Add Data\nWe used Box (this repo here) to gather and store data for this version of the class, and I didn’t find a good way to add/update data from Box to the VM. I ended up downloading from Box to my machine, and then scp-ing it to the VM. This means that each time the data in Box changed, I had to re-upload or manually update the data on the VM. Not pretty. I hope (hope) there is some way to download from Box out there, and a custom download script could be added to the VM creation script, so that a fresh version of the Box repository is added with each new VM.\nThe data should end up in /home/student/ and should be owned, readable, and writeable by the student."
  },
  {
    "objectID": "post/2020-06-16-introstatgen-r-studio-servers-using-google-cloud/index.html#add-lab-documents",
    "href": "post/2020-06-16-introstatgen-r-studio-servers-using-google-cloud/index.html#add-lab-documents",
    "title": "IntroStatGen R Studio Servers using Google Cloud",
    "section": "Add Lab Documents",
    "text": "Add Lab Documents\nLog in as the student, and clone the lab documents link again into the student’s home directory."
  },
  {
    "objectID": "post/2020-06-16-introstatgen-r-studio-servers-using-google-cloud/index.html#make-the-server-image-publicly-available",
    "href": "post/2020-06-16-introstatgen-r-studio-servers-using-google-cloud/index.html#make-the-server-image-publicly-available",
    "title": "IntroStatGen R Studio Servers using Google Cloud",
    "section": "Make the Server Image Publicly Available",
    "text": "Make the Server Image Publicly Available\nIn addition to the snapshot, there is a publicly available image named intro-stat-gen-rstudio-server-2020-06-16. It was made publicly available by this command (suggested by this page )\ngcloud compute images add-iam-policy-binding intro-stat-gen-rstudio-server-2020-06-16 \\\n    --member='allAuthenticatedUsers' \\\n    --role='roles/compute.imageUser"
  },
  {
    "objectID": "post/2020-11-23-how-to-convert-gtex-v8-model-to-hg19-based-on-uk-biobank-snp-set-mapping/index.html",
    "href": "post/2020-11-23-how-to-convert-gtex-v8-model-to-hg19-based-on-uk-biobank-snp-set-mapping/index.html",
    "title": "How to convert GTEx v8 model to hg19 based on UK Biobank SNP set mapping",
    "section": "",
    "text": "/gpfs/data/im-lab/nas40t2/Data/References/mappings/UKB2GTEx_mapping.txt.gz contains information for variants in UK Biobank genotypes. The columns are\n\n\nShow the code\nvariant chromosome  position    non_effect_allele   effect_allele   rsid    zscore  panel_variant_id\n1:692794:CA:C   chr1    757414  CA  C   1:692794_CA_C   1   NA\n1:693731:A:G    chr1    758351  A   G   rs12238997  1   chr1_758351_A_G_b38\n1:707522:G:C    chr1    772142  G   C   rs371890604 1   chr1_772142_G_C_b38\n1:717587:G:A    chr1    782207  G   A   rs144155419 1   chr1_782207_G_A_b38\n\n\nMetaXcan prediction models has its own format to identify variants, so a mapping file like UKB2GTEx_mapping.txt.gz allows us to convert the variants in prediction models to the corresponding ID used in UK Biobank data. The variant column has the variant’s ID in the UK Biobank data and the panel_variant_id column has it’s ID used in prediction models, specifically the GTEx V8 models.\n/gpfs/data/im-lab/nas40t2/Data/References/mappings/map_GTEx_v8_models_to_UKB_SNPset.tar has a mashr model trained on GTEx V8 Whole Blood tissue along with its covariance matrix. They have variant ids in the panel_variant_id chr_pos_ref_alt_b38 format, which we want to swap to the UK Biobank format. Unpack it:\n\n\nShow the code\ntar -xvf /gpfs/data/im-lab/nas40t2/Data/References/mappings/map_GTEx_v8_models_to_UKB_SNPset.tar\n\n\nThe prediction models are SQLite databases, which can be queried in R with the RSQLite package.\nStart by reading the prediction model into R.\n\n\nShow the code\nlibrary(data.table)\nmapping <- fread(\"/gpfs/data/im-lab/nas40t2/Data/References/mappings/UKB2GTEx_mapping.txt.gz\")\n\n\nNext, load the prediction model.\n\n\nShow the code\nlibrary(RSQLite)\nconn <- dbConnect(RSQLite::SQLite(), \"/gpfs/data/im-lab/nas40t2/Data/References/mappings/map_GTEx_v8_models_to_UKB_SNPset/mashr_Whole_Blood.db\")\ndbListTables(conn)\nweights <- dbGetQuery(conn, 'SELECT * FROM weights')\nextra <- dbGetQuery(conn, 'SELECT * FROM extra')\ndbDisconnect(conn)\n\n\nThe mashr prediction model has two tables weights and extra, and the dbGetQuery calls pull all entries from the weights and extra table.\nWe only need the two columns, variant and panel_variant_id. Then we inner join the mapping with the variants in the weights table.\n\n\nShow the code\nlibrary(tidyverse)\nmapping <- select(mapping, variant, panel_variant_id)\nmapped_weights <- inner_join(weights, mapping, by=c(\"varID\" = \"panel_variant_id\"))\nmapped_weights <- mapped_weights %>% mutate(varID = variant) %>% select(-variant)\n\n\nUpdate the n.snps.in.model column in the extra table:\n\n\nShow the code\nn.snps <- mapped_weights %>% group_by(gene) %>% summarise(n.snps.in.model = n())\nupdated_extra <- inner_join(n.snps, extra %>% select(-n.snps.in.model), by=\"gene\")\n\n\nWrite the new tables to a new RSQLite database file.\n\n\nShow the code\nconn <- dbConnect(RSQLite::SQLite(), \"/gpfs/data/im-lab/nas40t2/Data/References/mappings/map_GTEx_v8_models_to_UKB_SNPset/mashr_Whole_Blood_UKB.db\")\ndbWriteTable(conn, \"weights\", mapped_weights)\ndbWriteTable(conn, \"extra\", updated_extra)\ndbDisconnect(conn)\n\n\nSimilarly, we inner join to convert both RSID1 and RSID2 columns in /gpfs/data/im-lab/nas40t2/Data/References/mappings/map_GTEx_v8_models_to_UKB_SNPset/map_GTEx_v8_models_to_UKB_SNPset/mashr_Whole_Blood.txt.gz\n\n\nShow the code\ncovariance <- fread(\"/gpfs/data/im-lab/nas40t2/Data/References/mappings/map_GTEx_v8_models_to_UKB_SNPset/map_GTEx_v8_models_to_UKB_SNPset/mashr_Whole_Blood.txt.gz\")\ncovariance <- inner_join(covariance, merged, by=c(\"RSID1\" = \"varID\")) %>% mutate(RSID1 = variant) %>% select(-variant)\ncovariance <- inner_join(covariance, merged, by=c(\"RSID2\" = \"varID\")) %>% mutate(RSID2 = variant) %>% select(-variant)\ncovariance <- covariance %>% select(GENE, variant.x, variant.y, VALUE) %>% rename(RSID1 = variant.x, RSID2 = variant.y) \nwrite.table(covariance, \"/gpfs/data/im-lab/nas40t2/Data/References/mappings/map_GTEx_v8_models_to_UKB_SNPset/map_GTEx_v8_models_to_UKB_SNPset/mashr_Whole_Blood_UKB.txt\")"
  },
  {
    "objectID": "post/2020-11-04-exploration-on-regressing-out-pcs/index.html",
    "href": "post/2020-11-04-exploration-on-regressing-out-pcs/index.html",
    "title": "Exploration on regressing out PCs",
    "section": "",
    "text": "\\[\\newcommand{\\var}{\\text{Var}}\n\\newcommand{\\E}{\\text{E}}\n\\newcommand{\\diag}{\\text{diag}}\n\\newcommand{\\cov}{\\text{Cov}}\\]\n\nAbout\nRecently, we encountered a data matrix with correlated columns. To account for this, we thought about performing SVD on the matrix and then regressing out the left singular vectors from the columns (or equivalently regressing out the right singular vectors from the rows).\nIt turns out that we could removing a lot of correlation after regressing out top N PCs (which captures about 50% PVE). But the issue is that this approach is not removing all of the correlations. There are some left over correlations that cannot be removed by PCA (in our case we ended up having block wise diagonal correlation).\nAnd if we focus on the block of features that have remaining correlation and perform PCA for them, we cannot furthre remove any correlation anymore.\nNow that the question is why PCA works at first but fails afterwards. And more specifically, we want to know with what data pattern/characteristic, the PCA approach works and when it could not work out.\nHere, I do a very simple exploration on this problem. The arguments to deliver are the following:\n\nWhen regressing out PCs removes the correlation structure, it captures the common factors shared by almost all features.\nWhen the features do not share common factor, this approach could fail.\n\nHere we do a super simple simulation. Suppose we have \\(K = 16\\) hidden factors \\(z_1, \\cdots, z_K\\). The factor sharing pattern is governed by a structure matrix \\(S\\). For each feature \\(x_{ij} = \\sum_k S_{kj} z_{ik}\\) and the data matrix is formed by \\(X = Z S\\) and we let \\(z_{ik} \\sim N(0, 1)\\). Here we impose two types of \\(S\\) and, in both cases, try to regress out the first PC from the data matrix \\(X = Z S\\).\n\n\nShow the code\nset.seed(2020)\nstandardize = function(x) {\n  apply(x, 2, function(y) { (y - mean(y)) / sd(y) })\n}\nsplit_by_pca = function(x, pve_cutoff = 0.5, npc = NULL) {\n  x = standardize(x)\n  res = svd(x)\n  v = res$v\n  if(is.null(npc)) {\n    pve = cumsum(res$d^2 / sum(res$d^2))\n    npc = sum(pve <= pve_cutoff) + 1\n  }\n  pc_mat = res$u[, 1 : npc, drop = F]\n  res = x - pc_mat %*% (t(pc_mat) %*% x)\n  list(residual = res, pc = pc_mat, v = v)\n}\nk = 16\nn = 1000\nstruct_mat = matrix(0, nrow = k, ncol = k)\nfor(j in 1 : k) {\n  struct_mat[j, j] = 1\n  if(j < k) {\n    struct_mat[j, j + 1] = 1\n  }\n  if(j > 1) {\n    struct_mat[j - 1, j] = 1\n  }\n}\nstruct_mat2 = matrix(0, nrow = k, ncol = k)\nstruct_mat2[4 : 10, 1] = 1\nstruct_mat2[1:3, 2] = 1\nstruct_mat2[11:k, 3] = 1\nfor(j in 1 : k) {\n  struct_mat2[j, j] = 1\n}\nzz = matrix(rnorm(n * k), nrow = n)\nx = zz %*% t(struct_mat) \nx2 = zz %*% t(struct_mat2) \ntmp = split_by_pca(x, npc = 1)\nres = tmp$residual\npar(mfrow=c(2,3))\nimage(struct_mat, main = 'Structure matrix')\nimage(cor(x), main = 'Correlation in X matrix')\nfor(k in 1 : 4) {\n  tmp = split_by_pca(x, npc = k)\n  res = tmp$residual\n  image(cor(res), main = paste0('Correlation in residual matrix, nPC = ', k))\n}\n\n\n\n\n\nShow the code\npar(mfrow=c(2,3))\nimage(struct_mat2, main = 'Structure matrix')\nimage(cor(x2), main = 'Correlation in X matrix')\nfor(k in 1 : 4) {\n  tmp2 = split_by_pca(x2, npc = k)\n  res2 = tmp2$residual\n  image(cor(res2), main = paste0('Correlation in residual matrix, nPC = ', k))\n}"
  },
  {
    "objectID": "post/2023-03-01-how-to-create-new-posts/index.html",
    "href": "post/2023-03-01-how-to-create-new-posts/index.html",
    "title": "How to create a new blog post",
    "section": "",
    "text": "create a new folder under post/new_folder\nname the folder with year-month-date-slug, 2023-03-02-descriptive-title\ncreate a file named index.qmd and add the following header, update the title, your name and the date"
  },
  {
    "objectID": "post/2023-03-01-how-to-create-new-posts/index.html#other-links",
    "href": "post/2023-03-01-how-to-create-new-posts/index.html#other-links",
    "title": "How to create a new blog post",
    "section": "other links",
    "text": "other links\nUseful tips for new posts here"
  },
  {
    "objectID": "post/2023-03-01-how-to-create-new-posts/index.html#functions",
    "href": "post/2023-03-01-how-to-create-new-posts/index.html#functions",
    "title": "How to create a new blog post",
    "section": "functions",
    "text": "functions"
  },
  {
    "objectID": "post/2023-03-28-erap2-finemapping/index.html",
    "href": "post/2023-03-28-erap2-finemapping/index.html",
    "title": "ERAP2 fine-mapping",
    "section": "",
    "text": "Show the code\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\n##PRE=\"/Users/margaretperry/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data \"\n##PRE=\"/Users/temi/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data\"\n## COPY THE DATE AND SLUG fields FROM THE HEADER\nSLUG=\"erap2-fine-mapping\" ## copy the slug from the header\nbDATE='2023-03-28' ## copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n\n## move data to DATA\n#tempodata=(\"~/Downloads/tempo/gwas_catalog_v1.0.2-associations_e105_r2022-04-07.tsv\")\n#system(glue::glue(\"cp {tempodata} {DATA}/\"))\n\n## system(glue(\"open {DATA}\")) ## this will open the folder \n\n\nERAP2 fine-mapping results DAPG\n\n\nShow the code\n## query\n## SELECT * FROM `gtex-awg-im.GTEx_V8_DAPG.variants_pip_eqtl` where gene like \"ENSG00000164308%\"\nerap2 = read_csv(glue(\"{DATA}/bquxjob_41de6a2f_18728cf1999.csv\"))\n\n\nRows: 2260 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): tissue, gene, variant_id\ndbl (4): rank, pip, log10_abvf, cluster_id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\n## SELECT * FROM `gtex-awg-im.GTEx_V8_DAPG.variants_pip_sqtl` where variant_id like \"chr5_96900192%\" order by pip desc\ntauras_snp = read_csv(glue(\"{DATA}/bquxjob_719c0131_18728db8878.csv\"))\n\n\nRows: 113 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): tissue, gene_id, variant_id\ndbl (4): rank, pip, log10_abvf, cluster_id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\n## finemapping for the intron affected by chr5_96900192\n## SELECT * FROM `gtex-awg-im.GTEx_V8_DAPG.variants_pip_sqtl` where gene_id like \"intron_5_96900189_96901506\" order by pip desc\n\nintron = read_csv(glue(\"{DATA}/bquxjob_41bc6351_18728e4ee94.csv\"))\n\n\nRows: 2439 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): tissue, gene_id, variant_id\ndbl (4): rank, pip, log10_abvf, cluster_id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\n##\nintron %>% filter(tissue==\"Cells_EBV-transformed_lymphocytes\") %>% arrange(desc(pip))\n\n\n# A tibble: 122 × 7\n   tissue                  gene_id  rank variant_id    pip log10_abvf cluster_id\n   <chr>                   <chr>   <dbl> <chr>       <dbl>      <dbl>      <dbl>\n 1 Cells_EBV-transformed_… intron…     2 chr5_9690… 0.250        25.8          2\n 2 Cells_EBV-transformed_… intron…     1 chr5_9690… 0.250        25.8          2\n 3 Cells_EBV-transformed_… intron…     3 chr5_9690… 0.218        27.7          1\n 4 Cells_EBV-transformed_… intron…     4 chr5_9690… 0.218        27.7          1\n 5 Cells_EBV-transformed_… intron…     5 chr5_9690… 0.0902       27.7          1\n 6 Cells_EBV-transformed_… intron…     7 chr5_9690… 0.0426       27.7          1\n 7 Cells_EBV-transformed_… intron…     6 chr5_9690… 0.0426       27.7          1\n 8 Cells_EBV-transformed_… intron…     8 chr5_9690… 0.0426       27.7          1\n 9 Cells_EBV-transformed_… intron…     9 chr5_9689… 0.0380       25.8          2\n10 Cells_EBV-transformed_… intron…    10 chr5_9690… 0.0371       25.8          2\n# ℹ 112 more rows\n\n\nShow the code\n## \n\n## erap2 %>% filter(pip>0.1) %>% group_by(variant_id) %>% summarise(sumpip=sum(pip),ntissues=n()) %>% ggplot(aes(variant_id,sumpip)) + geom_bar(stat = \"identity\") + geom_point() + ggtitle(\"ERAP2 expr: most tissues assign pip to 16728 & 16885\")\n\nerap2 %>% filter(pip>0.1) %>% ggplot(aes(variant_id,pip)) + geom_violin() + geom_boxplot(width=0.05,alpha=0.5,outlier.shape = NA) + geom_point() + ggtitle(\"ERAP2 expr: most tissues assign pip to 16728 & 16885\") + ylim(0,NA)\n\n\nWarning: Groups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\n\n\n\n\n\nShow the code\nprint(\"intron intron_5_96900189_96901506 \")\n\n\n[1] \"intron intron_5_96900189_96901506 \"\n\n\nShow the code\nintron %>% filter(pip>0.1) %>% ggplot(aes(variant_id,pip)) + geom_violin() + geom_boxplot(width=0.05,alpha=0.5,outlier.shape = NA) + geom_point() + ggtitle(\"ERAP2 intron_5_96900189_96901506:\") + ylim(0,NA) + coord_flip()\n\n\nWarning: Groups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\n\n\n\n\n\nShow the code\n#intron %>% filter(pip>0.1) %>% group_by(variant_id) %>% summarise(sumpip=sum(pip),ntissues=n()) %>% ggplot(aes(variant_id,sumpip)) + geom_bar(stat = \"identity\") + ggtitle(\"ERAP2 intron_5_96900189_96901506: \") + coord_flip()\n\n#ggplot(aes(variant_id,sumpip)) + geom_bar() \n\n\nCausal SNP according to the black death paper and others is rs2248374 chr5_96900192"
  },
  {
    "objectID": "post/2023-06-14-gwas-catalog/index.html",
    "href": "post/2023-06-14-gwas-catalog/index.html",
    "title": "gwas-catalog",
    "section": "",
    "text": "Show the code\nsuppressMessages(library(tidyverse))\n\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\n\nSLUG=\"gwas-catalog\" ## copy the slug from the header\nbDATE='2020-08-02' ## copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n\n##  system(glue(\"open {DATA}\")) ## this will open the folder \n\n\n\n\nShow the code\n## download fron https://www.ebi.ac.uk/gwas/api/search/downloads/alternative\n\n## DATA = \"/Users/haekyungim/Box/LargeFiles/imlab-data/data-Github/analysis-hki\"\ngwascat = read_tsv(glue(\"{DATA}/gwas_catalog_v1.0.2-associations_e109_r2023-06-03.tsv.gz\"), guess_max = 100000)\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n\n\nRows: 529481 Columns: 38\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (28): FIRST AUTHOR, JOURNAL, LINK, STUDY, DISEASE/TRAIT, INITIAL SAMPLE...\ndbl   (8): PUBMEDID, UPSTREAM_GENE_DISTANCE, DOWNSTREAM_GENE_DISTANCE, MERGE...\ndate  (2): DATE ADDED TO CATALOG, DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\n##\"gwas_catalog_v1.0.2-associations_e100_r2020-08-05.tsv\"\n##\"gwas_catalog_v1.0.2-associations_e100_r2020-07-14.tsv\"\n##\"gwas_catalog_v1.0.2-associations_e100_r2020-08-05.tsv\"\n\ndim(gwascat)\n\n\n[1] 529481     38\n\n\nShow the code\nglimpse(gwascat)\n\n\nRows: 529,481\nColumns: 38\n$ `DATE ADDED TO CATALOG`      <date> 2022-07-04, 2022-07-04, 2022-07-04, 2022…\n$ PUBMEDID                     <dbl> 33462482, 33462482, 33462482, 33462482, 3…\n$ `FIRST AUTHOR`               <chr> \"Ruhlemann MC\", \"Ruhlemann MC\", \"Ruhleman…\n$ DATE                         <date> 2021-01-18, 2021-01-18, 2021-01-18, 2021…\n$ JOURNAL                      <chr> \"Nat Genet\", \"Nat Genet\", \"Nat Genet\", \"N…\n$ LINK                         <chr> \"www.ncbi.nlm.nih.gov/pubmed/33462482\", \"…\n$ STUDY                        <chr> \"Genome-wide association study in 8,956 G…\n$ `DISEASE/TRAIT`              <chr> \"TestASV_20 (Phascolarctobacterium) preva…\n$ `INITIAL SAMPLE SIZE`        <chr> \"8,956 German ancestry individuals\", \"8,9…\n$ `REPLICATION SAMPLE SIZE`    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ REGION                       <chr> \"9p21.3\", NA, NA, \"17q24.3\", \"20q13.31\", …\n$ CHR_ID                       <chr> \"9\", NA, NA, \"17\", \"20\", \"3\", NA, NA, \"11…\n$ CHR_POS                      <chr> \"22175189\", NA, NA, \"72438652\", \"56989649…\n$ `REPORTED GENE(S)`           <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MAPPED_GENE                  <chr> \"CDKN2B-AS1 - DMRTA1\", NA, NA, \"LINC00511…\n$ UPSTREAM_GENE_ID             <chr> \"ENSG00000240498\", NA, NA, NA, \"ENSG00000…\n$ DOWNSTREAM_GENE_ID           <chr> \"ENSG00000176399\", NA, NA, NA, \"ENSG00000…\n$ SNP_GENE_IDS                 <chr> NA, NA, NA, \"ENSG00000227036\", NA, \"ENSG0…\n$ UPSTREAM_GENE_DISTANCE       <dbl> 47086, NA, NA, NA, 203562, NA, NA, NA, NA…\n$ DOWNSTREAM_GENE_DISTANCE     <dbl> 271635, NA, NA, NA, 179104, NA, NA, NA, N…\n$ `STRONGEST SNP-RISK ALLELE`  <chr> \"rs10965279-?\", \"chr11:60833276-?\", \"chr1…\n$ SNPS                         <chr> \"rs10965279\", \"chr11:60833276\", \"chr13:43…\n$ MERGED                       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ SNP_ID_CURRENT               <chr> \"10965279\", NA, NA, \"7223271\", \"910832\", …\n$ CONTEXT                      <chr> \"intergenic_variant\", NA, NA, \"intron_var…\n$ INTERGENIC                   <dbl> 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,…\n$ `RISK ALLELE FREQUENCY`      <chr> \"NR\", \"NR\", \"NR\", \"NR\", \"NR\", \"NR\", \"NR\",…\n$ `P-VALUE`                    <dbl> 5e-08, 2e-06, 9e-06, 9e-06, 8e-06, 3e-06,…\n$ PVALUE_MLOG                  <dbl> 7.301030, 5.698970, 5.045757, 5.045757, 5…\n$ `P-VALUE (TEXT)`             <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `OR or BETA`                 <dbl> 0.5047070, 0.3521481, 0.3854320, 0.427226…\n$ `95% CI (TEXT)`              <chr> \"[0.32-0.69] unit increase\", \"[0.21-0.5] …\n$ `PLATFORM [SNPS PASSING QC]` <chr> \"NR [6900000] (imputed)\", \"NR [6900000] (…\n$ CNV                          <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"…\n$ MAPPED_TRAIT                 <chr> \"gut microbiome measurement\", \"gut microb…\n$ MAPPED_TRAIT_URI             <chr> \"http://www.ebi.ac.uk/efo/EFO_0007874\", \"…\n$ `STUDY ACCESSION`            <chr> \"GCST90011694\", \"GCST90011694\", \"GCST9001…\n$ `GENOTYPING TECHNOLOGY`      <chr> \"Genome-wide genotyping array\", \"Genome-w…\n\n\nShow the code\ngwascat %>% count(MAPPED_TRAIT,CHR_POS) %>% dim()\n\n\n[1] 373864      3\n\n\nShow the code\nlength(unique(gwascat$MAPPED_TRAIT))\n\n\n[1] 8723\n\n\nShow the code\nlength(unique(gwascat$CHR_POS))\n\n\n[1] 256855\n\n\nShow the code\n## 2023-06-14 256855 distinct trait/variants\n## 2023-06-14 8723 distinct traits\n## 2020 146,359 distinct trait/variant\n## 2020 3,758 distinct traits\n\ngwascat_sig = gwascat %>% mutate(year=as.factor(lubridate::year(lubridate::as_date(`DATE ADDED TO CATALOG`)))) %>% filter(`P-VALUE`<5e-8)\n\ngwascat_sig %>% filter(year!=\"2023\") %>% ggplot(aes(year)) + geom_bar() + theme_bw(base_size = 15) + scale_x_discrete(breaks=c(\"2008\",\"2012\",\"2016\",\"2020\",\"2022\")) + xlab(\"year\") + ylab(\"GWAS loci reported p<5e-8\") + ggtitle(\"GWAS Catalog Downloaded 2023-06-14\")\n\n\n\n\n\nShow the code\n##ggsave(glue::glue(\"{DATA}/gwas-catalog/gwas-catalog-by-year.pdf\"))\n\n\n\nnumber of significant SNPs\n\n\n\nShow the code\ngwascat_sig %>% count(CHR_POS) %>% dim()\n\n\n[1] 185850      2\n\n\n\nnumber of significant trait/SNP pairs\n\n\n\nShow the code\ngwascat_sig %>% count(CHR_POS,MAPPED_TRAIT) %>% dim()\n\n\n[1] 288566      3\n\n\nin 2020 [1] 94664 3\n\nnumber of traits with significant SNPs\n\n\n\nShow the code\ngwascat_sig %>% count(MAPPED_TRAIT) %>% dim()\n\n\n[1] 7427    2\n\n\nin 2020 [1] 2584 2"
  },
  {
    "objectID": "post/2020-07-30-downloading-and-decrypting-dbgap-data/index.html",
    "href": "post/2020-07-30-downloading-and-decrypting-dbgap-data/index.html",
    "title": "Downloading and Decrypting dbGaP Data",
    "section": "",
    "text": "This page is about downloading and decrypting data from NCBI dbGaP. I will split up the instructions into Downloading and Decrypting"
  },
  {
    "objectID": "post/2020-07-30-downloading-and-decrypting-dbgap-data/index.html#downloading",
    "href": "post/2020-07-30-downloading-and-decrypting-dbgap-data/index.html#downloading",
    "title": "Downloading and Decrypting dbGaP Data",
    "section": "Downloading",
    "text": "Downloading\nTo download, one must be approved in the dbGaP controlled data access system, receive an email that the data is ready, and follow the given instructions.\n\nGet approved to download dbGaP data. I did this through eRA Commons.\nDownload IBM’s Aspera CLI: download is available here. Because I was downloading to CRI, I selected the Linux x86_64 version.\nRun the downloaded file. You might have to run chmod to make it executable. It should create a directory .aspera/ in your home directory.\nReceive an email about a new dataset available for download. Click on the email’s link to see your personal dbGaP Authorized Access: Downloads page. Then click Download, and there will be a pop-up giving you a command to run Aspera to do the download:\n\n\"%ASPERA_CONNECT_DIR%\\bin\\ascp\" -QTr -l 300M -k 1 -i \"%ASPERA_CONNECT_DIR%\\etc\\asperaweb_id_dsa.openssh\" -W <some_long_key> <some_dbGaP_server_address> .\n\nChange \"%ASPERA_CONNECT_DIR%\\bin\\ascp\" to aspera. If you’re running on CRI, the command is already in the PATH\nChange \"%ASPERA_CONNECT_DIR%\\etc\\asperaweb_id_dsa.openssh\" to ~/.aspera/etc/asperaweb_id_dsa.openssh. I’m not entirely sure if specifying a private key is necessary, but this is what I did.\nRun the command in the directory you want the data to go, or edit the . at the end of the command. Wondering where you should put the data? Read all of the section on decrypting, because the data needs to be inside a certain directory to be decrypted."
  },
  {
    "objectID": "post/2020-07-30-downloading-and-decrypting-dbgap-data/index.html#decrypting",
    "href": "post/2020-07-30-downloading-and-decrypting-dbgap-data/index.html#decrypting",
    "title": "Downloading and Decrypting dbGaP Data",
    "section": "Decrypting",
    "text": "Decrypting\nI decrypted using NCBI’s SRA Toolkit.\n\nBecause I was working on CRI, I downloaded the Linux version from here.\nThe project key (necessary for decryption) can be found on the My Projects tab of the dbGaP data portal. Download the key and move it to CRI.\nTo install and configure the SRA Toolkit, I used this guide which was very helpful.\nThe guide also walks through how to configure the toolkit, which seems like a lot because I ended up using just one command, vdb-decrypt, but it was necessary.\nWhen in the interactive program vdb-config -i, you need to do three things: set a home workspace, import the project key (the one you downloaded earlier), and set a project directory for the project corresponding to the key.\nI have the home workspace in my directory inside our CRI lab share, and the project directory inside the home workspace. 71954 and 71955 are two data downloads corresponding with this project.\n\nncbi\n`-- dbGaP-11644\n    |-- files\n    |   |-- 71954\n    |   `-- 71955\n    |-- nannot\n    |-- refseq\n    |-- sra\n    `-- wgs\n\nTo be decrypted, the data must be inside the project directory that you chose, so in this case, all of the data must be inside dbGaP-11644.\nWhen the project directory is set up and the data is inside, you can use the vdb-decrypt command to decrypt the data (SRA data is decrypted differently, and there’s a lot of documentation on the NCBI website about how to deal with that data.)"
  },
  {
    "objectID": "post/2020-12-01-predixcan-0-variant-mapping-issue/index.html",
    "href": "post/2020-12-01-predixcan-0-variant-mapping-issue/index.html",
    "title": "PrediXcan 0% variant mapping issue",
    "section": "",
    "text": "Many users had difficulties matching the genotype variant id to the prediction model variant id.\nHere is one example added to the PrediXcan tutorial where the matching was failing because of the on the fly option not taking into account that in the GTEx v8 vcf file, chromosomes are names as chr# whereas in other vcf’s (more common in hg19?) chromosomes are indicated by their number or letter (no chr prefix).\nA working example for the GTEx genotype data with GTEx v8 mashr models is shown below.\n\n\nShow the code\n\nexport PRE=/gpfs/data/im-lab/nas40t2/Data/test-PrediXcan-GTEx\nexport DATA=$PRE/data\nexport MODEL=$PRE/models\nexport RESULTS=$PRE/results/\nexport METAXCAN=$PRE/repos/MetaXcan-master/software\nexport VCFSMALL=$PRE/data/gtex-small-common-test.vcf.gz\n\nprintf \"Predict expression\\n\\n\"\n\npython3   $METAXCAN/Predict.py \\\n--model_db_path $PRE/models/gtex_v8_mashr/mashr_Whole_Blood.db \\\n--model_db_snp_key varID \\\n--vcf_genotypes  $VCFSMALL \\\n--vcf_mode genotyped \\\n--prediction_output $RESULTS/Whole_Blood__predict.txt  \\\n--prediction_summary_output $RESULTS/Whole_Blood__summary.txt \\\n--verbosity  \\\n--throw \\\n--on_the_fly_mapping METADATA \"{}_{}_{}_{}_b38\" \n\n\n** Thank you, Yanyu, for solving the mystery **"
  },
  {
    "objectID": "post/2021-07-29-installing-r-packages-without-admin-access/index.html",
    "href": "post/2021-07-29-installing-r-packages-without-admin-access/index.html",
    "title": "Installing R packages without admin access",
    "section": "",
    "text": "** Note that this content was copied from the Berkely Statistics Website Original article linked here\nLibrary Path Management\nFirst, note that in general, the administrators of the High Performance Computing clusters will install an R package on the system.\nHowever, you can also install packages locally within your home directory. So if you need a package quickly or on a one-time basis, or if the package is particularly specialized, you might install it locally.\nBy default, R searches a set of paths when you request actions involving libraries. The first path is used by default when invoking functions such as install.packages() leading to messages like this:\nmkdir: cannot create directory  '/server/linux/lib/R/site-library/00LOCK': Permission denied ERROR: failed to lock directory  '/server/linux/lib/R/site-library' for modifying\nFortunately, R provides a number of methods for controlling the library path to accommodate just about any user’s need.\n\n\nYou can modify R’s notion of your library path on a one-time basis by specifying the lib= argument to install.packages. Suppose there is a directory called MyRlibs in your home directory. The command:\ninstall.packages('caTools',lib='~/MyRlibs')\nwill install the specified package in your local directory. To access it, the lib.loc= argument of library must be used:\nlibrary('caTools',lib='~/MyRlibs')\nOne problem with this scheme is that if a local library invokes the library() function, it won’t know to also search the local library\n\n\n\nThe .libPaths() function accepts a character vector naming the libraries which are to be used as a search path. Note that it does not automatically retain directories which are already on the search path. Since the .libPaths() function returns the current search path when called with no arguments, a call like\n.libPaths(c('~/MyRlibs',.libPaths()))\nwill put your local directory at the beginning of the search path. This means that install.packages() will automatically put packages there, and the library() function will find libraries in your local directory without additional arguments.\n\n\n\nThe environmental variable R_LIBS is set by the script that invokes R, and can be overridden (in a shell startup file, for example) to customize your library path. This variable should be set to a colon-separated string of directories to search. Since it’s always set inside of an R session, the easiest way to get a starting point for it is to use Sys.getenv():\n> Sys.getenv('R_LIBS')\n[1] \"/usr/local/linux/lib/R/site-library:/usr/local/lib/R/site-library:/usr/lib/R/site-library:/usr/lib/R/library\"\nYou could then make a copy of this path, modify it, and set the R_LIBS environmental variable to that value in the shell or a startup script."
  },
  {
    "objectID": "post/2021-07-12-links-to-how-to-s/index.html",
    "href": "post/2021-07-12-links-to-how-to-s/index.html",
    "title": "Links to How To’s",
    "section": "",
    "text": "Yanyu’s notes on uploading to zenodo"
  },
  {
    "objectID": "post/2001-07-30-dbgap-project-renewal/index.html",
    "href": "post/2001-07-30-dbgap-project-renewal/index.html",
    "title": "dbGaP Project Renewal",
    "section": "",
    "text": "The email reminder to renew dbGaP projects should link to My Projects tab after logging in, where you can select the project to request renewal. For the most part, the application will walk through the steps if you click ‘save and continue’ at the bottom of each page, but there are tabs you have to remember to click on that otherwise might be skipped, like, ‘Research Progress’ or ‘Data Security’.\n\nUpload IRB (make sure to click ‘upload’ after choosing the IRB file), then check ‘accept terms and conditions’ at the bottom\nSummarize updates in ‘Research Progress’. A new summary must be written, with a description of whether and how the data were used, and reference the dataset(s) by name.\nAdd Presentations and Publications. Publication info can be filled in automatically by pasting its PubMed ID. The PMID can be found in the first few lines from opening the paper from search\nUpdate Collaborators in ‘Data Security’. Changes can be made in ‘IT Director’ and ‘Signing Officer’.\nDownload copy of renewal application. Click ‘Review Complete Application’ in ‘Review Applications’."
  },
  {
    "objectID": "post/2020-11-06-calculating-h2-across-gtex-brain-samples/index.html",
    "href": "post/2020-11-06-calculating-h2-across-gtex-brain-samples/index.html",
    "title": "Calculating H2 across GTEX Brain Samples",
    "section": "",
    "text": "After receiving a .RAR file with reaction rate data for 13 brain tissue samples, we wanted to calculate h2 and see if any of the thousands of reactions were heritable.\nThese are the packages necessary to run this analysis, along with the path to the list of 13 brain tissue data sets.\nWe then created a pipeline to coalesce all 13 files into one and create the long format of that data. The pipeline is as follows:\n## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i = sheet, :\n## Coercing boolean to numeric in GJ3243 / R3243C192\nThis pipeline simply reads each file in the list of 13 and then creates a column to say which tissue that sample came from. Finally it creates the long format using pivot_longer()\nNext - I created an intermediary data frame x that has an additional column for individual_id. I then grouped by individual_id and reaction_id and summarized.\nThe groupby() command will only include unique reaction_id/individual_id pairs so that any pairs that were common across tissues will be parsed into one and given the average value.\nFinally, we created the wide formate of the data that can be read by gcta for h2 calculation.\nAbove, we added an additional column for Family_ID that is the same as Individual_ID. We also filtered at this step. This is important since normally we would have filtered during the pipeline step. We filtered for reaction ids whose average across the column was <0.9 and >0.1 so that those reactions are variable across the population.\nWe filtered here as to not introduce NAs into the wide formate of teh data since gcta could not appropriately handle NAs.\nWe can check that there are no NAs using the function below\nsummary(apply(widedata[,-c(1,2)],2,mean))\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##  0.1001  0.2662  0.4924  0.5048  0.7666  0.8994\nGCTA can then calculate h2 for all 2430 phenotypes. \nSadly the histograms for both h2 and pvalues do not look significant."
  },
  {
    "objectID": "post/2020-07-30-how-to-configure-custom-ssh-connection/index.html",
    "href": "post/2020-07-30-how-to-configure-custom-ssh-connection/index.html",
    "title": "How to Configure Custom SSH Connection",
    "section": "",
    "text": "This wiki will show you how to set up SSH keys and configure custom connection options for accessing uchicago tarbell. After successful configuration, you can login your tarbell much more simply by running the following command: ssh tarbell\n\nCreate the RSA key pair:\n\n\n\nOpen terminal\nssh-keygen -t rsa\n\nPress enter when you are prompted to Enter a file in which to sae the key\nType a passport when you are prompted to Enter passphrase (empty for no passphrase):\nType the same passport when you are prompted to Enter same passphrase again:\n\n\nAppend your public key to the server:\n\n\nAppend your public key to the server using SSH (replace jiamaoz@tarbell.cri.uchicago.edu with your own one):\n\ncat ~/.ssh/id_rsa.pub | ssh jiamaoz@tarbell.cri.uchicago.edu \"mkdir -p ~/.ssh && cat >>  ~/.ssh/authorized_keys\"\n\n\n\nCreate and configure the SSH config file:\n\n\ntouch ~/.ssh/config\nchmod 600 ~/.ssh/config\nemacs ~/.ssh/config\nEnter the following into config file:  Host tarbell     HostName tarbell.cri.uchicago.edu     User jiamaoz \n\n\nLogin your uchicago tarbell:\n\nssh tarbell\n\n\nReference: - https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys–2 - https://www.digitalocean.com/community/tutorials/how-to-configure-custom-connection-options-for-your-ssh-client"
  },
  {
    "objectID": "post/2021-06-16-creating-a-new-post/index.html",
    "href": "post/2021-06-16-creating-a-new-post/index.html",
    "title": "Creating a new post",
    "section": "",
    "text": "Lab notes in Rmarkdown\nTo publish an analysis note in the notebook, you need to have blogdown and hugo installed on your computer. - install.packages('blogdown') - blogdown::install_hugo() - git clone this repository (for example git clone https://github.com/hakyimlab/web-lab-notes.git) - Go to the folder where you cloned the repo and open RStudio by double clicking web-lab-notes.Rproj (your Rproj name may be different depending on which repo you cloned) - Start a new analysis by adding a New Post from the addin option at the top of the source panel (this creates a folder in the contents/post/ folder with and index.md and subfolders with figures)\n\n\nChoose the title, author, tags\nChoose the md format unless you will be be running R commands in the post in which case select R Markdown (.Rmd) format option\nSave the md or Rmd (for the Rmd this will trigger the rendering of the html). The reason we use *.md format instead of the *.Rmd format is the the md format can be edited directly from github and will be rendered automatically. Rmd needs to be rendered by Rstudio.\ngit add, commit and push\ncheck your note has been added in https://lab-notes.hakyimlab.org/\n\n\n\nPublishing in the internal notebook\n\nIf we don’t want to make the post publicly available, we should use the http://internal-notes.hakyimlab.org with the repo at https://github.com/hakyimlab/web-internal-notes\n\n\n\nMoving posts between different websites\n\nTo move posts between websites (lab-notes.hakyimlab.org, internal-notes.hakyimlab.org, predictdb.org, etc) just move the specific folder under contents/post. For example, move the folder with all its contents in ~/Github/web-internal-notes/content/post/2020-10-29-first-note-hki/ to ~/Github/web-lab-notes/content/post/2020-10-29-first-note-hki/ or vice versa. When in doubt, publish first on the internal repo.\n\n\n\nLarge data should be posted in Box\nWhen running analysis, data should be placed in Box not under the githup repo. Add block as shown in the next block, which will automatically create a folder with the same name in the relevant Box data folder.\n\n\nAdd the following to every new post in Rmd\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\n##PRE=\"/Users/margaretperry/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data \"\n##PRE=\"/Users/temi/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data\"\n## COPY THE DATE AND SLUG fields FROM THE HEADER\nSLUG=\"correlation-between-ptrs-and-rat-height-bmi\" ## TODO copy the slug from the header\nbDATE='2022-07-07' ## TODO copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n\n## move data to DATA\n#tempodata=(\"~/Downloads/tempo/gwas_catalog_v1.0.2-associations_e105_r2022-04-07.tsv\")\n#system(glue::glue(\"cp {tempodata} {DATA}/\"))\nsystem(glue(\"open {DATA}\")) ## this will open the folder \nNetlify is hosting the content here"
  },
  {
    "objectID": "post/2020-07-30-molecular-data-available-in-gdc/index.html",
    "href": "post/2020-07-30-molecular-data-available-in-gdc/index.html",
    "title": "Molecular data available in GDC",
    "section": "",
    "text": "<div class=\"container\">\n<br>\n<br>\n    <h1><center><b>Genomic Data Commons (GDC)</b></center></h1>\n    <p><a href=\"https://gdc.cancer.gov\" target=\"_blank\"> The NCI's Genomic Data Commons (GDC) </a> provides the cancer research community with a unified data repository that enables data sharing across cancer genomic studies in support of precision medicine. </p>\n   <p>The GDC supports several cancer genome programs at the NCI Center for Cancer Genomics (CCG), including The Cancer Genome Atlas (TCGA) and Therapeutically Applicable Research to Generate Effective Treatments (TARGET). </p>\n    <p><a href=\"https://gdc.cancer.gov/support/gdc-workshops\" target=\"_blank\"> Navigating the GDC - A Case Study </a> is a workshop which helps users to learn GDC tools and data types available to support cancer genomic study. </p>\n</div>\n\n\n\n\nMolecular data available in GDC.  \n\n\n\n\n\n\n\n\n\n\n\nCancer Types\n\n\nPrimary Site\n\n\nProgram\n\n\n# Cases\n\n\nGenotyping Array\n\n\nWXS\n\n\nRNA-Seq\n\n\nmiRNA-Seq\n\n\nMethylation array\n\n\nProtein expression array\n\n\nGene expression array\n\n\nmiRNA expression array\n\n\nTotal RNA-Seq\n\n\nBisulfite-Seq\n\n\n\n\n\n\nAcute Myeloid Leukemia[AML]\n\n\nBlood\n\n\nTARGET\n\n\n923\n\n\n0\n\n\n19\n\n\n179\n\n\n265\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nAcute Myeloid Leukemia[LAML]\n\n\nBone Marrow\n\n\nTCGA\n\n\n200\n\n\n143\n\n\n149\n\n\n151\n\n\n103\n\n\n140\n\n\n0\n\n\n140\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nAdrenocortical Carcinoma[ACC]\n\n\nAdrenal Gland\n\n\nTCGA\n\n\n80\n\n\n92\n\n\n92\n\n\n79\n\n\n80\n\n\n80\n\n\n46\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nBladder Urothelial Carcinoma[BLCA]\n\n\nBladder\n\n\nTCGA\n\n\n412\n\n\n412\n\n\n412\n\n\n408\n\n\n409\n\n\n417\n\n\n344\n\n\n0\n\n\n0\n\n\n4\n\n\n6\n\n\n\n\nBrain Lower Grade Glioma[LGG]\n\n\nBrain\n\n\nTCGA\n\n\n516\n\n\n514\n\n\n515\n\n\n512\n\n\n512\n\n\n516\n\n\n430\n\n\n27\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nBreast Invasive Carcinoma[BRCA]\n\n\nBreast\n\n\nTCGA\n\n\n1097\n\n\n1096\n\n\n1050\n\n\n1092\n\n\n1079\n\n\n1095\n\n\n885\n\n\n527\n\n\n0\n\n\n11\n\n\n5\n\n\n\n\nCervical Squamous Cell Carcinoma and endocervical adenocarcinoma[CESC]\n\n\nCervix\n\n\nTCGA\n\n\n307\n\n\n302\n\n\n305\n\n\n304\n\n\n307\n\n\n307\n\n\n173\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nCholangiocarcinoma[CHOL]\n\n\nBile Duct\n\n\nTCGA\n\n\n36\n\n\n36\n\n\n51\n\n\n36\n\n\n36\n\n\n36\n\n\n30\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nClear Cell Sarcoma of the Kidney[CCSK]\n\n\nKidney\n\n\nTARGET\n\n\n13\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nColon Adenocarcinoma[COAD]\n\n\nColorectal\n\n\nTCGA\n\n\n461\n\n\n458\n\n\n443\n\n\n456\n\n\n444\n\n\n459\n\n\n362\n\n\n162\n\n\n0\n\n\n13\n\n\n2\n\n\n\n\nEsophageal Carcinoma[ESCA]\n\n\nEsophagus\n\n\nTCGA\n\n\n185\n\n\n185\n\n\n184\n\n\n164\n\n\n184\n\n\n185\n\n\n126\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nGlioblastoma Multiforme[GBM]\n\n\nBrain\n\n\nTCGA\n\n\n528\n\n\n593\n\n\n401\n\n\n166\n\n\n5\n\n\n599\n\n\n238\n\n\n587\n\n\n576\n\n\n0\n\n\n6\n\n\n\n\nHead and Neck Squamous Cell Carcinoma[HNSC]\n\n\nHead and Neck\n\n\nTCGA\n\n\n528\n\n\n521\n\n\n527\n\n\n501\n\n\n524\n\n\n528\n\n\n357\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nHigh-Risk Wilms Tumor[WT]\n\n\nKidney\n\n\nTARGET\n\n\n663\n\n\n0\n\n\n41\n\n\n122\n\n\n127\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nKidney Chromophobe[KICH]\n\n\nKidney\n\n\nTCGA\n\n\n66\n\n\n66\n\n\n66\n\n\n66\n\n\n66\n\n\n66\n\n\n63\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nKidney Renal Clear Cell Carcinoma[KIRC]\n\n\nKidney\n\n\nTCGA\n\n\n536\n\n\n532\n\n\n345\n\n\n530\n\n\n516\n\n\n533\n\n\n476\n\n\n72\n\n\n0\n\n\n4\n\n\n0\n\n\n\n\nKidney Renal Papillary Cell Carcinoma[KIRP]\n\n\nKidney\n\n\nTCGA\n\n\n291\n\n\n290\n\n\n290\n\n\n289\n\n\n291\n\n\n291\n\n\n215\n\n\n16\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nLiver Hepatocellular Carcinoma[LIHC]\n\n\nLiver\n\n\nTCGA\n\n\n377\n\n\n376\n\n\n376\n\n\n371\n\n\n373\n\n\n377\n\n\n184\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nLung Adenocarcinoma[LUAD]\n\n\nLung\n\n\nTCGA\n\n\n521\n\n\n518\n\n\n582\n\n\n515\n\n\n513\n\n\n579\n\n\n365\n\n\n32\n\n\n0\n\n\n12\n\n\n5\n\n\n\n\nLung Squamous Cell Carcinoma[LUSC]\n\n\nLung\n\n\nTCGA\n\n\n504\n\n\n504\n\n\n502\n\n\n501\n\n\n478\n\n\n503\n\n\n328\n\n\n154\n\n\n0\n\n\n0\n\n\n4\n\n\n\n\nLymphoid Neoplasm Diffuse Large B-cell Lymphoma[DLBC]\n\n\nLymph Nodes\n\n\nTCGA\n\n\n48\n\n\n48\n\n\n48\n\n\n48\n\n\n47\n\n\n48\n\n\n33\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nMesothelioma[MESO]\n\n\nPleura\n\n\nTCGA\n\n\n87\n\n\n87\n\n\n83\n\n\n86\n\n\n87\n\n\n87\n\n\n63\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nNeuroblastoma[NBL]\n\n\nNervous System\n\n\nTARGET\n\n\n1120\n\n\n0\n\n\n221\n\n\n151\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nOsteosarcoma[OS]\n\n\nBone\n\n\nTARGET\n\n\n384\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nOvarian Serous Cystadenocarcinoma[OV]\n\n\nOvary\n\n\nTCGA\n\n\n586\n\n\n573\n\n\n460\n\n\n376\n\n\n489\n\n\n602\n\n\n426\n\n\n582\n\n\n578\n\n\n0\n\n\n0\n\n\n\n\nPancreatic Adenocarcinoma[PAAD]\n\n\nPancreas\n\n\nTCGA\n\n\n185\n\n\n185\n\n\n185\n\n\n177\n\n\n178\n\n\n184\n\n\n123\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nPheochromocytoma and Paraganglioma[PCPG]\n\n\nAdrenal Gland\n\n\nTCGA\n\n\n179\n\n\n179\n\n\n179\n\n\n179\n\n\n179\n\n\n179\n\n\n80\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nProstate Adenocarcinoma[PRAD]\n\n\nProstate\n\n\nTCGA\n\n\n498\n\n\n498\n\n\n498\n\n\n496\n\n\n494\n\n\n498\n\n\n352\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n\n\nRectum Adenocarcinoma[READ]\n\n\nColorectal\n\n\nTCGA\n\n\n171\n\n\n166\n\n\n168\n\n\n167\n\n\n161\n\n\n165\n\n\n131\n\n\n69\n\n\n0\n\n\n0\n\n\n2\n\n\n\n\nRhabdoid Tumor[RT]\n\n\nKidney\n\n\nTARGET\n\n\n75\n\n\n0\n\n\n0\n\n\n5\n\n\n44\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n72\n\n\n\n\nSarcoma[SARC]\n\n\nSoft Tissue\n\n\nTCGA\n\n\n261\n\n\n261\n\n\n255\n\n\n259\n\n\n259\n\n\n261\n\n\n223\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nSkin Cutaneous Melanoma[SKCM]\n\n\nSkin\n\n\nTCGA\n\n\n470\n\n\n470\n\n\n470\n\n\n468\n\n\n448\n\n\n470\n\n\n353\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nStomach Adenocarcinoma[STAD]\n\n\nStomach\n\n\nTCGA\n\n\n443\n\n\n443\n\n\n443\n\n\n380\n\n\n436\n\n\n478\n\n\n392\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n\n\nTesticular Germ Cell Tumors[TGCT]\n\n\nTestis\n\n\nTCGA\n\n\n150\n\n\n134\n\n\n150\n\n\n150\n\n\n50\n\n\n140\n\n\n118\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nThymoma[THYM]\n\n\nThymus\n\n\nTCGA\n\n\n127\n\n\n124\n\n\n123\n\n\n119\n\n\n124\n\n\n124\n\n\n90\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nThyroid Carcinoma[THCA]\n\n\nThyroid\n\n\nTCGA\n\n\n507\n\n\n505\n\n\n502\n\n\n502\n\n\n506\n\n\n507\n\n\n372\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nUterine Carcinosarcoma[UCS]\n\n\nUterus\n\n\nTCGA\n\n\n54\n\n\n57\n\n\n57\n\n\n56\n\n\n57\n\n\n57\n\n\n47\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nUterine Corpus Endometrial Carcinoma[UCEC]\n\n\nUterus\n\n\nTCGA\n\n\n548\n\n\n547\n\n\n553\n\n\n555\n\n\n550\n\n\n559\n\n\n440\n\n\n69\n\n\n0\n\n\n4\n\n\n5\n\n\n\n\nUveal Melanoma[UVM]\n\n\nEye\n\n\nTCGA\n\n\n80\n\n\n80\n\n\n80\n\n\n80\n\n\n80\n\n\n80\n\n\n12\n\n\n0\n\n\n0\n\n\n0\n\n\n0"
  },
  {
    "objectID": "post/2001-07-30-how-to-download-the-data-from-the-uk-biobank/index.html",
    "href": "post/2001-07-30-how-to-download-the-data-from-the-uk-biobank/index.html",
    "title": "How to Download the Data from the UK Biobank",
    "section": "",
    "text": "Application 19526 is the main application from which imlab downloads data. Other ID’s correspond to specific collaborations with other investigators at Uchicago and Argonne.\nWe have requested data in 8 baskets, with overlapping fields. The composition of the baskets are somewhat haphazard, they were put together as need arose. Basket 7947 was the first one with large number of fields, but incomplete. MRI and IDP’s were requested in two baskets (), not completely overlapping.\nDetails of applications and baskets and field list are here\n##Requesting a Data Refresh\nThe button to refresh is not very easy to find, so here are the instructions from the UK Biobank Accessing Data Guide:\n\nIn order to gain access to updated data for fields in a previous data basket a researcher can request a “refresh” of that basket through AMS. A refresh of a dataset is a new extraction of the fields in the basket, and will include any additional data added to Showcase when it was updated. It will also remove the data for participants who have withdrawn since the basket was last released.\n\n\nIn order to request a refresh of a basket, a researcher will need to login to the Access Management System (AMS), navigate to their project (click Projects then View/Update), then click on the Data tab, and then on the “Go to Showcase to refresh or download data” button which will lead to the Downloads page. Next click ‘Application’ (at the top of the page) and then select the basket to be refreshed."
  },
  {
    "objectID": "post/2001-07-30-how-to-download-the-data-from-the-uk-biobank/index.html#requesting-new-data",
    "href": "post/2001-07-30-how-to-download-the-data-from-the-uk-biobank/index.html#requesting-new-data",
    "title": "How to Download the Data from the UK Biobank",
    "section": "Requesting New Data",
    "text": "Requesting New Data\nTo request additional data fields, login to the AMS, navigate to the project, then click on the Requests tab. Check “Do you wish to request additional data fields?”. Then select fields from Quick Start tab, or Browse and Search buttons.\n##Standard data (phenotype data)\n\nDownloading standard data file (ukb7948.enc) directly from  the UK Biobank website  using the MD5 Checksum and password, which are included in the email. The email should also contain the key (k1952r7948.key) file, which will be used to decrypted the downloaded phenotypic data.\nDownloading utilities (ukbmd5, ukbunpack, ukbconv and encoding.ukb.txt) directly from  the UK Biobank website . These data and utility files (from this step and last step) should be saved in the same file directory.\nVerifying the integrity of ukb7948.enc. The displayed MD5 Checksum of the dataset should be the same as the MD5 Checksum supplied via email. You may need to run chmod +x ukbmd5 beforehand, and repeat for the other utility files. This step created a decrypted file (ukb7948.enc.ukb)\n\n./ukbmd5 ukb7948.enc \n\nDecrypting ukb7948.enc\n\n./ukbunpack ukb7948.enc k1952r7948.key.txt\n\nConverting the decrypted data (ukb7948.enc_ukb) into various useful formats.\n\n./ukbconv ukb7948.enc_ukb bulk \n./ukbconv ukb7948.enc_ukb docs \n./ukbconv ukb7948.enc_ukb r \n./ukbconv ukb7948.enc_ukb csv \ncsv format: simple comma-separated-variable output, all fields double-quoted. r format: produces a tab deliminated file and an R script for labeling and putting levels on the variables. docs format: rather than output the data itself, this option generates an html file describing the data, listing the names and types of each field. You can access html file  here .  bulk format: a list of IDs for use with the ukbfetch utility (we will not use this file, instead we will download genotype data using gfech utility).\n##Complex data (genetic data)\n\nCreating the authentication file as .ukbkey The authentication file should be named “.ukbkey” and contain two lines of text, the first containing the Application ID (1952) and the second the truncated password (the first 24 characters). The authentication file should be stored in the home directory of your rcc account.\n\n1952\na1b2c3d4a1b2c3d4a1b2c3d4\ncp .ukbkey ~/ \n\nDownloading utilities (gfetch, genotype_map.csv and gconv) directly from  the UK Biobank website . These files should be saved in the same file directory.\nDownloading genetic data. Datatype is one of “cal” (calls), “imp” (imputed data), “con” (confidences) or “int” (intensities). It is possible to run more than one gfetch download in parallel, however I strongly recommend that you just download only one dataset at a time to avoid overload on either the server side or your limited rcc cds provision. Please beware that imputed chromosome X, Y, mitochondria datasets are currently not available. If a download is interrupted, then please delete the incomplete file and restart.\n\nseq 1 22 | parallel -j1 ./gfetch cal {}\n./gfetch cal X \n./gfetch cal Y\n./gfetch cal MT \n\nseq 1 22 | parallel -j1 ./gfetch imp {}\n\nseq 1 22 | parallel -j1 ./gfetch con {}\n./gfetch con X \n./gfetch con Y\n./gfetch con MT \n\nseq 1 22 | parallel -j1 ./gfetch int {}\n./gfetch int X \n./gfetch int Y\n./gfetch int MT\n\nA single sample map (impv1.sample) for the imputed data also was downloaded\n\n    ./gfetch imp 1 -m\n##Complex data (episode-level hospital data)\nThis data is held within multiple standard relational databases.  1. General structure of the hospital data The HES data has been divided into 5 tables. The master table is hesin, which connects to 4 subsidiary tables (hesin_diag10, hesin_diag9, hesin_oper, hesin_birth) via the record_id key field. 2. Record data can be manipulated and extracted directly using SQL via  the UK Biobank website .  More detailed information please see  How to access episode-level hospital data  \n##Data\n\nData is in bionimbus. See instructions on how to access here\n\naws s3 ls --endpoint-url https://bionimbus-objstore.opensciencedatacloud.org \\\n --recursive s3://uk-biobank.hakyimlab.org/data/phenotype\n\nNow also in CRI /gpfs/data/ukb-share/genotypes and /gpfs/data/im-lab/nas40t2/Data/UKB\n\n##References\n\n Essential information \n Accessing your data \n Using UK Biobank Data \n How to access bulk data \n How to access episode-level hospital data \n Hospital Episode Statistics data in Showcase \n How to access genetic data? \n A document outlining the methods used to create the imputed dataset"
  },
  {
    "objectID": "post/2020-07-30-how-to-use-rcc-cluster/index.html",
    "href": "post/2020-07-30-how-to-use-rcc-cluster/index.html",
    "title": "How to Use RCC Cluster",
    "section": "",
    "text": "https://rcc-uchicago.github.io/user-guide/"
  },
  {
    "objectID": "post/2020-07-30-how-to-use-rcc-cluster/index.html#rcc-account",
    "href": "post/2020-07-30-how-to-use-rcc-cluster/index.html#rcc-account",
    "title": "How to Use RCC Cluster",
    "section": "RCC account",
    "text": "RCC account\nPlease follow the instruction to request your RCC account:  General RCC User Account Request \nPI Account Name: pi-haky\nRcc Software: R, RStudio, Python, Python\nRcc Research: My main research goal is to perform biomedical big data analysis and \ndevelop computation tools to address a wide range of important biomedical research questions."
  },
  {
    "objectID": "post/2020-07-30-how-to-use-rcc-cluster/index.html#login",
    "href": "post/2020-07-30-how-to-use-rcc-cluster/index.html#login",
    "title": "How to Use RCC Cluster",
    "section": "Login",
    "text": "Login\n\nType ssh YOUR_CNetID@midway.rcc.uchicago.edu\nType in your password when prompted\nType yes if you are prompted to accept a key\n\nNote: A shortcut to login your RCC account, please follow the instruction:  How-to-set-up-SSH-keys-and-configure-custom-connection-options-for-your-SSH-Client"
  },
  {
    "objectID": "post/2020-07-30-how-to-use-rcc-cluster/index.html#accessing-your-rcc-account-from-a-mac",
    "href": "post/2020-07-30-how-to-use-rcc-cluster/index.html#accessing-your-rcc-account-from-a-mac",
    "title": "How to Use RCC Cluster",
    "section": "Accessing your RCC account From a MAC",
    "text": "Accessing your RCC account From a MAC\nPlease follow the instructions to access /project/haky/ from the University of Chicago’s Network: - Open Finder - Go to menu option Go -> Connect to Server - Type smb://midwaysmb.rcc.uchicago.edu/project/haky - Make sure to prefix your username with ADLOCAL if from off-campus"
  },
  {
    "objectID": "post/2020-07-30-how-to-use-rcc-cluster/index.html#storing-your-data",
    "href": "post/2020-07-30-how-to-use-rcc-cluster/index.html#storing-your-data",
    "title": "How to Use RCC Cluster",
    "section": "Storing your data",
    "text": "Storing your data\nStorage Space: /cds/haky directory is used for long term data storage at a cheaper price. This storage space is not optimized for computation.\nTarbell Lab Share-like Space: /project/haky directory is used to store lab data or persistent job outputs. Please create your own folder under the directory /project/haky/im-lab/nas40t2. Please perform regular maintenance by removing unneeded data.\n\nFor more detail, please read  RCC User Guide"
  },
  {
    "objectID": "post/2020-07-30-how-to-use-rcc-cluster/index.html#thinlinc-and-rstudio-access",
    "href": "post/2020-07-30-how-to-use-rcc-cluster/index.html#thinlinc-and-rstudio-access",
    "title": "How to Use RCC Cluster",
    "section": "ThinLinc and RStudio access",
    "text": "ThinLinc and RStudio access\n\nhttps://midway-login1.rcc.uchicago.edu/main/\nhttps://rstudio.rcc.uchicago.edu/auth-sign-in"
  },
  {
    "objectID": "post/2020-07-30-how-to-use-rcc-cluster/index.html#rcc-contact",
    "href": "post/2020-07-30-how-to-use-rcc-cluster/index.html#rcc-contact",
    "title": "How to Use RCC Cluster",
    "section": "RCC contact",
    "text": "RCC contact\nQuestions and issues should be sent to help@rcc.uchicago.edu or 773-795-2667."
  },
  {
    "objectID": "post/2020-06-22-fusion-to-predictdb-format/index.html",
    "href": "post/2020-06-22-fusion-to-predictdb-format/index.html",
    "title": "Converting Fusion weight to PredictDB format",
    "section": "",
    "text": "If you want to use the MetaXcan suite of tools, you will need to format your prediction weights in sqlite format."
  },
  {
    "objectID": "post/2020-06-22-fusion-to-predictdb-format/index.html#how-to-convert-fusion-weights-to-predictdb-format-sqlite",
    "href": "post/2020-06-22-fusion-to-predictdb-format/index.html#how-to-convert-fusion-weights-to-predictdb-format-sqlite",
    "title": "Converting Fusion weight to PredictDB format",
    "section": "how to convert fusion weights to predictdb format (sqlite)",
    "text": "how to convert fusion weights to predictdb format (sqlite)\nFind a sample script that formats FUSION weights to predictdb format here"
  },
  {
    "objectID": "post/2020-06-22-fusion-to-predictdb-format/index.html#how-to-compute-covariances-between-snps-to-use-summary-predixcan",
    "href": "post/2020-06-22-fusion-to-predictdb-format/index.html#how-to-compute-covariances-between-snps-to-use-summary-predixcan",
    "title": "Converting Fusion weight to PredictDB format",
    "section": "how to compute covariances between SNPs to use summary PrediXcan",
    "text": "how to compute covariances between SNPs to use summary PrediXcan\nIf you want to use the summary version of PrediXcan, you will also need the covariances between SNPs Fine a sample script here"
  },
  {
    "objectID": "post/2023-02-28-predictdb-weights-distribution/index.html",
    "href": "post/2023-02-28-predictdb-weights-distribution/index.html",
    "title": "PredictDB weight distribution",
    "section": "",
    "text": "Goal: get effect size distribution of omic traits"
  },
  {
    "objectID": "post/2023-02-28-predictdb-weights-distribution/index.html#get-protein-predictors-with-fdr-0.05",
    "href": "post/2023-02-28-predictdb-weights-distribution/index.html#get-protein-predictors-with-fdr-0.05",
    "title": "PredictDB weight distribution",
    "section": "get protein predictors with FDR < 0.05",
    "text": "get protein predictors with FDR < 0.05\njust need to count the number of prediction models\n\n\nShow the code\nprint(\"Protein ARIC models AA\")\n\n\n[1] \"Protein ARIC models AA\"\n\n\nShow the code\ndbname <- glue::glue(\"/Users/{USERNAME}/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/Within-Lab-Sharing/Sabrina-Data/ARIC/ARIC_AA_hg38.db\")\nprint(dbname)\n\n\n/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/Within-Lab-Sharing/Sabrina-Data/ARIC/ARIC_AA_hg38.db\n\n\nShow the code\ndb = dbConnect(sqlite,dbname)\n##weights = dbGetQuery(db, \"select * from weights\")\nextra = dbGetQuery(db,\"select * from extra\")\nnrow(extra)\n\n\n[1] 1368\n\n\nShow the code\ndbDisconnect(db)\n\nprint(\"Protein ARIC models EUR\")\n\n\n[1] \"Protein ARIC models EUR\"\n\n\nShow the code\ndbname <- glue::glue(\"/Users/{USERNAME}/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/Within-Lab-Sharing/Sabrina-Data/ARIC/ARIC_EA_hg38.db\")\nprint(dbname)\n\n\n/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/Within-Lab-Sharing/Sabrina-Data/ARIC/ARIC_EA_hg38.db\n\n\nShow the code\ndb = dbConnect(sqlite,dbname)\n##weights = dbGetQuery(db, \"select * from weights\")\nextra = dbGetQuery(db,\"select * from extra\")\nnrow(extra)\n\n\n[1] 1318\n\n\nShow the code\ndbDisconnect(db)\n\n\n\n\nShow the code\ndbname <- glue(\"{DATA}/en_Adipose_Subcutaneous.db\") ## add full path if db file not in current directory\nprint(\"Adipose models\")\n\n\n[1] \"Adipose models\"\n\n\nShow the code\nprint(dbname)\n\n\n/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/2023-02-28-predictdb-weight-distribution/en_Adipose_Subcutaneous.db\n\n\nShow the code\ndb = dbConnect(sqlite,dbname)\n##weights = dbGetQuery(db, \"select * from weights\")\nextra = dbGetQuery(db,\"select * from extra\")\nnrow(extra)\n\n\n[1] 8650\n\n\nShow the code\ndbDisconnect(db)\n\n\ndbname <- glue(\"{DATA}/en_Brain_Substantia_nigra.db\") ## add full path if db file not in current directory\nprint(\"Brain Substantia nigra models\")\n\n\n[1] \"Brain Substantia nigra models\"\n\n\nShow the code\nprint(dbname)\n\n\n/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/2023-02-28-predictdb-weight-distribution/en_Brain_Substantia_nigra.db\n\n\nShow the code\ndb = dbConnect(sqlite,dbname)\n##weights = dbGetQuery(db, \"select * from weights\")\nextra = dbGetQuery(db,\"select * from extra\")\nnrow(extra)\n\n\n[1] 2559\n\n\nShow the code\ndbDisconnect(db)"
  },
  {
    "objectID": "post/2020-11-02-gtex-reaction-rates/index.html",
    "href": "post/2020-11-02-gtex-reaction-rates/index.html",
    "title": "GTEx reaction rates h2 similar to permuted h2",
    "section": "",
    "text": "We want to investigate whether reaction rates estimated with imat(?) are heritable. Reaction rates (binary variables) were estimated by D using GTEx gene expression data.\nInitially, D generated reaction rates with brain cortex expression data from ~250 individuals.\n\n\n[1] \"\"\n\n\nHeritability of reaction rates was calculated with GCTA. Actual observed h2 and two permuted h2 values are compared next. Spoiler alert: no difference between actual and permuted values. :(\n\n\nShow the code\ntempo = read_tsv(glue::glue(\"{DATA}/reaction-rates/df.GTEX.txt\"))\n\n\nRows: 256 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (7): Phenotype, H2, Perm1, Perm2, H2 constrained, Perm constrained 1, Pe...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\ntempo_constrained = tempo %>% select(pheno=Phenotype,observed=`H2 constrained`, perm1=`Perm constrained 1`, perm2=`Perm Constrained 2`) \ntempo_constrained %>% pivot_longer(-pheno, names_to = \"type\", values_to=\"h2\") %>% ggplot(aes(h2)) + geom_histogram() + facet_wrap(~type)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nqqplots comparing actual vs permuted h2\n\n\nShow the code\nqqplot(tempo_constrained$observed, tempo_constrained$perm1); abline(0,1)\n\n\n\n\n\nShow the code\nqqplot(tempo_constrained$observed, tempo_constrained$perm2); abline(0,1)"
  },
  {
    "objectID": "post/2021-03-31-bionimbus-pdc/index.html",
    "href": "post/2021-03-31-bionimbus-pdc/index.html",
    "title": "Bionimbus PDC",
    "section": "",
    "text": "Apply for a Bionimbus Account\nSetup and Logging In\nCreating a Virtual Machine\nUsing S3 Storage"
  },
  {
    "objectID": "post/2021-03-31-bionimbus-pdc/index.html#bionimbus-website",
    "href": "post/2021-03-31-bionimbus-pdc/index.html#bionimbus-website",
    "title": "Bionimbus PDC",
    "section": "Bionimbus Website",
    "text": "Bionimbus Website\nThe Bionimbus PDC console link is a nice, user-friendly way to set up VMs. The interface makes it pretty much self-explanatory. Just go to the “Instances” tab, and click the “Launch Instance” button in the top right corner. When choosing configurations, make sure to choose the correct SSH key so you have access to the machine you just created.\nWhen the instance is launched, it will show an IP address. To access the instance, you first log into the Bionimbus login node, and then SSH into that address. For instance:\nssh -A ubuntu@<ip_address>\nIf you choose an image with an Ubuntu operating system, the user assigned to you is named ubuntu."
  },
  {
    "objectID": "post/2021-03-31-bionimbus-pdc/index.html#command-line",
    "href": "post/2021-03-31-bionimbus-pdc/index.html#command-line",
    "title": "Bionimbus PDC",
    "section": "Command Line",
    "text": "Command Line\nThe Bionimbus website is always having troubles, so a more reliable option is spinning up a VM from the command line. The first step is to log into the Bionimbus login node.\n\nChoosing an image\n$ nova image-list\n/usr/lib/python2.7/dist-packages/urllib3/util/ssl_.py:97: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.\n  InsecurePlatformWarning\n+--------------------------------------+---------------------------------------------+--------+--------------------------------------+\n| ID                                   | Name                                        | Status | Server                               |\n+--------------------------------------+---------------------------------------------+--------+--------------------------------------+\n| 76154988-d1c1-4e4f-9ccc-16b6f65f89bb | GSE26494_Ubuntu_14.04_2015-05-29            | ACTIVE |                                      |\n| 48df8504-e2f0-418b-b54f-88066b858ebc | Ubuntu-14.04                                | ACTIVE | 7c337e94-abbc-43d8-a67b-5c9b36489297 |\n| 3c4c6ac3-2723-48c6-89b6-9510878b17ea | Ubuntu-16.04-20170705                       | ACTIVE | 27605fc7-aed0-4d74-8a67-de422f417967 |\n| ddd6a452-0d50-456a-bd4d-0f4fd05cbf78 | Ubuntu-16.04-20170801                       | ACTIVE | ff86aa20-6a4a-4c95-85ff-c9bcff888703 |\n| 5d80c155-16db-45da-b9b0-dddeb56b1325 | Ubuntu-16.04-20171003                       | ACTIVE | 7f33c164-7561-4676-abd7-8ed8689e86b8 |\n| aa465c1b-0d35-4655-85ec-571e6c06f0a0 | Ubuntu-16.04-raw-20170306                   | ACTIVE | c8f72c89-caa1-4954-9146-14ef0462a60d |\n| 6fb5fb36-6dc2-4b49-b772-999a2b6c9287 | Ubuntu-18.04                                | ACTIVE | 8bce91f1-ab13-40f9-a994-8cee7b164e10 |\n| ff69d7f8-4ed5-42d8-9b7a-6551498af392 | Ubuntu-18.04                                | ACTIVE | 5493affa-af61-4060-b49d-c9754d554a38 |\n| c5dd47f8-6508-4cc9-93a8-4597d4754225 | Ubuntu_18_multipurpose_node                 | ACTIVE | 90ced9c2-943b-4026-a5e0-c364a36440cc |\n| 30464444-fb4f-48cb-971b-cb79a73b8949 | centos-7-2017-07-20                         | ACTIVE |                                      |\n| cc263d56-d6ca-47fb-8f93-bbd0da576e6a | docker-20160418                             | ACTIVE |                                      |\n| 3f841efc-1964-4f0e-8a10-401f6c6ba437 | kubernetes-PoC-image-ubuntu16.04-2018-01-11 | ACTIVE | b582f560-12db-4aaa-b617-8ca1e7a5e9e9 |\n| be16c465-0e00-41a9-a15f-7a95e84dd7ca | torque-cluster-node-14.04-20161101          | ACTIVE | a9dbbfaf-53a3-4be5-8d5a-5d0e6cb169ce |\n| 73c16451-30c2-4ba3-9dce-ce93c8242c5d | torque-headnode-14.04-20161021              | ACTIVE |                                      |\n| fd670f25-c2f4-45c1-9902-cc713dd3b94f | torque-headnode-14.04-20161101              | ACTIVE | 5d93a97f-858d-4e40-a123-a73d8879f5ed |\n| fd86aa94-5363-4fe0-9282-51097f92ec7a | ukbREST_server                              | SAVING | be009f2e-a0a2-4e61-89ab-1714d04b1805 |\n+--------------------------------------+---------------------------------------------+--------+--------------------------------------+\nI haven’t yet had a use-case where I needed something more specialized than a plain Ubuntu machine, so I’ve always chosen the most recent Ubuntu image available. At the time of writing, that is Ubuntu-18.04 with image ID ff69d7f8-4ed5-42d8-9b7a-6551498af392.\n\n\nChoosing a Flavor\nNext you need to choose a VM ‘flavor’. This is akin to choosing the hardware specs for your machine. There are different configurations with different amounts of memory, disk space, and processors. The command to show all of the flavors is nova flavor-list\n$ nova flavor-list\n/usr/lib/python2.7/dist-packages/urllib3/util/ssl_.py:97: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.\n  InsecurePlatformWarning\n+--------------------------------------+------------------------------+-----------+------+-----------+------+-------+-------------+-----------+\n| ID                                   | Name                         | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |\n+--------------------------------------+------------------------------+-----------+------+-----------+------+-------+-------------+-----------+\n| 05df75bf-1f2d-4f34-988d-d57ec13cdb27 | ram16.disk100.eph0.core4     | 16384     | 100  | 0         |      | 4     | 1.0         | True      |\n| 0b86bc0b-73c3-4f05-88ce-8c957e1804e6 | ram125.disk50.eph4000.core32 | 128000    | 50   | 4000      |      | 32    | 1.0         | True      |\n| 0e0fc4ee-8d40-4fc7-9af4-91d723f85dd0 | ram32.disk100.eph0.core4     | 32768     | 100  | 0         |      | 4     | 1.0         | True      |\n| 10                                   | m1.xxlarge                   | 49152     | 10   | 0         |      | 16    | 1.0         | True      |\n| 11                                   | m1.xxxlarge                  | 98304     | 10   | 0         |      | 32    | 1.0         | True      |\n| 12                                   | hs1.medium                   | 6144      | 20   | 0         |      | 2     | 1.0         | True      |\n| 13                                   | hs1.large                    | 12288     | 20   | 0         |      | 4     | 1.0         | True      |\n| 14                                   | gm.large                     | 24576     | 30   | 0         |      | 8     | 1.0         | True      |\n| 15                                   | ram12.disk40.eph10.core2     | 12288     | 40   | 10        |      | 2     | 1.0         | True      |\n| 16                                   | hs3.xlarge                   | 24576     | 40   | 0         |      | 8     | 1.0         | True      |\n| 17                                   | gm.xlarge                    | 32768     | 40   | 0         |      | 8     | 1.0         | True      |\n| 18                                   | es1.small                    | 3072      | 10   | 1024      |      | 1     | 1.0         | True      |\n| 20d675e6-b9b4-4d90-a989-afa5d84d4141 | ram125.disk10.eph0.core32    | 128000    | 10   | 0         |      | 32    | 1.0         | True      |\n| 27                                   | es1.xxxlarge                 | 122880    | 40   | 1024      |      | 32    | 1.0         | True      |\n| 28                                   | es1.xxlarge                  | 61440     | 40   | 1024      |      | 16    | 1.0         | True      |\n| 29                                   | es1.xlarge                   | 30720     | 10   | 1024      |      | 8     | 1.0         | True      |\n| 2b6d2383-59fa-4f3b-8b6d-6a82a88ca2c3 | ram16.disk100.eph0.core2     | 16384     | 100  | 0         |      | 2     | 1.0         | True      |\n| 2b84186c-c701-484a-8898-4b872db8aef3 | ram8.disk10.eph0.core8       | 8192      | 10   | 0         |      | 8     | 1.0         | True      |\n| 2f8d695e-5de6-4cb7-8a39-b8edacacf4b9 | ram64.disk10.eph0.core16     | 65536     | 10   | 0         |      | 16    | 1.0         | True      |\n| 30                                   | es1.large                    | 15360     | 10   | 1024      |      | 4     | 1.0         | True      |\n| 31                                   | hs3.large                    | 12288     | 40   | 0         |      | 4     | 1.0         | True      |\n| 32                                   | hs3.xxlarge                  | 49152     | 40   | 0         |      | 16    | 1.0         | True      |\n| 33                                   | br1.large                    | 15360     | 1000 | 0         |      | 4     | 1.0         | True      |\n| 34                                   | es4.xxxlarge                 | 98304     | 10   | 4096      |      | 32    | 1.0         | True      |\n| 35                                   | ram2.disk10.eph0.core1       | 2048      | 10   | 0         |      | 1     | 1.0         | True      |\n| 4eecfa83-2265-426d-aee5-5863d4755d4b | ram2.disk3.eph0.core1        | 2048      | 3    | 0         |      | 1     | 1.0         | True      |\n| 57195829-973f-42d5-9eb6-64c9f68b9ad0 | ram12.disk10.eph4096.core8   | 12288     | 10   | 4096      |      | 8     | 1.0         | True      |\n| 5b04ebf5-9620-45f8-8260-7f5043b63343 | ram64.disk10.eph1024.core16  | 65536     | 10   | 1024      |      | 16    | 1.0         | True      |\n| 6                                    | m1.small                     | 3072      | 10   | 0         |      | 1     | 1.0         | True      |\n| 6194655e-7681-4a3a-8ed9-62f012000d97 | ram6.test                    | 6144      | 10   | 1024      |      | 2     | 1.0         | True      |\n| 7                                    | m1.medium                    | 6144      | 10   | 0         |      | 2     | 1.0         | True      |\n| 72b625a6-64c2-4360-ad99-957d4054f7b4 | ram8.disk10.eph0.core2       | 8192      | 10   | 0         |      | 2     | 1.0         | True      |\n| 8                                    | m1.large                     | 12288     | 10   | 0         |      | 4     | 1.0         | True      |\n| 84a51f22-88f5-418c-8a2f-2ac5062fb546 | ram6.disk10.eph1024.core2    | 6144      | 10   | 1024      |      | 2     | 1.0         | True      |\n| 877502af-9358-4332-93d0-c182e9b80bfc | ram4.disk10.eph0.core4       | 4096      | 10   | 0         |      | 4     | 1.0         | True      |\n| 8e45fe50-18b0-4f4a-b5b0-60d7fe3dd0e9 | ram125.disk10.eph4096.core32 | 128000    | 10   | 4096      |      | 32    | 1.0         | True      |\n| 9                                    | m1.xlarge                    | 24576     | 10   | 0         |      | 8     | 1.0         | True      |\n| 9ba2e74e-dc4d-4d3b-9ca3-481294d6b06b | ram32.disk10.eph1024.core16  | 32768     | 10   | 1024      |      | 16    | 1.0         | True      |\n| b41506d4-4f8f-4aae-98ed-f77f71e44f41 | ram16.disk10.eph512.core4    | 16384     | 10   | 512       |      | 4     | 1.0         | True      |\n| bf7e81f3-39ae-4b9b-ad65-e7252614fa43 | ram60.disk10.eph0.core16     | 61440     | 10   | 0         |      | 16    | 1.0         | True      |\n| c5195a4d-6d70-4e83-9bbe-04dc7d7a7f5c | ram16.disk10.eph2048.core4   | 16384     | 10   | 2048      |      | 4     | 1.0         | True      |\n| ef18330e-41d3-4cbb-9a78-67a9afca182c | ram60.disk10.eph10240.core16 | 61440     | 10   | 10240     |      | 16    | 1.0         | True      |\n+--------------------------------------+------------------------------+-----------+------+-----------+------+-------+-------------+-----------+\nA quick explanation of the columns: - Memory_MB is the amount of memory allocated to the VM - Disk is the amount of disk storage mounted on / - Ephemeral is the amount of disk storage mounted to a separate drive (usually /mnt/). This storage goes away if a snapshot is taken of the VM. - VCPUs is the number of CPUs allocated.\nUnfortunately, not all of these flavors currently work. As of May 2020, I was able to confirm that the m1.xlarge, es1.xlarge, and es1.xxlarge flavors were all working.\nFor our demo, we will use the es1.xlarge flavor, which has flavor ID 29.\n\n\nChoosing a Keypair\nYou’ll need to specify a keypair to give access to the VM. You can run the command nova keypair-list to show which keypairs are available for you to use.\nIf you need to import a keypair, there’s a command for that too. NOTE: I haven’t used this command, I am just reading the documentation here.\nFirst, scp your PUBLIC ssh key to the bionimbus login node. I’ll assume that the public key is at ~/.ssh/id_rsa.pub. Then, you can use the nova keypair-add command:\n$ nova keypair-add --pub-key ~/.ssh/id_rsa.pub KEY_NAME\nFor our demonstration, I’ll use the keypair named owen-macbook.\n\n\nStarting the VM\nSo we chose an image, a flavor, and a keypair to configure the VM. Now, we need to choose a name; let’s say test_node. Here’s the command to create the VM and start it up:\nnova boot \\\ntest_node \\\n--image ff69d7f8-4ed5-42d8-9b7a-6551498af392 \\\n--flavor 29 \\\n--key_name owen-macbook\nThe output looked like this:\n/usr/lib/python2.7/dist-packages/urllib3/util/ssl_.py:97: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.\n  InsecurePlatformWarning\n+--------------------------------------+-----------------------------------------------------+\n| Property                             | Value                                               |\n+--------------------------------------+-----------------------------------------------------+\n| OS-DCF:diskConfig                    | MANUAL                                              |\n| OS-EXT-AZ:availability_zone          |                                                     |\n| OS-EXT-STS:power_state               | 0                                                   |\n| OS-EXT-STS:task_state                | scheduling                                          |\n| OS-EXT-STS:vm_state                  | building                                            |\n| OS-SRV-USG:launched_at               | -                                                   |\n| OS-SRV-USG:terminated_at             | -                                                   |\n| accessIPv4                           |                                                     |\n| accessIPv6                           |                                                     |\n| adminPass                            | HWCbS63SiHaf                                        |\n| config_drive                         |                                                     |\n| created                              | 2020-08-25T14:51:22Z                                |\n| flavor                               | es1.xlarge (29)                                     |\n| hostId                               |                                                     |\n| id                                   | f3c8270c-f71c-4644-b708-da3c0df6ea8d                |\n| image                                | Ubuntu-18.04 (ff69d7f8-4ed5-42d8-9b7a-6551498af392) |\n| key_name                             | owen-macbook                                        |\n| metadata                             | {}                                                  |\n| name                                 | test_node                                           |\n| os-extended-volumes:volumes_attached | []                                                  |\n| progress                             | 0                                                   |\n| security_groups                      | default                                             |\n| status                               | BUILD                                               |\n| tenant_id                            | 18a7bd7295044b64a8117e0eb53e8830                    |\n| updated                              | 2020-08-25T14:51:22Z                                |\n| user_id                              | b28b67ef34e74aefae0bafe4c5bb328f                    |\n+--------------------------------------+-----------------------------------------------------+\nAnd when I ran nova list to see all of the VMs, it showed up:\n$ nova list\n/usr/lib/python2.7/dist-packages/urllib3/util/ssl_.py:97: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.\n  InsecurePlatformWarning\n+--------------------------------------+--------------------+--------+------------+-------------+-----------------------+\n| ID                                   | Name               | Status | Task State | Power State | Networks              |\n+--------------------------------------+--------------------+--------+------------+-------------+-----------------------+\n| f4107261-a2eb-4989-910c-6276e3dd6c39 | owen_macbook       | ACTIVE | -          | Running     | private=172.16.149.42 |\n| 18687472-6184-4bab-b6da-404748d45eef | sabrina_macbook    | ACTIVE | -          | Running     | private=172.16.168.45 |\n| f3c8270c-f71c-4644-b708-da3c0df6ea8d | test_node          | ACTIVE | -          | Running     | private=172.16.136.46 |\n| 9bb71982-f41b-422d-8aa2-30bf38d9d8c3 | ukbREST            | ACTIVE | -          | Running     | private=172.16.179.43 |\n| ed5c2621-1ec3-4408-813d-91932fda734c | ukbREST_1          | ACTIVE | -          | Running     | private=172.16.171.45 |\n| 0636df1d-3cb2-40e7-be8a-936d399e556f | ukbREST_query_node | ACTIVE | -          | Running     | private=172.16.169.45 |\n| d603dc2a-a899-469d-80c4-c68e0a120bb5 | ukbREST_sabrina    | ACTIVE | -          | Running     | private=172.16.135.46 |\n+--------------------------------------+--------------------+--------+------------+-------------+-----------------------+\n\n\nShutting off the VM\nTo shut off the VM, the command is nova stop <VM name>, and to delete it permanently, the command is nova delete <VM name>."
  },
  {
    "objectID": "post/2021-06-04-installing-conda-on-cri/index.html",
    "href": "post/2021-06-04-installing-conda-on-cri/index.html",
    "title": "Installing Conda on CRI",
    "section": "",
    "text": "CRI has versions of miniconda already downloaded through the module commands. However, those sometimes do not work so it is best to just install conda for yourself on CRI. The whole process is quite simple and should only take a few minutes.\nFirst download the conda installer from their website\nYou can download using\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nIn your terminal run:\nbash Miniconda3-latest-Linux-x86_64.sh\nMake sure to follow the prompts on the installer screens. At the end, it’ll ask you where to install miniconda. If you have a software directory in your folder, you can tell miniconda to install there. /gpfs/data/im-lab/nas40t2//software/miniconda\nFinally, you can test your installation by running the command conda list\nYou can run the lab version of conda by typing /gpfs/data/im-lab/nas40t2/lab_software/miniconda/bin/conda -h"
  },
  {
    "objectID": "post/2022-06-28-query-gene-annotations-from-biomart/index.html",
    "href": "post/2022-06-28-query-gene-annotations-from-biomart/index.html",
    "title": "How to annotate genes - BioMart Basics",
    "section": "",
    "text": "BioMart is a database containing Ensembl annotations of genes across many species and builds. To query data, you first pick one the databases: 1. Ensembl Genes 2. Ensembl Variation 3. Ensembl Variation 4. Vega\nWe typically uses only the Ensembl Genes database, which lists all genes for the selected species and build, along with their positions, alternate names, and other descriptions."
  },
  {
    "objectID": "post/2022-06-28-query-gene-annotations-from-biomart/index.html#querying-gene-annotations",
    "href": "post/2022-06-28-query-gene-annotations-from-biomart/index.html#querying-gene-annotations",
    "title": "How to annotate genes - BioMart Basics",
    "section": "Querying Gene Annotations",
    "text": "Querying Gene Annotations\nThe full tutorial is online. Below is a quick example from here.\nlibrary(tidyverse)\n\n## install biomaRt if not avalable\n## if (!require(\"BiocManager\", quietly = TRUE))\n##     install.packages(\"BiocManager\")\n## BiocManager::install(\"biomaRt\")\n\nlibrary(biomaRt)\n\n# connect to BioMart database, choosing gene annotations for rats\nensembl = useMart(biomart=\"ENSEMBL_MART_ENSEMBL\", dataset = \"rnorvegicus_gene_ensembl\")\n\n# returns the type of information we can query from the dataset\nlistAttributes(mart=ensembl)$name %>% unique\n\n# query all relevant data and store in a dataframe\north.rat = getBM( attributes=\n                    c(\"ensembl_gene_id\", \n                      \"hsapiens_homolog_ensembl_gene\",\n                      \"external_gene_name\"),\n                  filters = \"with_hsapiens_homolog\",\n                  values =TRUE,\n                  mart = ensembl,\n                  bmHeader=FALSE)\n\n# write to file\nwrite.table(orth.rat, file=\"ortholog_genes_rats_humans.tsv\", sep=\\t, header=TRUE, quote=FALSE)"
  },
  {
    "objectID": "post/2021-07-13-download-ld-blocks/index.html",
    "href": "post/2021-07-13-download-ld-blocks/index.html",
    "title": "Download LD blocks",
    "section": "",
    "text": "This is a short tutorial on how to download ld block data as summarized by Berisa and Pickrell. The LD data is available in hg19 genome build and for three different populations i.e. AFR, ASN and EUR.\nFirst download the data from the bit bucket repository\ngit clone https://bitbucket.org/nygcresearch/ldetect-data.git\nIf you need the LD blocks for use with data in a different genome build then you will need to do a liftover.\nTo demonstrate how to lift over the ld block co-ordinates from hg19 to hg18 we will use the EUR LD blocks\n\nIdentify the data location\n\nless ./EUR/fourier_ls-all.bed\n\nUsing a custom script do the lift over\n\npython liftover_blocks.py -input ./EUR/fourier_ls-all.bed -output hg38_fourier_ls-all.bed.gz\nCheck out for error messages in your liftover process\nThe liftover_blocks.py is a custom script that contains this info;\n#!/usr/bin/env python3\n\nimport gzip\nimport pyliftover\n\ndef _l(liftover, chr, pos):\n    _new_chromosome = \"NA\"\n    _new_position = \"NA\"\n    try:\n        pos = int(pos)\n        l_ = liftover.convert_coordinate(chr, pos)\n        if l_:\n            if len(l_) > 1:\n                print(\"Liftover with more than one candidate: %s\", t.variant_id)\n            _new_chromosome = l_[0][0]\n            _new_position = int(l_[0][1])\n    except:\n        pass\n    return _new_chromosome, _new_position\n\ndef run(args):\n    print(\"starting lifting over.\")\n    liftover = pyliftover.LiftOver('hg19', 'hg38') # change genome builds here\n    with gzip.open(args.output, \"w\") as _o:\n        line = \"{}\\n\".format(\"\\t\".join([\"chr\", \"start\", \"stop\"]))\n        _o.write(line.encode())\n        with open(args.input) as _i:\n            for i,line in enumerate(_i):\n                try:\n                    comps = line.strip().split()\n                    chrom = 'chr' + str(comps[0])\n                    chrom = str(comps[0])\n                    start = int(comps[1])\n                    end = int(comps[2])\n\n                    _chrs, _s = _l(liftover, chrom, start)\n                    _chre, _e = _l(liftover, chrom, end)\n                    if _chrs != _chre:\n                        print(\"{}:{}:{} have different target chromosomes: {}/{}\".format(chr, start, end, _chrs, _chre))\n                    line = \"{}\\n\".format(\"\\t\".join([_chrs, str(_s), str(_e)]))\n                    _o.write(line.encode())\n                except Exception as e:\n                    print(\"Error for: %s\", line)\n\n    print(\"Finished lifting over.\")\n\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser(\"Liftover ld regions file\")\n    parser.add_argument(\"-input\", help=\"region file\")\n    parser.add_argument(\"-output\", help=\"Where the output should go\")\n    args = parser.parse_args()\n\n    run(args)"
  },
  {
    "objectID": "post/2022-05-17-how-to-open-jupyter-notebook-on-cri-or-rcc/index.html",
    "href": "post/2022-05-17-how-to-open-jupyter-notebook-on-cri-or-rcc/index.html",
    "title": "How-to-open-jupyter-notebook-on-CRI-or-RCC",
    "section": "",
    "text": "This is a workflow to show how to open jupyter notebook on CRI or RCC. There are some detailed instructions here, CRI’s instructions and More general instructions"
  },
  {
    "objectID": "post/2022-05-17-how-to-open-jupyter-notebook-on-cri-or-rcc/index.html#on-rcc-follow-these-steps",
    "href": "post/2022-05-17-how-to-open-jupyter-notebook-on-cri-or-rcc/index.html#on-rcc-follow-these-steps",
    "title": "How-to-open-jupyter-notebook-on-CRI-or-RCC",
    "section": "On RCC follow these steps",
    "text": "On RCC follow these steps\nhttps://rcc.uchicago.edu/docs/software/environments/python/index.html#running-jupyter-notebooks"
  },
  {
    "objectID": "post/2022-05-17-how-to-open-jupyter-notebook-on-cri-or-rcc/index.html#on-gardner",
    "href": "post/2022-05-17-how-to-open-jupyter-notebook-on-cri-or-rcc/index.html#on-gardner",
    "title": "How-to-open-jupyter-notebook-on-CRI-or-RCC",
    "section": "On Gardner",
    "text": "On Gardner\n\nOn remote side (gardner)\nlog into gardner\n$ ssh <username>@gardner.cri.uchicago.edu\non gardner run\n$ PATH=/apps/software/gcc-6.2.0/miniconda3/4.7.10/bin:$PATH\n$ jupyter notebook --no-browser --port=XXXX\nThis version of jupyter works, others may fail.\nThe default port will be 8888, and “–no-browser” is required because if you do not specify –no-browser –ip=, the web browser will be launched on the node and the URL returned cannot be used on your local machine.\nIf you set port=8889, the result should be like the following:\nTo access the notebook, open this file in a browser: file:///home/<username>/.local/share/jupyter/runtime/nbserver-129406-open.html Or copy and paste one of these URLs: http://localhost:8889/?token=47871f1a27e41715e04362540f5730611a30b17ae2072827 or http://127.0.0.1:8889/?token=47871f1a27e41715e04362540f5730611a30b17ae2072827\n\n\nLocal machine\nOpen a new terminal and run the following command.\n$ ssh -N -f -L localhost:YYYY:localhost:XXXX <username>@gardner.cri.uchicago.edu\nSelect any port=YYYY which you haven’t used for other work.\nThen open a browser, type in localhost=<YYYY> and copy and paste the token from the last step. Finally, it should worked out."
  },
  {
    "objectID": "post/2022-05-17-how-to-open-jupyter-notebook-on-cri-or-rcc/index.html#on-theta",
    "href": "post/2022-05-17-how-to-open-jupyter-notebook-on-cri-or-rcc/index.html#on-theta",
    "title": "How-to-open-jupyter-notebook-on-CRI-or-RCC",
    "section": "On theta",
    "text": "On theta\nFind notes in notion here"
  },
  {
    "objectID": "post/2021-07-28-how-to-get-the-cytogenetic-band-of-a-gene/index.html",
    "href": "post/2021-07-28-how-to-get-the-cytogenetic-band-of-a-gene/index.html",
    "title": "How to annotate a gene with the cytogenetic band",
    "section": "",
    "text": "Adding the cytogenetic to genes is convenient because it provides a somewhat memorable names for the genomic region where the gene is located. Biomart package in bioconductor has the database and function for the annnotation.\n\n\nShow the code\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\nsuppressMessages(library(RSQLite))\n\n\nadd cytogenetic band to genes\n\n\nShow the code\n# install biomaRt if not installed \n# if (!requireNamespace(\"BiocManager\", quietly = TRUE))\n#     install.packages(\"BiocManager\")\n# BiocManager::install(\"biomaRt\")\n## YOU MAY WANT TO RESTART R/RSTUDIO AFTER INSTAALLING BIOCMANAGER\nlibrary(biomaRt)\n\n#ensembl <- useMart(biomart=\"ENSEMBL_MART_ENSEMBL\",dataset=\"hsapiens_gene_ensembl\")\n\n\nensembl <- useEnsembl(biomart = \"ensembl\", \n                   dataset = \"hsapiens_gene_ensembl\", \n                   mirror = \"useast\")\n\n\n\n\n\n\n\n\nNote\n\n\n\nmirror = “useast” was necessary to make quarto render work; interactive runs seems to be able to be redirected to the useast mirror automatically but not quarto render\n\n\n\n\nShow the code\n## get the gene annotation with cytoband  from biomart\nanno_gene <- getBM(attributes =c(\"ensembl_gene_id\",\"external_gene_name\",\"chromosome_name\",\"start_position\",\"end_position\",\"band\", \"gene_biotype\"),mart=ensembl)\n\n## define function to add cytoband to gene\naddband2gene = function(df,geneid = \"ensembl_gene_id\")\n{\n  if(!exists(\"anno_gene\"))\n  {\n    ensembl <- useMart(biomart=\"ENSEMBL_MART_ENSEMBL\",dataset=\"hsapiens_gene_ensembl\")\n    anno_gene <<- getBM(attributes = c(\"ensembl_gene_id\",\"external_gene_name\",\"chromosome_name\",\"start_position\",\"end_position\",\"band\", \"gene_biotype\"),mart=ensembl )\n  print(\"defined anno_gene\")\n  }\n  if(geneid != \"ensembl_gene_id\") names(df)[names(df)==geneid] = \"ensembl_gene_id\" ## this is an ugly workaround - need to find a way to use rename_ for this but don't know how to specify a string instead of name in rename(geneid = \"ensembl_gene_id\")\n  df = df %>% left_join(anno_gene, by=c( \"ensembl_gene_id\" ))\n  if(geneid != \"ensembl_gene_id\") names(df)[names(df)==\"ensembl_gene_id\"] = geneid\n  df %>% mutate(cytoband = paste0(chromosome_name,band))\n}\nanno_gene <- anno_gene %>% mutate(cytoband = paste0(chromosome_name,band))\n\n\nTo get start and end of the cytegenetic bands\n\n\nShow the code\n## Download cytoband table from http://genome.ucsc.edu/cgi-bin/hgTables\n## 1. Go to the UCSC Genome browser: https://genome.ucsc.edu/index.html\n## 2. Mouse over \"Tools\" and select \"Table Browser\"\n## 3. In the table browser window, set the following parameters: clade = mammal, genome = human, assembly = hg38 (or other), group = Mapping and Sequencing, track = Chromosome Band, table = cytoBand, position = chr1 (or whatever chromosome you are interested in), output format = all fields from selected table, file type returned = plain text\n## 4. Click [get output]\n## The results will show the following 5 columns: chromosome number, cytoband start position, end position, cytoband name, and staining result.\n## Haynes, Karmella. (2018). Re: How can i gen the length in mb from a cytoband?. Retrieved from: https://www.researchgate.net/post/How-can-i-gen-the-length-in-mb-from-a-cytoband/5b2147a0565fba5e2820b3de/citation/download.\n#\n\nWEBDATA=\"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\ncytoband_hg38 <- read_table(glue(\"{WEBDATA}/2021-07-28-how-to-get-the-cytogenetic-band-of-a-gene/hgTables-cytoband-positions-hg38.txt\"),comment = \"#\",guess_max = 10000)\n\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  chrom = col_character(),\n  chromStart = col_double(),\n  chromEnd = col_double(),\n  name = col_character(),\n  gieStain = col_character()\n)\n\n\nWarning: 571 parsing failures.\nrow col  expected    actual                                                                                                                                                                                  file\n455  -- 5 columns 4 columns '/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/2021-07-28-how-to-get-the-cytogenetic-band-of-a-gene/hgTables-cytoband-positions-hg38.txt'\n864  -- 5 columns 4 columns '/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/2021-07-28-how-to-get-the-cytogenetic-band-of-a-gene/hgTables-cytoband-positions-hg38.txt'\n865  -- 5 columns 4 columns '/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/2021-07-28-how-to-get-the-cytogenetic-band-of-a-gene/hgTables-cytoband-positions-hg38.txt'\n866  -- 5 columns 4 columns '/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/2021-07-28-how-to-get-the-cytogenetic-band-of-a-gene/hgTables-cytoband-positions-hg38.txt'\n867  -- 5 columns 4 columns '/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/2021-07-28-how-to-get-the-cytogenetic-band-of-a-gene/hgTables-cytoband-positions-hg38.txt'\n... ... ......... ......... .....................................................................................................................................................................................\nSee problems(...) for more details.\n\n\nShow the code\n#\ncytoband_hg37 <- read_table(glue(\"{WEBDATA}/2021-07-28-how-to-get-the-cytogenetic-band-of-a-gene/hgTables-cytoband-positions-hg37.txt\"),comment = \"#\",guess_max = 10000)\n\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  chrom = col_character(),\n  chromStart = col_double(),\n  chromEnd = col_double(),\n  name = col_character(),\n  gieStain = col_character()\n)\n\n\nShow the code\n# What's the distribution of the weights\n\ncytoband_hg38 %>% filter(chrom %in% paste0(\"chr\",c(1:22)) )%>%  mutate(bandw = chromEnd - chromStart) %>% .[[\"bandw\"]] %>% summary()/1e6\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.100   2.000   3.200   3.545   4.600  18.100 \n\n\nShow the code\n  #  Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  # 0.100   2.000   3.200   3.545   4.600  18.100 \ncytoband_hg37 %>% filter(chrom %in% paste0(\"chr\",1:2) ) %>%  mutate(bandw = chromEnd - chromStart) %>% .[[\"bandw\"]] %>% summary()/1e6\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.50    2.40    3.60    3.94    4.90   15.20 \n\n\nShow the code\n  #  Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  # 0.100   2.100   3.300   3.552   4.600  15.200 \n\ncytoband_hg37 %>% mutate(bandw = chromEnd - chromStart) %>% ggplot(aes(bandw,fill=chrom)) + geom_density(alpha=0.5)\n\n\n\n\n\nShow the code\n# How many cytogenetic bands are there in the autosomes (non sex chromosomes nor mitochondria)\n\ndim(cytoband_hg38)\n\n\n[1] 1433    5\n\n\nShow the code\ndim(cytoband_hg37)\n\n\n[1] 862   5\n\n\nShow the code\n## there are many more cytobands in hg38 because there are many patches added after the official release of hg19\n\n\n\n\nShow the code\n## number of genes per cytoband\nanno_gene %>% count(cytoband) %>% arrange(desc(n))\n\n\n                                   cytoband   n\n1                                   16p11.2 457\n2                                   14q11.2 434\n3                                   16p13.3 432\n4                                   19p13.3 389\n5                                   11p15.4 386\n6                                   17p11.2 369\n7                   CHR_HSCHR6_MHC_COX_CTG1 340\n8                                   19p13.2 326\n9                   CHR_HSCHR6_MHC_QBL_CTG1 322\n10                  CHR_HSCHR6_MHC_DBB_CTG1 316\n11                                  17q11.2 311\n12                 CHR_HSCHR6_MHC_SSTO_CTG1 303\n13                 CHR_HSCHR6_MHC_MANN_CTG1 299\n14                                 14q32.33 292\n15                  CHR_HSCHR6_MHC_MCF_CTG1 292\n16                                   8p23.1 284\n17                                  15q11.2 274\n18                                  19q13.2 271\n19                                 19q13.42 268\n20                                  17q25.3 267\n21                                   1q21.3 267\n22                                    17q12 262\n23                                  3p21.31 262\n24                                   2p11.2 260\n25                                   1q32.1 252\n26                                     Xq28 249\n27                                  11q12.1 247\n28                                 12p13.31 247\n29                                   6p22.1 247\n30                                 22q11.21 245\n31                                   7q22.1 237\n32                                 19q13.33 231\n33                                  17p13.1 228\n34                                   8q24.3 227\n35                                  16q22.1 223\n36                                  21q22.3 218\n37                                     7q34 217\n38                                 19p13.11 216\n39                                  7q11.21 213\n40                                 17q21.31 208\n41                           CHR_HG76_PATCH 207\n42                                 12q24.31 199\n43                                     3q29 198\n44                                    19p12 197\n45                                 12q13.13 196\n46                                  7q11.23 196\n47                                 19q13.43 193\n48                                  11q13.4 191\n49                                  11q13.1 190\n50                                  17q21.2 188\n51                                   5q31.3 188\n52                                     6q21 188\n53                       CHR_HSCHR14_3_CTG1 188\n54                                  11q23.3 187\n55                                   5q35.3 187\n56                                     1q44 186\n57                                   9q34.3 184\n58                                   1p31.1 183\n59                  CHR_HSCHR6_MHC_APD_CTG1 183\n60                                     2q35 181\n61                                  14q24.3 179\n62                                  17q25.1 177\n63                                   6p21.1 174\n64                                  15q26.1 172\n65                                   1p32.3 172\n66                                  6p21.33 171\n67                                 19q13.12 168\n68                                   5q31.1 168\n69                                 20q13.33 167\n70                                   9p13.3 167\n71                                   2q31.1 165\n72                                   Yp11.2 165\n73                                  11p11.2 162\n74                                    14q12 162\n75                                 19q13.41 162\n76                                  1p36.33 162\n77                                   7p11.2 162\n78                                  11p15.5 161\n79                                 12q24.33 161\n80                                    20p13 161\n81                                  1p36.11 159\n82                                 20q13.12 159\n83                                   7q36.1 156\n84                                 18p11.21 155\n85                                   2q37.3 155\n86                                 19q13.32 154\n87                                   4p16.3 153\n88                                   1q21.1 151\n89                                 22q11.22 151\n90                                   2q37.1 150\n91                                   4p16.1 149\n92                                   6p22.3 148\n93                                     6q27 148\n94                                   1p34.2 147\n95                                   2q11.2 146\n96                                   1p31.3 145\n97                                   2q33.1 145\n98                                    17q22 144\n99                                   1p34.3 144\n100                                 Xp11.23 143\n101                                 22q12.3 142\n102                                 22q13.1 142\n103                                  2q21.1 142\n104                                 11q13.2 141\n105                                  1q21.2 141\n106                                 17p13.3 139\n107                                  1q23.3 139\n108                                14q32.31 138\n109                                  5q11.2 138\n110                                19p13.12 137\n111                                   16q21 136\n112                                 9q34.11 135\n113                                    1q41 133\n114                                 1p36.13 132\n115                                21q22.11 132\n116                                 11q24.2 131\n117                                 16q12.1 131\n118                                    4q12 131\n119                                  1p13.3 130\n120                                  2p23.3 130\n121                                 1q42.13 129\n122                                17q21.32 128\n123                                22q11.23 127\n124                                 Xp11.22 127\n125                                 16q23.1 126\n126                                 15q15.1 125\n127                                 17p13.2 125\n128                                    2p21 125\n129                                 18q11.2 124\n130                                 22q12.2 123\n131                                 22q13.2 123\n132                                 15q26.3 122\n133                                 8q21.13 122\n134                                 15q21.1 121\n135                                   13q34 120\n136                                 11p15.1 119\n137                                 14q32.2 119\n138                                 16p12.3 119\n139                                  1q25.3 119\n140                                    8p12 119\n141                                  Xq13.1 119\n142                                 15q25.2 118\n143                                  2p25.1 118\n144                                  Xp22.2 118\n145                                  3q21.3 117\n146                                  Xq26.3 117\n147                                   12q12 116\n148                                 1p36.12 116\n149                                  5q13.2 116\n150                                  Xq22.1 116\n151                                 11q14.1 115\n152                                 1p36.22 115\n153                                12q13.12 114\n154                                    4q25 114\n155                                  7p14.3 114\n156                                 18q21.1 113\n157                                    1q43 113\n158                       CHR_HSCHR7_2_CTG6 113\n159                                 10q26.3 112\n160                                  1p13.2 112\n161                                    Xq24 112\n162                                Yq11.223 112\n163                                 10q22.1 111\n164                                   15q14 111\n165                                 15q25.1 111\n166                                   17p12 111\n167                                  5q14.3 111\n168                                    5q32 111\n169                                  8q22.3 111\n170                                    Xq23 111\n171                                 Yq11.23 111\n172                                 11q22.3 110\n173                                15q22.31 110\n174                                  4q13.3 108\n175                                   11p13 107\n176                                 15q21.3 107\n177                                  8q22.1 107\n178                                10q11.21 106\n179                                   15q23 106\n180                                17q21.33 106\n181                                19q13.11 106\n182                                 6p21.32 106\n183                      CHR_HSCHR17_7_CTG4 106\n184                                13q12.11 105\n185                                   18q23 105\n186                                  7p14.1 105\n187                                20q11.21 104\n188                                  2q14.1 104\n189                                  6p22.2 104\n190                                  1p34.1 103\n191                                13q14.11 102\n192                                 1p36.21 102\n193                                 5p15.33 102\n194                                  8p21.3 102\n195                                 12q23.3 101\n196                                 15q22.2 101\n197                                20q11.23 101\n198                                  6q25.3 101\n199                                 11q12.3 100\n200                                   11q21 100\n201                                 16p12.2 100\n202                                  6q14.1 100\n203                                 10q22.2  99\n204                                20p11.21  99\n205                                  3p22.1  99\n206                                    4p14  99\n207                                 10p12.1  98\n208                                 22q11.1  98\n209                                    5q15  98\n210                                  1q23.1  97\n211                                22q13.31  97\n212                                  4q32.1  97\n213                                  7p22.1  97\n214                                    9q13  97\n215                                10q11.22  96\n216                                 12p13.2  96\n217                                 12q13.2  96\n218                                 16q24.1  96\n219                                    2q13  96\n220                                  4q35.1  96\n221                                 11q14.3  95\n222                                12q13.11  95\n223                                 12q23.1  95\n224                                 21p11.2  95\n225                                 21q21.1  95\n226                                 6p21.31  95\n227                                Yq11.222  95\n228                                  2q14.3  94\n229                                  7q21.3  94\n230                                 8q24.13  94\n231                                19q13.31  93\n232                                20q11.22  93\n233                                   10p13  92\n234                                20q13.13  92\n235                                    2p14  92\n236                                 10q22.3  91\n237                                  3p25.3  91\n238                                  3q22.1  91\n239                                  4q13.2  91\n240                                  5q14.1  91\n241                                  5q35.2  91\n242                                 18q12.1  90\n243                                    1q22  90\n244                                 22q12.1  90\n245                                  5q23.2  90\n246                                  6q25.1  90\n247                                    2p12  89\n248                                    8p22  89\n249                                 9q21.11  89\n250                                  Xp11.3  89\n251                                  Xp11.4  89\n252                                  2q33.3  88\n253                                    6q13  88\n254                                  9p21.3  88\n255                                12p13.33  87\n256                                 12q13.3  87\n257                                 15q21.2  87\n258                                    3q23  87\n259                                  5q35.1  87\n260                                  6q23.2  87\n261                                  2p13.3  86\n262                                  3q25.1  86\n263                                    7q33  86\n264                                  9p24.1  86\n265                                   12q15  85\n266                                 13q14.3  85\n267                                 16p12.1  85\n268                                18p11.31  85\n269                                  1p35.3  85\n270                                  1q24.2  85\n271                                22q13.33  85\n272                                  5p13.2  85\n273                                  5q31.2  85\n274                                  5q33.3  85\n275                                  8q12.1  85\n276                                 15q24.1  84\n277                                16p13.11  84\n278                                    1p33  84\n279                                  1q42.2  84\n280                                  2p16.1  84\n281                                  7p15.2  84\n282                                  9q33.3  84\n283                                12p11.21  83\n284                                 12p12.1  83\n285                                 17q24.2  83\n286                                  4q22.1  83\n287                                  4q31.3  83\n288                                    7q35  83\n289                                10q26.13  82\n290                                   12q22  82\n291                                    4q24  82\n292                                   10p14  81\n293                                  2q11.1  81\n294                                  4q35.2  80\n295                                  5q13.3  80\n296                                    6q15  80\n297                                  7p22.3  80\n298                                 9q22.33  80\n299                                  9q31.1  80\n300                                 17q23.2  79\n301                                  1q32.2  79\n302                                 3q26.33  79\n303                                    4q26  79\n304                      CHR_HSCHR15_4_CTG8  79\n305                                 10q21.3  78\n306                                12q24.11  78\n307                                 14q22.1  78\n308                                 14q23.1  78\n309                                 16q24.3  78\n310                                  3p22.2  78\n311                                 5p15.31  78\n312                                  7p15.3  78\n313                                  7q32.1  78\n314                                 8p11.21  78\n315                                  Xq21.1  78\n316                                 10p15.1  77\n317                                 13q14.2  77\n318                                 16q12.2  77\n319                                  1q25.2  77\n320                                  3p25.1  77\n321                                10q23.31  76\n322                                20p11.23  76\n323                                  2p25.3  76\n324                                 11q12.2  75\n325                                 12p12.3  75\n326                                  4q28.3  75\n327                                  5p13.3  75\n328                                  6p21.2  75\n329                    CHR_HSCHR19_4_CTG3_1  75\n330                                 11q23.1  74\n331                                 12q14.1  74\n332                                  1p22.1  74\n333                                  1p35.2  74\n334                                  2p13.1  74\n335                                 4q31.21  74\n336                                  6p12.3  74\n337                                 6q22.31  74\n338                                  9p11.2  74\n339                                 14q24.2  73\n340                                16p13.13  73\n341                                 21q21.3  73\n342                                 3q13.33  73\n343                                  4p15.2  73\n344                                  5p15.1  73\n345                                  5p15.2  73\n346                                  7q36.3  73\n347                                  8q21.3  73\n348                                 9q22.31  73\n349                                 9q22.32  73\n350                                 1p36.32  72\n351                                  1q32.3  72\n352                                    2p15  72\n353                                  4q32.3  72\n354                                  5q23.1  72\n355                                    7p13  72\n356                                    9q32  72\n357                                  Xq27.3  71\n358                                 15q24.2  70\n359                                 15q26.2  70\n360                                  3q27.1  70\n361                                  4q21.1  70\n362                                    5q34  70\n363                                  7q31.1  70\n364                                19p13.13  69\n365                                   19q12  69\n366                                  9q33.2  69\n367                                 10q25.3  68\n368                                 12q21.2  68\n369                                12q24.32  68\n370                                  1q23.2  68\n371                                  2p24.1  68\n372                                  3p14.1  68\n373                                    3q24  68\n374                                  6p12.1  68\n375                                  6q23.3  68\n376                                  7p21.3  68\n377                                 8q24.22  68\n378                                 14q24.1  67\n379                                 17q23.3  67\n380                                 18q21.2  67\n381                                  1p22.2  67\n382                                 20p12.1  67\n383                                  3q26.2  67\n384                                 7q21.11  67\n385                                10q24.32  66\n386                                 14q23.3  66\n387                                18p11.32  66\n388                                  3p14.3  66\n389                                  8p21.2  66\n390                                10p11.21  65\n391                                18p11.22  65\n392                                    1p12  65\n393                                 21q11.2  65\n394                                  3p24.3  65\n395                                  7p21.1  65\n396                                    Xq25  65\n397                                10q23.33  64\n398                                 11q24.1  64\n399                                 13q31.1  64\n400                                 14q21.3  64\n401                                   16q13  64\n402                                    3p13  64\n403                                10q11.23  63\n404                                11p11.12  63\n405                                13q12.12  63\n406                                  1p21.3  63\n407                                  2q36.3  63\n408                                  3p21.1  63\n409                                  3q27.3  63\n410                                  5p13.1  63\n411                                  6p25.2  63\n412                                 9q21.13  63\n413                                 15q13.1  62\n414                                 15q13.2  62\n415                                  1p22.3  62\n416                                  1q42.3  62\n417                                  2q24.2  62\n418                                  3q11.2  62\n419                                  Xq13.2  62\n420                                10p12.31  61\n421                                 11q14.2  61\n422                                 13q12.3  61\n423                                 15q25.3  61\n424                                 20p12.3  61\n425                                  2q32.1  61\n426                                    2q34  61\n427                                  3p22.3  61\n428                                12q21.31  60\n429                                 13q13.3  60\n430                                14q32.12  60\n431                                 16q22.2  60\n432                                 20q13.2  60\n433                                  2q24.1  60\n434                        CHR_HG2365_PATCH  60\n435                                 Xp11.21  60\n436                                 Xp22.33  60\n437                                  1q25.1  59\n438                                 21q22.2  59\n439                                  2p22.3  59\n440                                  3p21.2  59\n441                                    5p12  59\n442                                 8q24.21  59\n443                           CHR_HG1_PATCH  59\n444                      CHR_HSCHR16_1_CTG1  59\n445                                 10q23.1  58\n446                                12q24.23  58\n447                                 15q13.3  58\n448                                 16q24.2  58\n449                                  1p21.2  58\n450                                  3p12.3  58\n451                                  5q33.1  58\n452                                  7q22.3  58\n453                                  7q31.2  58\n454                                 8p11.23  58\n455                                  8q22.2  58\n456                                 10q21.1  57\n457                                   11q25  57\n458                                 14q22.3  57\n459                                 16p11.1  57\n460                                  3q21.2  57\n461                                 11q24.3  56\n462                                12p13.32  56\n463                                 14q21.1  56\n464                                 18q12.2  56\n465                                 1q42.12  56\n466                                  2p16.3  56\n467                                  2q12.1  56\n468                                  2q31.2  56\n469                                 8q21.11  56\n470                                 11q13.3  55\n471                                12q21.33  55\n472                                 13q32.3  55\n473                                18q21.32  55\n474                                  3q26.1  55\n475                                  5q21.1  55\n476                                 8q11.23  55\n477                                  9q31.3  55\n478                                  Xp21.1  55\n479                                  Xq22.3  55\n480                                  Xq26.2  55\n481                                  1p35.1  54\n482                                 3q26.31  54\n483                                  4q13.1  54\n484                                  4q34.1  54\n485                                  6q22.1  54\n486                                  8p23.3  54\n487                                  9p21.1  54\n488                                 11q13.5  53\n489                                 12q14.2  53\n490                                  2q32.3  53\n491                                 3q13.31  53\n492                                  6p24.3  53\n493                                 8q11.21  53\n494                     CHR_HSCHR5_2_CTG1_1  53\n495                                10p11.22  52\n496                                 10q23.2  52\n497                                 12q14.3  52\n498                                 16q23.2  52\n499                                 18q12.3  52\n500                                 18q22.3  52\n501                                 1p36.23  52\n502                                20q13.32  52\n503                                  6p25.1  52\n504                                  7p12.3  52\n505                                  7p14.2  52\n506                                 9q21.33  52\n507              CHR_HSCHR19LRC_PGF1_CTG3_1  52\n508                                 10q24.1  51\n509                                 10q25.2  51\n510                                12q24.13  51\n511                                13q12.13  51\n512                                  2q21.2  51\n513                                  2q23.3  51\n514                                  3q13.2  51\n515                                  3q22.3  51\n516                                10q26.11  50\n517                                 12p13.1  50\n518                                 14q31.3  50\n519                                 18q22.1  50\n520                                  5q12.1  50\n521                                  6q14.3  50\n522                                  6q16.1  50\n523                                  8q12.3  50\n524                                 9q21.32  50\n525                                 9q34.13  50\n526                                 11p14.1  49\n527                                 12q23.2  49\n528                                14q32.13  49\n529                                  2q24.3  49\n530                                 3q25.31  49\n531                                  4p15.1  49\n532                                  4q28.1  49\n533                                    6q12  49\n534                                  6q25.2  49\n535                                12p11.22  48\n536                                 12q21.1  48\n537                                 13q31.3  48\n538                                 13q32.1  48\n539                                  1q24.3  48\n540                                  2q32.2  48\n541                                    4p13  48\n542                                    4q23  48\n543                                  4q31.1  48\n544                                 10q25.1  47\n545                                   11q11  47\n546                                 11q23.2  47\n547                                 15q15.3  47\n548                                  1q24.1  47\n549                                  2p24.3  47\n550                                  2q14.2  47\n551                                  2q36.1  47\n552                                 3q13.13  47\n553                                  6q24.1  47\n554              CHR_HSCHR19LRC_COX1_CTG3_1  47\n555                                10p11.23  46\n556                                 10q24.2  46\n557                                 16p13.2  46\n558                                  1q31.1  46\n559                                  2p22.1  46\n560                                  2q22.1  46\n561                                  5q12.3  46\n562                                  6q24.2  46\n563                                  8p21.1  46\n564                                  9q34.2  46\n565                                  Xp21.3  46\n566                                   11p12  45\n567                                 14q23.2  45\n568                                 17q24.3  45\n569                                18q21.33  45\n570                                21q22.13  45\n571                                  2q22.3  45\n572                                  9q31.2  45\n573                                 10p15.3  44\n574                                10q24.31  44\n575                                12q24.21  44\n576                                13q14.13  44\n577                                14q32.11  44\n578                                  1p13.1  44\n579                                 20p12.2  44\n580                                  2p22.2  44\n581                                  8q23.1  44\n582                      CHR_HSCHR17_2_CTG5  44\n583                                 11p15.3  43\n584                                 1p36.31  43\n585                                  1q31.3  43\n586                                21q22.12  43\n587                                  3p24.1  43\n588                                 4p15.32  43\n589                                 7q31.32  43\n590                                  7q32.2  43\n591                                 Xp22.11  43\n592                                 14q31.1  42\n593                                 16q23.3  42\n594                                20q13.31  42\n595                                  3q25.2  42\n596                                  4q34.3  42\n597                                  9p13.2  42\n598                                 15q24.3  41\n599                                 17q24.1  41\n600                                   20q12  41\n601                                  2q33.2  41\n602                                 3q26.32  41\n603                                    3q28  41\n604                                    4p12  41\n605                                    4q27  41\n606                                  5p14.1  41\n607                                  6q24.3  41\n608                                  8q13.1  41\n609                                  8q13.3  41\n610                                  9q22.1  41\n611                        CHR_HG2290_PATCH  41\n612                                 Xp22.31  41\n613                                 14q13.2  40\n614                                 14q21.2  40\n615                                  1p11.2  40\n616                                 6q22.33  40\n617                                  8q21.2  40\n618                      CHR_HSCHR17_1_CTG5  40\n619                                 Xq21.31  40\n620                                 10q26.2  39\n621                                   21p12  39\n622                                 4p15.33  39\n623                                  5q21.3  39\n624                                  6p25.3  39\n625                      CHR_HSCHR15_6_CTG8  39\n626                                  Xq22.2  39\n627                                13q21.33  38\n628                                18q21.31  38\n629                                  1q31.2  38\n630                                  3p14.2  38\n631                                 4q21.21  38\n632                                 4q21.22  38\n633                                  5q33.2  38\n634                                  7p12.1  38\n635                                 8q24.12  38\n636                                    9p23  38\n637                                  9p24.3  38\n638                                 13q22.3  37\n639                                16p13.12  37\n640                                  2p23.2  37\n641                                  2q12.3  37\n642                                  3p25.2  37\n643                                  5p14.3  37\n644                                    6q26  37\n645                                  9q33.1  37\n646                      CHR_HSCHR14_7_CTG1  37\n647              CHR_HSCHR19LRC_PGF2_CTG3_1  37\n648                                      MT  37\n649                                 11p14.3  36\n650                                 11p15.2  36\n651                                 13q33.3  36\n652                                  9p22.3  36\n653                                  Xq26.1  36\n654                                Yq11.221  36\n655                                 13q33.1  35\n656                                 17q23.1  35\n657                                  3p26.1  35\n658                                  3q21.1  35\n659                                  6p12.2  35\n660                                  7q21.2  35\n661                                    Xq12  35\n662                                 Xq21.33  35\n663                                   15q12  34\n664                                  1p21.1  34\n665                                 21q21.2  34\n666                                  3p24.2  34\n667                                 7q21.13  34\n668                         CHR_HG109_PATCH  34\n669                      CHR_HSCHR22_1_CTG7  34\n670                                 10q21.2  33\n671                                 13q12.2  33\n672                                 7q11.22  33\n673                                 7q31.33  33\n674                                  9q22.2  33\n675                                    4p11  32\n676                                 4q21.23  32\n677                                 4q31.23  32\n678                                  5q22.3  32\n679                                  8p23.2  32\n680                 CHR_HG1342_HG2282_PATCH  32\n681                                 10p12.2  31\n682                                10q24.33  31\n683                                 11q22.1  31\n684                                   13q11  31\n685                                 13q13.1  31\n686                                14q32.32  31\n687                                  1p32.1  31\n688                                  2q12.2  31\n689                                  2q31.3  31\n690                                  3q12.3  31\n691                                 3q13.12  31\n692                                  7q32.3  31\n693                                  9p21.2  31\n694                      CHR_HSCHR12_3_CTG2  31\n695              CHR_HSCHR19LRC_COX2_CTG3_1  31\n696                                 Yq11.21  31\n697                                 10p11.1  30\n698                                12p11.23  30\n699                                 13q21.1  30\n700                                 3q25.32  30\n701                                  6q16.3  30\n702                                  8q11.1  30\n703                      CHR_HSCHR17_3_CTG1  30\n704                                  Xq27.1  30\n705                                 11q22.2  29\n706                                 15q15.2  29\n707                                  2p16.2  29\n708                                  5q22.1  29\n709                                    6p23  29\n710                                  6p24.2  29\n711                          CHR_HG30_PATCH  29\n712                                 Xp22.12  29\n713                                13q21.31  28\n714                                 14q13.3  28\n715                                  2p23.1  28\n716                                 3q25.33  28\n717                                    4q33  28\n718                                  5q23.3  28\n719                                 7q21.12  28\n720                                  9p11.1  28\n721                                  1p32.2  27\n722                                 20q11.1  27\n723                                 5p15.32  27\n724                                 7q31.31  27\n725                                  8q23.3  27\n726                                    9p12  27\n727                                  9p13.1  27\n728                                  9p24.2  27\n729                                12q24.22  26\n730                                  2p25.2  26\n731                                  2q21.3  26\n732                                  3p26.3  26\n733                                  3q12.1  26\n734                                  5q13.1  26\n735                                  8q13.2  26\n736                                  9p22.1  26\n737                                  9q21.2  26\n738                          CHR_HG26_PATCH  26\n739                      CHR_HSCHR12_2_CTG2  26\n740                      CHR_HSCHR17_4_CTG4  26\n741                                 Xp22.13  26\n742                                10q23.32  25\n743                                 13q31.2  25\n744                                 15q11.1  25\n745                                  2p13.2  25\n746                                  2p24.2  25\n747                                  4q28.2  25\n748                                  7q36.2  25\n749                                 8p11.22  25\n750                                 9q21.12  25\n751                        CHR_HG1815_PATCH  25\n752                        CHR_HG2023_PATCH  25\n753                      CHR_HSCHR15_1_CTG8  25\n754                                  Xq13.3  25\n755                                12q21.32  24\n756                                 13q21.2  24\n757                                 13q22.1  24\n758                                20p11.22  24\n759                                 4q31.22  24\n760                      CHR_HSCHR15_5_CTG8  24\n761                      CHR_HSCHR22_1_CTG3  24\n762                     CHR_HSCHR2_6_CTG7_2  24\n763                     CHR_HSCHR5_1_CTG1_1  24\n764                                  Xp21.2  24\n765                                 10p15.2  23\n766                                 17q21.1  23\n767                                  3q22.2  23\n768                                  4q22.3  23\n769                                  5q22.2  23\n770                        CHR_HG2030_PATCH  23\n771                         CHR_HG708_PATCH  23\n772             CHR_HSCHR19LRC_LRC_I_CTG3_1  23\n773             CHR_HSCHR19LRC_LRC_T_CTG3_1  23\n774                     CHR_HSCHR4_11_CTG12  23\n775                       CHR_HSCHR8_3_CTG7  23\n776                                13q21.32  22\n777                                 14q13.1  22\n778                                 18q22.2  22\n779                                  3p12.2  22\n780                                  3q27.2  22\n781                                  5q14.2  22\n782                                  8p11.1  22\n783                      CHR_HSCHR15_1_CTG1  22\n784                       CHR_HSCHR4_1_CTG9  22\n785                     CHR_HSCHR7_3_CTG4_4  22\n786                                 16q11.2  21\n787                                 1q42.11  21\n788                                  3p11.1  21\n789                                  3p12.1  21\n790                                  6p24.1  21\n791                                  7p21.2  21\n792                                 9q21.31  21\n793                        CHR_HG2002_PATCH  21\n794                        CHR_HG2066_PATCH  21\n795          CHR_HG2246_HG2248_HG2276_PATCH  21\n796             CHR_HSCHR19LRC_LRC_J_CTG3_1  21\n797                       CHR_HSCHR8_3_CTG1  21\n798                                  Xq11.2  21\n799                                12q24.12  20\n800                                 13q32.2  20\n801                                 4p15.31  20\n802                        CHR_HG2198_PATCH  20\n803                         CHR_HG926_PATCH  20\n804                     CHR_HSCHR17_10_CTG4  20\n805             CHR_HSCHR19LRC_LRC_S_CTG3_1  20\n806                                 13q22.2  19\n807                                 14q22.2  19\n808                                 20p11.1  19\n809                                  2q37.2  19\n810                                  4q32.2  19\n811                        CHR_HG2046_PATCH  19\n812                        CHR_HG2513_PATCH  19\n813        CHR_HSCHR19KIR_FH15_B_HAP_CTG3_1  19\n814                       CHR_HSCHR1_5_CTG3  19\n815                                 12p11.1  18\n816                                 16q22.3  18\n817                                  4p16.2  18\n818                                  5q11.1  18\n819                                  6p11.2  18\n820                        CHR_HG2419_PATCH  18\n821        CHR_HSCHR19KIR_FH05_B_HAP_CTG3_1  18\n822                                  2q22.2  17\n823                                  2q23.1  17\n824                                  4q34.2  17\n825                                  6q16.2  17\n826                                  8q23.2  17\n827                                 8q24.11  17\n828                        CHR_HG2087_PATCH  17\n829                        CHR_HG2217_PATCH  17\n830                         CHR_HG545_PATCH  17\n831         CHR_HSCHR19KIR_CA01-TB04_CTG3_1  17\n832      CHR_HSCHR19KIR_FH13_BA2_HAP_CTG3_1  17\n833                       CHR_HSCHR1_2_CTG3  17\n834                                10p12.33  16\n835                                10q26.12  16\n836                                 13q13.2  16\n837                                18p11.23  16\n838                                   19q11  16\n839                                  5p14.2  16\n840                                  7p12.2  16\n841                        CHR_HG1362_PATCH  16\n842              CHR_HG142_HG150_NOVEL_TEST  16\n843                          CHR_HG28_PATCH  16\n844                      CHR_HSCHR11_1_CTG8  16\n845         CHR_HSCHR19KIR_CA01-TB01_CTG3_1  16\n846                    CHR_HSCHR19_3_CTG3_1  16\n847                       CHR_HSCHR8_8_CTG1  16\n848                                  Xq21.2  16\n849                                  Xq27.2  16\n850                                 12p12.2  15\n851                                 17q11.1  15\n852                                 3q13.32  15\n853                                 8q24.23  15\n854                                  9p22.2  15\n855                        CHR_HG2114_PATCH  15\n856                        CHR_HG2499_PATCH  15\n857                      CHR_HSCHR11_1_CTG7  15\n858       CHR_HSCHR19KIR_0019-4656-B_CTG3_1  15\n859      CHR_HSCHR19KIR_FH08_BAX_HAP_CTG3_1  15\n860                       CHR_HSCHR1_4_CTG3  15\n861                                 13q33.2  14\n862                                  4q21.3  14\n863                                    9q12  14\n864                        CHR_HG2021_PATCH  14\n865                         CHR_HG439_PATCH  14\n866                      CHR_HSCHR15_3_CTG3  14\n867                      CHR_HSCHR16_3_CTG1  14\n868         CHR_HSCHR19KIR_7191059-2_CTG3_1  14\n869        CHR_HSCHR19KIR_FH15_A_HAP_CTG3_1  14\n870        CHR_HSCHR19KIR_G248_A_HAP_CTG3_1  14\n871     CHR_HSCHR19KIR_GRC212_AB_HAP_CTG3_1  14\n872     CHR_HSCHR19KIR_LUCE_BDEL_HAP_CTG3_1  14\n873    CHR_HSCHR19KIR_T7526_BDEL_HAP_CTG3_1  14\n874                       CHR_HSCHR1_1_CTG3  14\n875                      CHR_HSCHR1_1_CTG31  14\n876                    CHR_HSCHR1_5_CTG32_1  14\n877                                 17q25.2  13\n878                                20q13.11  13\n879                                22q13.32  13\n880                                 3q13.11  13\n881                                  6q11.1  13\n882                                 6q22.32  13\n883                                  7p22.2  13\n884                                 9q34.12  13\n885                    CHR_HSCHR11_2_CTG1_1  13\n886       CHR_HSCHR19KIR_502960008-1_CTG3_1  13\n887       CHR_HSCHR19KIR_502960008-2_CTG3_1  13\n888         CHR_HSCHR19KIR_7191059-1_CTG3_1  13\n889  CHR_HSCHR19KIR_ABC08_AB_HAP_T_P_CTG3_1  13\n890       CHR_HSCHR19KIR_CA01-TA01_1_CTG3_1  13\n891       CHR_HSCHR19KIR_CA01-TA01_2_CTG3_1  13\n892        CHR_HSCHR19KIR_FH05_A_HAP_CTG3_1  13\n893        CHR_HSCHR19KIR_FH06_A_HAP_CTG3_1  13\n894      CHR_HSCHR19KIR_FH06_BA1_HAP_CTG3_1  13\n895        CHR_HSCHR19KIR_FH13_A_HAP_CTG3_1  13\n896        CHR_HSCHR19KIR_G085_A_HAP_CTG3_1  13\n897      CHR_HSCHR19KIR_G085_BA1_HAP_CTG3_1  13\n898            CHR_HSCHR19KIR_HG2394_CTG3_1  13\n899         CHR_HSCHR19KIR_RSH_A_HAP_CTG3_1  13\n900       CHR_HSCHR19KIR_RSH_BA2_HAP_CTG3_1  13\n901       CHR_HSCHR19KIR_T7526_A_HAP_CTG3_1  13\n902                      CHR_HSCHR4_6_CTG12  13\n903                                  Xq11.1  13\n904                                  3q12.2  12\n905                                  5q21.2  12\n906                                  8q12.2  12\n907                    CHR_HG151_NOVEL_TEST  12\n908                      CHR_HSCHR11_1_CTG6  12\n909                      CHR_HSCHR15_2_CTG3  12\n910                      CHR_HSCHR17_1_CTG4  12\n911       CHR_HSCHR19KIR_0019-4656-A_CTG3_1  12\n912      CHR_HSCHR19KIR_ABC08_A1_HAP_CTG3_1  12\n913        CHR_HSCHR19KIR_FH08_A_HAP_CTG3_1  12\n914         CHR_HSCHR19KIR_RP5_B_HAP_CTG3_1  12\n915                       CHR_HSCHR3_5_CTG1  12\n916                       CHR_HSCHR3_9_CTG3  12\n917                                13q14.12  11\n918                                19q13.13  11\n919                                    3p23  11\n920                                  6q14.2  11\n921                        CHR_HG1277_PATCH  11\n922                        CHR_HG1309_PATCH  11\n923                        CHR_HG1398_PATCH  11\n924                        CHR_HG2263_PATCH  11\n925                    CHR_HSCHR11_1_CTG1_2  11\n926                      CHR_HSCHR15_1_CTG3  11\n927                      CHR_HSCHR17_1_CTG1  11\n928      CHR_HSCHR19KIR_G248_BA2_HAP_CTG3_1  11\n929                      CHR_HSCHR19_1_CTG2  11\n930                      CHR_HSCHR19_3_CTG2  11\n931                      CHR_HSCHR22_3_CTG1  11\n932                                 Xq21.32  11\n933                                  2p11.1  10\n934                                  3p26.2  10\n935                        CHR_HG2057_PATCH  10\n936                        CHR_HG2266_PATCH  10\n937                        CHR_HG2334_PATCH  10\n938                      CHR_HSCHR10_1_CTG2  10\n939                      CHR_HSCHR17_1_CTG2  10\n940  CHR_HSCHR19KIR_ABC08_AB_HAP_C_P_CTG3_1  10\n941    CHR_HSCHR19KIR_GRC212_BA1_HAP_CTG3_1  10\n942            CHR_HSCHR19KIR_HG2393_CTG3_1  10\n943        CHR_HSCHR19KIR_LUCE_A_HAP_CTG3_1  10\n944                      CHR_HSCHR1_2_CTG31  10\n945                       CHR_HSCHR5_8_CTG1  10\n946                       CHR_HSCHR7_2_CTG1  10\n947                                 18q11.1   9\n948                                 22p11.2   9\n949                                  3p11.2   9\n950                        CHR_HG1832_PATCH   9\n951                        CHR_HG2058_PATCH   9\n952                        CHR_HG2095_PATCH   9\n953                        CHR_HG2115_PATCH   9\n954                        CHR_HG2232_PATCH   9\n955           CHR_HG2285_HG106_HG2252_PATCH   9\n956                        CHR_HG2291_PATCH   9\n957                        CHR_HG2525_PATCH   9\n958                      CHR_HSCHR11_1_CTG5   9\n959                      CHR_HSCHR11_2_CTG8   9\n960                      CHR_HSCHR12_1_CTG1   9\n961                      CHR_HSCHR13_1_CTG3   9\n962                      CHR_HSCHR14_1_CTG1   9\n963                      CHR_HSCHR15_3_CTG8   9\n964                      CHR_HSCHR17_6_CTG4   9\n965      CHR_HSCHR19KIR_0010-5217-AB_CTG3_1   9\n966            CHR_HSCHR19KIR_HG2396_CTG3_1   9\n967                      CHR_HSCHR19_2_CTG2   9\n968               CHR_HSCHR1_ALT2_1_CTG32_1   9\n969                      CHR_HSCHR22_1_CTG6   9\n970                      CHR_HSCHR22_2_CTG1   9\n971                      CHR_HSCHR2_3_CTG15   9\n972                     CHR_HSCHR2_8_CTG7_2   9\n973                       CHR_HSCHR6_1_CTG8   9\n974                     CHR_HSCHR7_2_CTG4_4   9\n975                       CHR_HSCHR8_9_CTG1   9\n976                       CHR_HSCHR9_1_CTG5   9\n977                                 15q22.1   8\n978                                  2q23.2   8\n979                                  3q11.1   8\n980                        CHR_HG1311_PATCH   8\n981                        CHR_HG1395_PATCH   8\n982                        CHR_HG1535_PATCH   8\n983                        CHR_HG2111_PATCH   8\n984                        CHR_HG2121_PATCH   8\n985                      CHR_HSCHR17_2_CTG2   8\n986                      CHR_HSCHR17_5_CTG4   8\n987              CHR_HSCHR19KIR_CA04_CTG3_1   8\n988                    CHR_HSCHR1_3_CTG32_1   8\n989                      CHR_HSCHR1_4_CTG31   8\n990                       CHR_HSCHR1_6_CTG3   8\n991                      CHR_HSCHR22_1_CTG1   8\n992                      CHR_HSCHR22_4_CTG1   8\n993                      CHR_HSCHR22_7_CTG1   8\n994                       CHR_HSCHR3_1_CTG1   8\n995                       CHR_HSCHR5_2_CTG5   8\n996                       CHR_HSCHR5_3_CTG5   8\n997                       CHR_HSCHR6_1_CTG4   8\n998                       CHR_HSCHR7_1_CTG1   8\n999                     CHR_HSCHR7_1_CTG4_4   8\n1000                      CHR_HSCHR8_5_CTG7   8\n1001                                11p14.2   7\n1002                                14q31.2   7\n1003                                 2q36.2   7\n1004                                 4q22.2   7\n1005                                 6q23.1   7\n1006                       CHR_HG1485_PATCH   7\n1007                        CHR_HG986_PATCH   7\n1008                     CHR_HSCHR10_1_CTG1   7\n1009                     CHR_HSCHR10_1_CTG4   7\n1010                   CHR_HSCHR11_1_CTG3_1   7\n1011                     CHR_HSCHR12_4_CTG2   7\n1012                     CHR_HSCHR16_5_CTG1   7\n1013                   CHR_HSCHR16_5_CTG3_1   7\n1014                       CHR_HSCHR16_CTG2   7\n1015                   CHR_HSCHR1_2_CTG32_1   7\n1016                     CHR_HSCHR22_8_CTG1   7\n1017                      CHR_HSCHR2_2_CTG7   7\n1018                      CHR_HSCHR3_1_CTG3   7\n1019                    CHR_HSCHR3_4_CTG2_1   7\n1020                      CHR_HSCHR3_5_CTG3   7\n1021                      CHR_HSCHR6_1_CTG5   7\n1022                      CHR_HSCHR7_1_CTG6   7\n1023                      CHR_HSCHR8_4_CTG7   7\n1024                      CHR_HSCHRX_1_CTG3   7\n1025                             KI270727.1   7\n1026                                Xp22.32   7\n1027                                 1p31.2   6\n1028                                3p21.33   6\n1029                                 7p15.1   6\n1030                       CHR_HG2072_PATCH   6\n1031                       CHR_HG2191_PATCH   6\n1032                       CHR_HG2213_PATCH   6\n1033                       CHR_HG2412_PATCH   6\n1034                   CHR_HSCHR12_1_CTG2_1   6\n1035                     CHR_HSCHR14_8_CTG1   6\n1036                   CHR_HSCHR18_1_CTG1_1   6\n1037                   CHR_HSCHR18_3_CTG2_1   6\n1038                     CHR_HSCHR19_5_CTG2   6\n1039                      CHR_HSCHR1_8_CTG3   6\n1040                     CHR_HSCHR20_1_CTG3   6\n1041                   CHR_HSCHR21_4_CTG1_1   6\n1042                     CHR_HSCHR22_5_CTG1   6\n1043                     CHR_HSCHR22_6_CTG1   6\n1044                      CHR_HSCHR3_3_CTG3   6\n1045                      CHR_HSCHR3_4_CTG3   6\n1046                      CHR_HSCHR3_6_CTG3   6\n1047                      CHR_HSCHR3_7_CTG3   6\n1048                      CHR_HSCHR3_8_CTG3   6\n1049                     CHR_HSCHR4_7_CTG12   6\n1050                     CHR_HSCHR4_9_CTG12   6\n1051                      CHR_HSCHR5_3_CTG1   6\n1052                      CHR_HSCHR8_1_CTG1   6\n1053                      CHR_HSCHR8_7_CTG1   6\n1054                             KI270728.1   6\n1055                                 7q22.2   5\n1056                                   9q11   5\n1057                        CHR_HG126_PATCH   5\n1058                       CHR_HG1445_PATCH   5\n1059                       CHR_HG2236_PATCH   5\n1060                     CHR_HSCHR10_1_CTG3   5\n1061                     CHR_HSCHR11_3_CTG1   5\n1062                     CHR_HSCHR13_1_CTG1   5\n1063                   CHR_HSCHR16_4_CTG3_1   5\n1064                     CHR_HSCHR17_2_CTG1   5\n1065                     CHR_HSCHR18_2_CTG2   5\n1066                     CHR_HSCHR1_3_CTG31   5\n1067                   CHR_HSCHR21_2_CTG1_1   5\n1068                     CHR_HSCHR21_5_CTG2   5\n1069                   CHR_HSCHR21_6_CTG1_1   5\n1070                     CHR_HSCHR2_1_CTG15   5\n1071                     CHR_HSCHR2_2_CTG15   5\n1072                    CHR_HSCHR3_6_CTG2_1   5\n1073                    CHR_HSCHR4_12_CTG12   5\n1074                      CHR_HSCHR6_1_CTG2   5\n1075                      CHR_HSCHR7_3_CTG6   5\n1076                      CHR_HSCHR8_1_CTG6   5\n1077                      CHR_HSCHR8_2_CTG7   5\n1078                      CHR_HSCHRX_2_CTG3   5\n1079                               15q22.33   4\n1080                        CHR_HG107_PATCH   4\n1081                       CHR_HG2060_PATCH   4\n1082                       CHR_HG2104_PATCH   4\n1083                       CHR_HG2235_PATCH   4\n1084                       CHR_HG2471_PATCH   4\n1085                   CHR_HSCHR12_3_CTG2_1   4\n1086                   CHR_HSCHR12_9_CTG2_1   4\n1087                   CHR_HSCHR16_2_CTG3_1   4\n1088                     CHR_HSCHR16_4_CTG1   4\n1089                     CHR_HSCHR17_1_CTG9   4\n1090                     CHR_HSCHR17_3_CTG2   4\n1091                CHR_HSCHR18_ALT2_CTG2_1   4\n1092                   CHR_HSCHR21_3_CTG1_1   4\n1093                     CHR_HSCHR22_1_CTG2   4\n1094                     CHR_HSCHR22_1_CTG4   4\n1095                     CHR_HSCHR22_1_CTG5   4\n1096                      CHR_HSCHR2_1_CTG1   4\n1097                      CHR_HSCHR2_1_CTG7   4\n1098                    CHR_HSCHR2_1_CTG7_2   4\n1099                    CHR_HSCHR2_3_CTG7_2   4\n1100                      CHR_HSCHR3_2_CTG3   4\n1101                    CHR_HSCHR3_9_CTG2_1   4\n1102                      CHR_HSCHR4_1_CTG4   4\n1103                     CHR_HSCHR4_3_CTG12   4\n1104                      CHR_HSCHR5_2_CTG1   4\n1105                      CHR_HSCHR5_4_CTG1   4\n1106                      CHR_HSCHR5_5_CTG1   4\n1107                      CHR_HSCHR5_6_CTG1   4\n1108                      CHR_HSCHR6_1_CTG9   4\n1109                      CHR_HSCHR8_4_CTG1   4\n1110                      CHR_HSCHR9_1_CTG3   4\n1111                             GL000220.1   4\n1112                             KI270713.1   4\n1113                             KI270721.1   4\n1114                             KI270733.1   4\n1115                             KI270734.1   4\n1116                                3p21.32   3\n1117                       CHR_HG1298_PATCH   3\n1118                       CHR_HG1320_PATCH   3\n1119                       CHR_HG1524_PATCH   3\n1120                       CHR_HG1708_PATCH   3\n1121                       CHR_HG2063_PATCH   3\n1122                       CHR_HG2133_PATCH   3\n1123                       CHR_HG2247_PATCH   3\n1124                CHR_HG2288_HG2289_PATCH   3\n1125                       CHR_HG2442_PATCH   3\n1126                       CHR_HG2509_PATCH   3\n1127                       CHR_HG2510_PATCH   3\n1128                       CHR_HG2511_PATCH   3\n1129                        CHR_HG699_PATCH   3\n1130                        CHR_HG705_PATCH   3\n1131                        CHR_HG721_PATCH   3\n1132                     CHR_HSCHR11_2_CTG1   3\n1133                   CHR_HSCHR12_2_CTG2_1   3\n1134                   CHR_HSCHR12_4_CTG2_1   3\n1135                   CHR_HSCHR12_5_CTG2_1   3\n1136                   CHR_HSCHR12_6_CTG2_1   3\n1137                     CHR_HSCHR13_1_CTG5   3\n1138                     CHR_HSCHR14_2_CTG1   3\n1139                     CHR_HSCHR15_2_CTG8   3\n1140                   CHR_HSCHR16_1_CTG3_1   3\n1141                     CHR_HSCHR17_3_CTG4   3\n1142                   CHR_HSCHR18_2_CTG2_1   3\n1143                   CHR_HSCHR19_1_CTG3_1   3\n1144                   CHR_HSCHR19_2_CTG3_1   3\n1145                     CHR_HSCHR1_1_CTG11   3\n1146                     CHR_HSCHR20_1_CTG1   3\n1147                     CHR_HSCHR20_1_CTG4   3\n1148                      CHR_HSCHR2_1_CTG5   3\n1149                      CHR_HSCHR2_2_CTG1   3\n1150                      CHR_HSCHR2_4_CTG1   3\n1151                    CHR_HSCHR3_1_CTG2_1   3\n1152                    CHR_HSCHR3_2_CTG2_1   3\n1153                      CHR_HSCHR3_4_CTG1   3\n1154                    CHR_HSCHR3_8_CTG2_1   3\n1155                     CHR_HSCHR4_1_CTG12   3\n1156                     CHR_HSCHR4_5_CTG12   3\n1157                    CHR_HSCHR5_4_CTG1_1   3\n1158                      CHR_HSCHR7_3_CTG1   3\n1159                      CHR_HSCHR8_2_CTG1   3\n1160                      CHR_HSCHR9_1_CTG2   3\n1161                                   Yq12   3\n1162                               15q22.32   2\n1163                                 6q22.2   2\n1164                                8q21.12   2\n1165                       CHR_HG1384_PATCH   2\n1166                       CHR_HG2022_PATCH   2\n1167                       CHR_HG2047_PATCH   2\n1168                       CHR_HG2067_PATCH   2\n1169                       CHR_HG2088_PATCH   2\n1170                       CHR_HG2116_PATCH   2\n1171                       CHR_HG2128_PATCH   2\n1172                       CHR_HG2233_PATCH   2\n1173                       CHR_HG2249_PATCH   2\n1174                       CHR_HG2512_PATCH   2\n1175                     CHR_HSCHR17_8_CTG4   2\n1176                     CHR_HSCHR17_9_CTG4   2\n1177                     CHR_HSCHR18_1_CTG1   2\n1178                   CHR_HSCHR18_1_CTG2_1   2\n1179                   CHR_HSCHR18_5_CTG1_1   2\n1180               CHR_HSCHR18_ALT21_CTG2_1   2\n1181                      CHR_HSCHR1_9_CTG3   2\n1182                     CHR_HSCHR20_1_CTG2   2\n1183                   CHR_HSCHR21_8_CTG1_1   2\n1184                    CHR_HSCHR2_7_CTG7_2   2\n1185                    CHR_HSCHR3_5_CTG2_1   2\n1186                     CHR_HSCHR4_8_CTG12   2\n1187                      CHR_HSCHR5_1_CTG5   2\n1188                      CHR_HSCHR6_1_CTG7   2\n1189                      CHR_HSCHR6_8_CTG1   2\n1190                      CHR_HSCHR8_1_CTG7   2\n1191                      CHR_HSCHR8_5_CTG1   2\n1192                      CHR_HSCHR8_6_CTG1   2\n1193                      CHR_HSCHR8_7_CTG7   2\n1194                      CHR_HSCHR9_1_CTG4   2\n1195                      CHR_HSCHR9_1_CTG6   2\n1196                             GL000194.1   2\n1197                             GL000195.1   2\n1198                             GL000213.1   2\n1199                             KI270442.1   2\n1200                             KI270726.1   2\n1201                             KI270731.1   2\n1202                                  12q11   1\n1203                                14p11.2   1\n1204                                 6q11.2   1\n1205                                8q11.22   1\n1206                       CHR_HG1531_PATCH   1\n1207                       CHR_HG1651_PATCH   1\n1208                       CHR_HG2062_PATCH   1\n1209                       CHR_HG2239_PATCH   1\n1210                     CHR_HSCHR10_1_CTG6   1\n1211                     CHR_HSCHR12_5_CTG2   1\n1212                   CHR_HSCHR12_8_CTG2_1   1\n1213                     CHR_HSCHR14_9_CTG1   1\n1214                    CHR_HSCHR17_11_CTG4   1\n1215                    CHR_HSCHR17_12_CTG4   1\n1216                     CHR_HSCHR17_2_CTG4   1\n1217                   CHR_HSCHR18_2_CTG1_1   1\n1218                     CHR_HSCHR19_4_CTG2   1\n1219                   CHR_HSCHR1_1_CTG32_1   1\n1220                      CHR_HSCHR1_3_CTG3   1\n1221                    CHR_HSCHR2_2_CTG7_2   1\n1222                      CHR_HSCHR2_3_CTG1   1\n1223                      CHR_HSCHR3_3_CTG1   1\n1224                      CHR_HSCHR4_1_CTG6   1\n1225                     CHR_HSCHR4_2_CTG12   1\n1226                      CHR_HSCHR4_2_CTG4   1\n1227                     CHR_HSCHR4_4_CTG12   1\n1228                      CHR_HSCHR5_1_CTG1   1\n1229                      CHR_HSCHR5_7_CTG1   1\n1230                      CHR_HSCHR6_1_CTG3   1\n1231                      CHR_HSCHR6_1_CTG6   1\n1232                      CHR_HSCHR7_1_CTG7   1\n1233                      CHR_HSCHR7_2_CTG7   1\n1234                      CHR_HSCHR9_1_CTG1   1\n1235                     CHR_HSCHRX_2_CTG12   1\n1236                             GL000009.2   1\n1237                             GL000205.2   1\n1238                             GL000216.2   1\n1239                             GL000218.1   1\n1240                             GL000219.1   1\n1241                             GL000225.1   1\n1242                             KI270711.1   1\n1243                             KI270744.1   1\n1244                             KI270750.1   1\n\n\nShow the code\nanno_gene %>% count(cytoband) %>% arrange(desc(n)) %>% summary()\n\n\n   cytoband               n         \n Length:1244        Min.   :  1.00  \n Class :character   1st Qu.: 10.00  \n Mode  :character   Median : 39.00  \n                    Mean   : 55.71  \n                    3rd Qu.: 78.00  \n                    Max.   :457.00"
  },
  {
    "objectID": "post/2022-08-28-ukbrest-sqlite-setup/index.html",
    "href": "post/2022-08-28-ukbrest-sqlite-setup/index.html",
    "title": "ukbREST SQLite Setup",
    "section": "",
    "text": "This post explains how to 1. Query UKBREST in CRI 2. Create an SQLite database from the postgres database 3. Update the withdrawal list\n\nThe version of ukbREST that runs on SQLite is HERE. If you come across an existing ukbrest repo in CRI, it might be the original version.\n\ngit clone https://github.com/sabrina-mi/ukbrest\n\n\nThe standard ukbREST setup involves loading UKB CSVs into a Postgres database, but because of CRI limitations, we copied it to an SQLite file.\n\n\nQuerying ukbREST\nThe SQLite file is kept in the UKB labshare: /gpfs/data/ukb-share/sqlite/db. Before we can make any queries, we need to start the ukbREST server. This program needs to either be kept running all the time in a background terminal, or restarted each time you need to pull queries. If you are querying in CRI, this part needs to be run in the login node, NOT as an interactive job.\nconda activate /gpfs/data/im-lab/nas40t2/lab_software/miniconda/envs/ukb_env/\nexport UKBREST_DB_URI=\"sqlite:////gpfs/data/ukb-share/sqlite/db/ukbrest.db\"\ncd /gpfs/data/ukb-share/sqlite/ukbrest\n/gpfs/data/im-lab/nas40t2/lab_software/miniconda/envs/ukb_env/bin/python \\\n-m ukbrest.app \\\n--db-uri sqlite:////gpfs/data/ukb-share/sqlite/db/ukbrest.db \\\n--debug\n\nIf program looks like it’s hanging and prints something like the line below, the server is running and ready to accept queried.\n2022-08-28 20:57:36,753 - werkzeug - INFO -  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\nFlask defaults to http://127.0.0.1:5000/, so if you see the error, OSError: [Errno 98] Address already in use, it means that some other server is running on port 5000. You’ll need to find the other process and kill it.\nOpen up a new terminal, or if you’ve started the server in CRI, login in a new window. Here’s an example query from a YAML file:\n$ cat test.yaml \nsamples_filters:\n  - eid not in (select eid from withdrawals)\n  - eid > 0\ndata:\n  cause_of_death: c40001_0_0\n  \ncurl -X POST \\\n  -H \"Accept: text/csv\" \\\n  -F file=@\"test.yaml\" \\\n  -F section=\"data\" \\\n  http://127.0.0.1:5000/ukbrest/api/v1.0/query \\\n  > test.csv\n  \nOr column query:\ncurl -G \\\n-HAccept:text/csv \\\n\"http://127.0.0.1:5000/ukbrest/api/v1.0/phenotype\" \\\n--data-urlencode \"columns=c3_0_0\" > column_query.csv\n\nThe following error prints after querying if you are running the original version of ukbREST. Replacing with the updated branch and restarting the server should do the trick.\n\n\nCreating the SQLite database from Postgres\nWe already had a ukbREST server running in Bionimbus, so it was quicker to copy the Postgres database to an SQLite file instead of loading from scratch. This step only needs to be done once.\nThe script pg2sqlite.py takes a list of table names in the Postgres database and copies them to ukbrest.db. I’ve added it to the Github, in migration/pg2sqlite.py, but the SQLite path and Postgres URI are hard-coded, if you plan to use it.\nconda activate ukbrest\ncd /mnt/sql\npython pg2sqlite.py tables.txt\n## find this code here https://github.com/sabrina-mi/ukbrest/blob/master/migration/pg2sqlite.py\n\n\nUpdate Withdrawal List Periodically\nWe decided to copy the Postgres database once, rather than loading data the documented way, because the jobs were extremely slow to finish in CRI. However, we will need to update the database every time a new withdrawal list is sent out, so we would want to update the withdrawals table directly from CSV.\nCreate a folder for withdrawals, and move the CSV there. The data in ukb-share was downloaded from UKB Application 19526, so the CSV should be named something like w19526_*.csv\nconda activate /gpfs/data/im-lab/nas40t2/lab_software/miniconda/envs/ukb_env/\nexport UKBREST_DB_URI=\"sqlite:////gpfs/data/ukb-share/sqlite/db/ukbrest.db\"\nexport UKBREST_WITHDRAWALS_PATH=\"/gpfs/data/ukb-share/withdrawals/\"\ncd /mnt/software/ukbrest\npython -m ukbrest.load_data \\\n --load-withdrawals \\\n --withdrawals-dir /gpfs/data/ukb-share/withdrawals/ \\\n --db-uri sqlite:////gpfs/data/ukb-share/sqlite/db/ukbrest.db \\\n --debug\nIf you are loading new phenotype data in CRI, it gets a little complicated. The main difference is that we have to break up each CSV into multiple jobs, so that CRI doesn’t kill it for taking too long. The directory /gpfs/data/ukb-share/sql/test includes example PBS scripts to split CSVs into tables with 750 columns (test_split.sh) and load the smaller file into test.db (test_load.sh)."
  },
  {
    "objectID": "post/2021-06-28-predictdb-tutorial/index.html",
    "href": "post/2021-06-28-predictdb-tutorial/index.html",
    "title": "PredictDB Tutorial",
    "section": "",
    "text": "This tutorial will provide the steps you need to follow to get you started training prediction models and putting them in a format that can be used to run the predixcan suite of software.\n\n\nThe training dataset can be downloaded from this link.The data set includes the following files you will use for learning; 1. Normalized gene expression - GD462.GeneQuantRPKM.50FN.samplename.resk10.txt.gz 2. SNP annotation file - geuvadis.annot.txt 3. Gene annotation file - gencode.v12.annotation.gtf.gz 4. Genotype (text format) - geuvadis.snps.dosage.txt\nDownload the files into a directory named data.\n\n\n\n\nParse the gene annotation file Use the script parse_gtf.py to parse the gene annotation file. This script extracts the important columns about the genes into a file that will be used downstream. The extracted columns include gene name, gene id, start, end, chr and gene type. Run the following command to get the parsed file\n\npython ./code/parse_gtf.py ./data/'gencode.v12.annotation.gtf' ./output/'gene_annot.parsed.txt'\n\nSNP annotation First do a quick renaming of the field headers to match the desired column names for downstream processing\n\nsed -e 's/Chr/chromosome/g' -e 's/Ref_b37/ref_vcf/g' -e 's/Alt/alt_vcf/g' ./data/geuvadis.annot.txt > ./data/snp_annotation.txt\nSplit the annotation file into individual chromosomes. You will end up with 22 files for each autosome\npython ./code/split_snp_annot_by_chr.py ./data/geuvadis.annot.txt ./output/snp_annot\n\n\n\nUsing your R software load the expression file\n# Load packages\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(RSQLite)\n\n# Load data\ngene_exp = read.table(file = \"./data/GD462.GeneQuantRPKM.50FN.samplename.resk10.txt\", header = TRUE, sep = \"\\t\" )\n\n##Dropping columns we don't need in the gene expression dataframe\ngene_exp = gene_exp[-c(1, 3, 4)]\n\n# rename a column\ngene_exp = rename(gene_exp, 'Gene_Name' = Gene_Symbol)\n\n# Transpose the data to enable you rub peer factors\nn = gene_exp$Gene_Name\ngene_exp_transpose <- as.data.frame(t(gene_exp[,-1]))\ncolnames(gene_exp_transpose) <- n\n\n# write the transposed gene expression. The .csv is for the peer tool\nwrite.table(gene_exp_transpose, file = './output/gene_exp.csv', sep = \",\", col.names = TRUE, row.names = FALSE)\nwrite.table(gene_exp_transpose, file = \"./output/transformed_expression.txt\", sep = \"\\t\",\n            row.names = TRUE)\n\nCalculate peer covariates\n\nGenerate PEER factors, these peer factors will be used as covariates to perform multiple linear regression for each gene in our gene expression matrix and then save the residuals from the regression as our new expressions for further analysis where needed.\nEnsure you have peer tool installed on your machine. There is a description of how to download the PEER tool here.\nUse the PEER tool to generate PEER factors from our transformed gene expression file (./output/gene_exp.csv). According to GTEx protocol, If the number of samples is greater than or equal to 350, we use 60 PEER factors. If the number of samples is between 250 and 350, we use 45. Between 150 and 250, we use 30, and less than 150 we use 15. For this study, the number of samples is 463 so we will use 60 PEER factors.\npeertool -f './output/gene_exp.csv' -n 60 --has_header -o ./output/peer_out\nNote: this takes a long time to run the peers\nOnce completed read the output of peertool into r to add column names. This will form our covariates matrix\npeer_factors = read.csv(file = \"./output/peer_out/X.csv\", header = FALSE)\n\n#Set the column names for the PEER factors (covariates) as the subject IDs\ncolnames(peer_factors) = rownames(gene_exp_transpose)\n\n# write out a covariates matrix\nwrite.table(peer_factors, file = \"./output/covariates.txt\", sep = \"\\t\",\n            row.names = TRUE)\n\nRegress out the covariates You may need the residuals for other analysis e.g estimating h2. We are going to regress out the covariates and save the residuals for any other analysis\n\n## Make a copy of the transposed gene expression dataframe so that we can replace the values with the residuals of the multiple linear regressions.\nexpression = gene_exp_transpose\n\n# This loops through all the columns of the transposed gene expression which correspond to each gene,\nfor each gene it runs linear regression on the PEER factor covariates. Then it sets the residuals to the new expression for that gene.\n\nfor (i in 1:length(colnames(gene_exp_transpose))) {\n    fit = lm(gene_exp_transpose[,i] ~ t(as.matrix(peer_factors)))\n    expression[,i] <- fit$residuals\n  }\n\n# Write out the residual expression file\nwrite.table(expression, file = \"./output/residuals_expression.txt\", sep = \"\\t\",\n            row.names = TRUE)\n\n\n\n\nCheck all processed data is available in the output dir To train the model you need the files you generated from the steps above;\n\ngene expression file: transformed_expression.txt\ncovariates file: covariates.txt\ngene annotation file: gene_annot.parsed.txt\nsnp_annotation file: snp_annot.chr*.txt for chr 1-22\ngenotype file: genotype.chr*.txt for chr 1-22\n\n\nConfirm you have all these files in your output directory.\n\nSet up the execution script We will process each chromosmome individually using this rscript .code/gtex_tiss_chrom_training.R. Edit the file to fit the paths to the constant files and the ones which will be changed for each chromosome i.e snp_annotation and genotype files.\nCreate the required directories\n\nmkdir -p ./summary ./covariances ./weights\n\nActual training Once everything is set you can process a chromosome at a time by executing this code\n\nRscript ./code/gtex_tiss_chrom_training.R 1 to train the model for all genes in chromosome 1\nTo run all the chromosomes you can use a for loop like this\nfor i in {1..22}\ndo\n  Rscript ./code/gtex_tiss_chrom_training.R ${i}\ndone\nThis will take sometime to run and it would be nice to parallelize the step\n\n\n\nMake dir for the database\nmkdir -p ../dbs\nCreate database once we have our model summaries we combine them into a single file then create a database Using R run this chuck of code below\n\"%&%\" <- function(a,b) paste(a,b, sep='')\ndriver <- dbDriver('SQLite')\nmodel_summaries <- read.table('./summary/Model_training_chr1_model_summaries.txt',header = T, stringsAsFactors = F)\ntiss_summary <- read.table('./summary/Model_training_chr1_summary.txt', header = T, stringsAsFactors = F)\n  \nn_samples <- tiss_summary$n_samples\n  \nfor (i in 2:22) {\n  model_summaries <- rbind(model_summaries,\n                            read.table('../summary/Model_training_chr'%&%as.character(i) %&% '_model_summaries.txt', header = T, stringsAsFactors = F))\n  tiss_summary <- rbind(tiss_summary,\n                             read.table('../summary/Model_training_chr' %&% as.character(i) %&% '_summary.txt', header = T, stringsAsFactors = F))\n  \n}\n  \nmodel_summaries <- rename(model_summaries, gene = gene_id)\n\n# Create a database connection\nconn <- dbConnect(drv = driver, './dbs/gtex_v7_models.db')\ndbWriteTable(conn, 'model_summaries', model_summaries, overwrite = TRUE)\ndbExecute(conn, \"CREATE INDEX gene_model_summary ON model_summaries (gene)\")\n\n# Weights Table -----\nweights <- read.table('../weights/Model_training_chr1_weights.txt', header = T,stringsAsFactors = F)\nfor (i in 2:22) {\n  weights <- rbind(weights,\n              read.table('../weights/Model_training_chr' %&% as.character(i) %&% '_weights.txt', header = T, stringsAsFactors = F))\n  \n}\n  \nweights <- rename(weights, gene = gene_id)\ndbWriteTable(conn, 'weights', weights, overwrite = TRUE)\ndbExecute(conn, \"CREATE INDEX weights_rsid ON weights (rsid)\")\ndbExecute(conn, \"CREATE INDEX weights_gene ON weights (gene)\")\ndbExecute(conn, \"CREATE INDEX weights_rsid_gene ON weights (rsid, gene)\")\n\n# Sample_info Table ----\nsample_info <- data.frame(n_samples = n_samples, population = 'EUR') # Provide the population info\ndbWriteTable(conn, 'sample_info', sample_info, overwrite = TRUE)\n  \n# Construction Table ----\nconstruction <- tiss_summary %>%\n                    select(chrom, cv_seed) %>%\n                    rename(chromosome = chrom)\n\ndbWriteTable(conn, 'construction', construction, overwrite = TRUE)\ndbDisconnect(conn)\n\n\n\nFilter the database to select significant models using R\nunfiltered_db <- './dbs/gtex_v7_models.db'\nfiltered_db <- './dbs/gtex_v7_models_filtered_signif.db'\ndriver <- dbDriver(\"SQLite\")\nin_conn <- dbConnect(driver, unfiltered_db)\nout_conn <- dbConnect(driver, filtered_db)\nmodel_summaries <- dbGetQuery(in_conn, 'select * from model_summaries where zscore_pval < 0.05 and rho_avg > 0.1')\nmodel_summaries <- model_summaries %>% \n                    rename(pred.perf.R2 = rho_avg_squared, genename = gene_name, pred.perf.pval = zscore_pval, n.snps.in.model = n_snps_in_model)\nmodel_summaries$pred.perf.qval <- NA\ndbWriteTable(out_conn, 'extra', model_summaries, overwrite = TRUE)\nconstruction <- dbGetQuery(in_conn, 'select * from construction')\ndbWriteTable(out_conn, 'construction', construction, overwrite = TRUE)\nsample_info <- dbGetQuery(in_conn, 'select * from sample_info')\ndbWriteTable(out_conn, 'sample_info', sample_info, overwrite = TRUE)\nweights <- dbGetQuery(in_conn, 'select * from weights')\nweights <- weights %>% filter(gene %in% model_summaries$gene) %>% rename(eff_allele = alt, ref_allele = ref, weight = beta)\ndbWriteTable(out_conn, 'weights', weights, overwrite = TRUE)\ndbExecute(out_conn, \"CREATE INDEX weights_rsid ON weights (rsid)\")\ndbExecute(out_conn, \"CREATE INDEX weights_gene ON weights (gene)\")\ndbExecute(out_conn, \"CREATE INDEX weights_rsid_gene ON weights (rsid, gene)\")\ndbExecute(out_conn, \"CREATE INDEX gene_model_summary ON extra (gene)\")\ndbDisconnect(in_conn)\ndbDisconnect(out_conn)\nYour database is ready to use.\nThis pipeline has also been implemented in nextflow pipeline here"
  },
  {
    "objectID": "post/2023-03-28-multiple-testing/index.html",
    "href": "post/2023-03-28-multiple-testing/index.html",
    "title": "Multiple Testing Vignette",
    "section": "",
    "text": "build intuition about p-values when multiple testing is performed via simulations\nrecognize the need for multiple testing correction\npresent methods to correct for multiple testing\n\nBonferroni correction\nFDR (false discovery rate)"
  },
  {
    "objectID": "post/2023-03-28-multiple-testing/index.html#why-do-we-need-multiple-testing-correction",
    "href": "post/2023-03-28-multiple-testing/index.html#why-do-we-need-multiple-testing-correction",
    "title": "Multiple Testing Vignette",
    "section": "Why do we need multiple testing correction",
    "text": "Why do we need multiple testing correction\n\n\n\nxkcd image for significance"
  },
  {
    "objectID": "post/2023-03-28-multiple-testing/index.html#what-do-p-values-look-like-under-the-null-and-alternative",
    "href": "post/2023-03-28-multiple-testing/index.html#what-do-p-values-look-like-under-the-null-and-alternative",
    "title": "Multiple Testing Vignette",
    "section": "What do p-values look like under the null and alternative?",
    "text": "What do p-values look like under the null and alternative?\n\nSimulate vectors X, Yalt=\\(X\\cdot \\beta + \\epsilon\\) and Ynull independent of X\nWe start defining some parameters for the simulations. The need for these will become obvious later.\n\n## set seed to make simulations reproducible\n## set.seed(20210108)\n\n## let's start with some parameter definitions\nnsamp = 100\nbeta = 2\nh2 = 0.1\nsig2X = h2\nsig2epsi = (1 - sig2X) * beta^2\nsigX = sqrt(sig2X)\nsigepsi = sqrt(sig2epsi)\n\nNext, we simulate a vectors X and \\(\\epsilon\\), and Ynull, all normally distributed\n\nX = rnorm(nsamp,mean=0, sd= sigX)\nepsi = rnorm(nsamp,mean=0, sd=sigepsi)\n## generate Ynull (X has no effect on Ynull)\nYnull = rnorm(nsamp, mean=0, sd=beta)\n\nCalculate Yalt = X * beta + epsi\n\nYalt = X * beta + epsi\n\nVisualize Yalt vs X\n\nplot(X, Yalt, main=\"Yalt vs X\"); grid()\n\n\n\n\nVisualize Ynull vs X\n\nplot(X, Ynull, main=\"Ynull vs X\");grid()\n\n\n\n\nTest association between Ynull and X\n\nsummary(lm(Ynull ~ X))\n\n\nCall:\nlm(formula = Ynull ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3057 -1.3026  0.0433  1.6954  7.7384 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  -0.4484     0.2082  -2.153   0.0337 *\nX             0.9107     0.6959   1.309   0.1937  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.072 on 98 degrees of freedom\nMultiple R-squared:  0.01717,   Adjusted R-squared:  0.007145 \nF-statistic: 1.712 on 1 and 98 DF,  p-value: 0.1937\n\n\n\nwhat’s the p-value of the association?\nis the p-value significant at 5% significance leve?\n\nNext, test the association between Yalt and X\n\nsummary(lm(Yalt ~ X))\n\n\nCall:\nlm(formula = Yalt ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1010 -1.1054  0.3046  1.2652  4.4881 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   0.0170     0.1853   0.092  0.92708   \nX             1.6342     0.6191   2.640  0.00966 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.843 on 98 degrees of freedom\nMultiple R-squared:  0.06637,   Adjusted R-squared:  0.05685 \nF-statistic: 6.967 on 1 and 98 DF,  p-value: 0.00966\n\n\n\nwhat’s the p-value of the association?\nis the p-value significant at 5% significance level?"
  },
  {
    "objectID": "post/2023-03-28-multiple-testing/index.html#calculate-the-empirical-distribution-of-p-values",
    "href": "post/2023-03-28-multiple-testing/index.html#calculate-the-empirical-distribution-of-p-values",
    "title": "Multiple Testing Vignette",
    "section": "Calculate the empirical distribution of p-values",
    "text": "Calculate the empirical distribution of p-values\nTo calculate the empirical distribution of p-values under the null and alternatives we will simulate X, Yalt, Ynull for 10,000 times.\n\nDefine a convenience function fastlm, will do linear regression much faster\nWe want to run 10,000 times this same regression, so here we define a function fastlm that will get us the p-values and regression coefficients.\n\nfastlm = function(xx,yy)\n{\n  ## compute betahat (regression coef) and pvalue with Ftest\n  ## for now it does not take covariates\n  \n  df1 = 2\n  df0 = 1\n  ind = !is.na(xx) & !is.na(yy)\n  xx = xx[ind]\n  yy = yy[ind]\n  n = sum(ind)\n  xbar = mean(xx)\n  ybar = mean(yy)\n  xx = xx - xbar\n  yy = yy - ybar\n  \n  SXX = sum( xx^2 )\n  SYY = sum( yy^2 )\n  SXY = sum( xx * yy )\n  \n  betahat = SXY / SXX\n  \n  RSS1 = sum( ( yy - xx * betahat )^2 )\n  RSS0 = SYY\n  \n  fstat = ( ( RSS0 - RSS1 ) / ( df1 - df0 ) )  / ( RSS1 / ( n - df1 ) )\n  pval = 1 - pf(fstat, df1 = ( df1 - df0 ), df2 = ( n - df1 ))\n  res = list(betahat = betahat, pval = pval)\n  \n  return(res)\n}\n\n\n\nSimulate vectors X, Ynull, Yalt 10,000 times\n\nnsim = 10000\n## simulate normally distributed X and epsi\nXmat = matrix(rnorm(nsim * nsamp,mean=0, sd= sigX), nsamp, nsim)\nepsimat = matrix(rnorm(nsim * nsamp,mean=0, sd=sigepsi), nsamp, nsim)\n\n## generate Yalt (X has an effect on Yalt)\nYmat_alt = Xmat * beta + epsimat\n\n## generate Ynull (X has no effect on Ynull)\nYmat_null = matrix(rnorm(nsim * nsamp, mean=0, sd=beta), nsamp, nsim)\n\n## let's look at the dimensions of the simulated matrices\n\ndim(Ymat_null)\n\n[1]   100 10000\n\ndim(Ymat_alt)\n\n[1]   100 10000\n\n\nNow we have 10000 independent simulations of X, Ynull, and Yalt\n\n## give them names so that we can refer to them later more easily\ncolnames(Ymat_null) = paste0(\"c\",1:ncol(Ymat_null))\ncolnames(Ymat_alt) = colnames(Ymat_null)\n\n\nTo calculate p-values under the null run 10,000 linear regressions using X and Ynull\n\n\npvec_null = rep(NA,nsim)\nbvec_null = rep(NA,nsim)\n\nfor(ss in 1:nsim)\n{\n  fit = fastlm(Xmat[,ss], Ymat_null[,ss])\n  pvec_null[ss] = fit$pval  \n  bvec_null[ss] = fit$betahat\n}\n\nsummary(pvec_null)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00005 0.25479 0.50579 0.50281 0.75277 0.99999 \n\nhist(pvec_null,xlab=\"p-value\",main=\"Histogram of p-values under Null\")\n\n\n\n\n\nhow many simulations under the null yield p-value below 0.05? What percentage is that?\n\n\nsum(pvec_null<0.05)\n\n[1] 480\n\nmean(pvec_null<0.05)\n\n[1] 0.048\n\n\n\nhow many simulations under the null yield p-value < 0.20?\nwhat do you think the proportion of simulations with p-values < \\(\\alpha\\) (\\(\\alpha\\) between 0 and 1) will be roughly?\nWhy do we need to use more stringent significance level when we test many times?"
  },
  {
    "objectID": "post/2023-03-28-multiple-testing/index.html#bonferroni-correction",
    "href": "post/2023-03-28-multiple-testing/index.html#bonferroni-correction",
    "title": "Multiple Testing Vignette",
    "section": "Bonferroni correction",
    "text": "Bonferroni correction\nUse as the new threshold the original one divided by the number of tests. So typically\n\\[\\frac{0.05}{\\text{total number of tests}}\\]\n\nwhat’s the Bonferroni threshold for significance in this simulation?\nhow many did we find?\n\n\nBF_thres = 0.05/nsim\n## Bonferroni significance threshold\nprint(BF_thres) \n\n[1] 5e-06\n\n## number of Bonferroni significant associations\nsum(pvec_null<BF_thres)\n\n[1] 0\n\n## proportion of Bonferroni significant associations\nmean(pvec_null<BF_thres)\n\n[1] 0"
  },
  {
    "objectID": "post/2023-03-28-multiple-testing/index.html#mix-of-ynull-and-yalt",
    "href": "post/2023-03-28-multiple-testing/index.html#mix-of-ynull-and-yalt",
    "title": "Multiple Testing Vignette",
    "section": "Mix of Ynull and Yalt",
    "text": "Mix of Ynull and Yalt\nLet’s see what happens when we add a bunch of true associations in the matrix of null associations\n\nprop_alt=0.20 ## define proportion of alternative Ys in the mixture\nselectvec = rbinom(nsim,1,prop_alt)\nnames(selectvec) = colnames(Ymat_alt)\nselectvec[1:10]\n\n c1  c2  c3  c4  c5  c6  c7  c8  c9 c10 \n  0   1   0   0   0   0   0   0   0   0 \n\nYmat_mix = sweep(Ymat_alt,2,selectvec,FUN='*') + sweep(Ymat_null,2,1-selectvec,FUN='*')\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun linear regression for all 10,000 phenotypes in the mix of true and false associations, Ymat_mix\n\npvec_mix = rep(NA,nsim)\nbvec_mix = rep(NA,nsim)\nfor(ss in 1:nsim)\n{\n  fit = fastlm(Xmat[,ss], Ymat_mix[,ss])\n  pvec_mix[ss] = fit$pval  \n  bvec_mix[ss] = fit$betahat\n}\nsummary(pvec_mix)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.08682 0.38936 0.40822 0.69073 0.99999 \n\nhist(pvec_mix,xlab=\"p-value\",main=\"Histogram of p-values under mixture of null and alt\")\n\n\n\nm_signif = sum(pvec_mix < 0.05) ## observed number of significant associations\nm_expected = 0.05*nsim ## expected number of significant associations under the worst case scenario, where all features belong to the null \nm_signif\n\n[1] 2146\n\nm_expected\n\n[1] 500\n\n\nUnder the null, we were expecting 500 significant columns by chance but got 2146\nQ: how can we estimate the proportion of true positives?\nWe got 1646 extra columns, so it’s reasonable to expect that the extra significant results come from the alternative distribution (Yalt). So \\[\\frac{\\text{observed number of significant} - \\text{expected number of significant}}{\\text{observed number of significant}}\\] should be a good estimate of the true discovery rate. False discovery rate is defined as 1 - the true discovery rate.\n\nthres = 0.05\nFDR = sum((pvec_mix<thres & selectvec==0)) / sum(pvec_mix<thres)\n## proportion of null columns that are significant among all significant\nFDR\n\n[1] 0.1766076\n\n\nIf we use a p-value threshold of 0.05, 82.34 percent of the signficant columns are true discoveries.  In this case, we know which ones are true or false associations because we decided using the selectvec vectors which simulated Y would be a function of X or unrelated to X.\n\nwhat’s the proportion of false discoveries if we use a significance level of 0.01\nwhat’s the proportion of false discoveries if we use Bonferroni correction as the significance level?\nWhat’s the proportion of missed signals, proportion of true associations that have p-values greater than the Bonferroni threshold?"
  },
  {
    "objectID": "post/2023-03-28-multiple-testing/index.html#common-approaches-to-control-type-i-errors",
    "href": "post/2023-03-28-multiple-testing/index.html#common-approaches-to-control-type-i-errors",
    "title": "Multiple Testing Vignette",
    "section": "Common approaches to control type I errors",
    "text": "Common approaches to control type I errors\nAssuming we are testing \\(m\\) hypothesis, let’s define the following terms for the different errors.\n\n\n\n\n\n\n\n\n\n\nCalled Significant\nCalled not significant\nTotal\n\n\n\n\nNull true\n\\(F\\)\n\\(m_0 - F\\)\n\\(m_0\\)\n\n\nAlt true\n\\(T\\)\n\\(m_1 - T\\)\n\\(m_1\\)\n\n\nTotal\n\\(S\\)\n\\(m - S\\)\n\\(m\\)\n\n\n\n\nBonferroni correction assures that the FWER (Familywise error rate) \\(P(F \\ge 1)\\) is below the acceptable type I error, typically 0.05. \\[P(F \\ge 1) < \\alpha. \\] We achieve that by requiring that for each test \\[p<\\alpha/\\text{# tests}.\\] This can be too stringent and lead to miss real signals.\npFDR (positive false discovery rate) \\[E\\left(\\frac{F}{S} \\rvert S>0\\right)\\]\nqvalue is the minimum false discovery rate attainable when the feature (SNP) is called significant"
  },
  {
    "objectID": "post/2023-03-28-multiple-testing/index.html#table-of-null-or-alternative-vs.-significant-or-not-significant",
    "href": "post/2023-03-28-multiple-testing/index.html#table-of-null-or-alternative-vs.-significant-or-not-significant",
    "title": "Multiple Testing Vignette",
    "section": "Table of null or alternative vs. significant or not significant",
    "text": "Table of null or alternative vs. significant or not significant\n\ncount_table = t(table(pvec_mix>0.05, selectvec))\ncolnames(count_table) = c(\"Called significant\", \"Called not significant\")\nrownames(count_table) = c(\"Null true\", \"Alt true\")\nknitr::kable(count_table)\n\n\n\n\n\nCalled significant\nCalled not significant\n\n\n\n\nNull true\n379\n7646\n\n\nAlt true\n1767\n208\n\n\n\n\n\nLet’s calculate the qvalue"
  },
  {
    "objectID": "post/2023-03-28-multiple-testing/index.html#use-qvalue-package-to-calculate-fdr-and",
    "href": "post/2023-03-28-multiple-testing/index.html#use-qvalue-package-to-calculate-fdr-and",
    "title": "Multiple Testing Vignette",
    "section": "Use qvalue package to calculate FDR and",
    "text": "Use qvalue package to calculate FDR and\nLet’s check whether small qvalues correspond to true associations (i.e. the phenotype was generated under the alternative distribution)\n\n## install qvalue if not available.\nif(F) ## I set it to F now because I already installed the qvalue package\n{if (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"qvalue\")\n}\n\n## calculate qvalue using the qvalue function, which returns a list of values, we select the qvalue vector, which assigns the false discovery rate if the threshold for significance was the p-value of the same simulation vector\n\nqres_mix = qvalue::qvalue(pvec_mix)\nqvec_mix = qres_mix$qvalue\n\nqres_null = qvalue::qvalue(pvec_null)\nqvec_null = qres_null$qvalue\n\nboxplot(qvec_mix~selectvec)\n\n\n\n##plot(qvec_mix,col=selectvec*2 + 1, pch=selectvec + 1, lwd=selectvec*2 + 1) \n## using selectvec*2 + 1 as a quick way to get the vector to be 1 and 3 (1 is black 3 is green) instead of 1 and 2 (2 is read and can be difficult to read for color blind people)\n\nPlot sorted qvalues and color by the selectvec status (true association status)\n\nind=order(qvec_mix,decreasing=FALSE)\nplot(sort(qvec_mix),col=selectvec[ind]*2 + 1, pch=selectvec[ind] + 1, lwd=selectvec[ind]*2 + 1) \n\n\n\nsummary(qvec_mix) \n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0000006 0.2808014 0.6296951 0.5054476 0.7448208 0.8087707 \n\n\n\ndo small qvalues tend to be true?\ninterpret the figure\n\n\n## distribution of qvalues and pvalues by causal status\nboxplot(pvec_mix ~ selectvec, main='pvalue by causal status; 1=alt, 0=null')\n\n\n\nboxplot(qvec_mix ~ selectvec, main='qvalue by causal status; 1=alt, 0=null')\n\n\n\n\n\ninterpret these figures"
  },
  {
    "objectID": "post/2023-03-28-multiple-testing/index.html#how-do-qvalues-and-pvalues-relate-to-each-other",
    "href": "post/2023-03-28-multiple-testing/index.html#how-do-qvalues-and-pvalues-relate-to-each-other",
    "title": "Multiple Testing Vignette",
    "section": "How do qvalues and pvalues relate to each other?",
    "text": "How do qvalues and pvalues relate to each other?\n\nplot(pvec_null,qvec_null,main='qvalue vs pvalue for null')\n\n\n\nplot(pvec_mix,qvec_mix,main='qvalue vs pvalue for mixture of null and alt')\n\n\n\n\n\nq-values are monotonic functions of p-values\n\n\nwhat’s the smallest qvalue when all simulations are from the null?\nwhat’s the smallest qvalue when all simulations are from the mixture?"
  },
  {
    "objectID": "post/2023-03-28-multiple-testing/index.html#pi0-and-pi1",
    "href": "post/2023-03-28-multiple-testing/index.html#pi0-and-pi1",
    "title": "Multiple Testing Vignette",
    "section": "pi0 and pi1",
    "text": "pi0 and pi1\npi0 is the proportion of null hypothesis which can be estimated using the qvalue package 1 - pi1 is the proportion of true positive associations. This is a useful parameter as we will see in later classes.\n\nqres_null$pi0\n\n[1] 1\n\nqres_mix$pi0\n\n[1] 0.8087771\n\n\n\nhow many true positive proportion did you expect given the simulations we performed?"
  },
  {
    "objectID": "post/2023-03-28-multiple-testing/index.html#references",
    "href": "post/2023-03-28-multiple-testing/index.html#references",
    "title": "Multiple Testing Vignette",
    "section": "References",
    "text": "References\nStorey, John D., and Robert Tibshirani. 2003. “Statistical Significance for Genomewide Studies.” Proceedings of the National Academy of Sciences 100 (16): 9440–45."
  },
  {
    "objectID": "post/2020-07-30-downloading-data-from-biobank-japan/index.html",
    "href": "post/2020-07-30-downloading-data-from-biobank-japan/index.html",
    "title": "Downloading Data from Biobank Japan",
    "section": "",
    "text": "The same info can be found at this page, https://www.ddbj.nig.ac.jp/jga/download-e.html, but the steps may be slightly different because we applied for Biobank Japan data before they integrated with the D-way system.\n\nRegister a user account:\n\nA D-way account should have been created for you, with your username and password sent in an email. If not, create an account here: https://ddbj.nig.ac.jp/D-way/contents/general/reserve_account_page\nAdd UChicago as your organization: Log in with D-way account, https://ddbj.nig.ac.jp/D-way/. Click the account tab at the top right corner, and at the bottom, type “The University of Chicago” in Center Full Name. Once selected, center name autofills with “U_CHICAGO”.\nRegister your public key: You may need to click update and refresh for the Public Key box to appear at the bottom. You will need to copy your public key to an unhidden folder, cp ~/.ssh/id_rsa.pub ~/Desktop, before selecting for upload. If you do not find a file named id_rsa.pub in .ssh, create a public key with ssh-keygen -t rsa. Note: The keypair you register will be used for downloading and decrypting data, and cannot be changed. Choose a public key generated by a lab computer (Mac or Linux) or the machine where the data will be stored.\n\n\n\nDownload Data:\n\nConnect to JGA server: sftp -i id_rsa -P 443 <D-way username>@jga-gw.ddbj.nig.ac.jp\ncd controlled-access/download/jga/\nDownload genotypes: get -r J-DU000138/JGAS000114/JGAD000123\nDownload a phenotype: get -r J-DU000138/JGAS000114/JGAD000*** (each folder contains encrypted individual data for one phenotype)\nDownload decryption tools: get -r J-DU000138/tools\n\nI wrote a batch script and ran the following lines to download all the phenotypes:\nssh-add\ncd BBJ\nmkdir JGAS000114\ncd JGAS000114\nsftp -i ~/.ssh/id_rsa -P 443 -b get_phenotypes.txt scmi@jga-gw.ddbj.nig.ac.jp\n\n\nDecrypt Data\n\ncd BBJ\nunzip tools/J-DU000138.tool.zip\nchmod 754 J-DU000138.decrypt.sh\nchmod 754 JGAS000114/**/**/*.decrypt.sh\nDecrypt: ./J-DU000138.decrypt.sh -k ~/.ssh/id_rsa -p <private key password>\n\nIf you find the following error message:\n\n\n\n\n\nError Message\n\n\nWhen I ran into this issue, the problem was that the tool folder provided was not encrypted with my public. If you email the NBDC contact with your public key, the updated tools folder should work."
  },
  {
    "objectID": "post/2021-07-20-error-0-snps-used/index.html",
    "href": "post/2021-07-20-error-0-snps-used/index.html",
    "title": "Error: 0 % SNPs used",
    "section": "",
    "text": "Mismatch between the model SNP ids and geneotype/gwas SNP ids, e.g using model rsids to match with genotype variant_ids\nWhen having different genome builds use chr{}{}{}_{}_b38” for the –on_the_fly_mapping argument. This parameter specifies a format string to be used when building a variant id from its (liftover coordinates) and alleles (genotype). Then these ids will be matched to the ids in the mashr/elastic net model which are all hg38-based.\nYou can use a text variant mapping instead of on_the_fly_mapping as below\n\nThe argument you would need to use is: -variant_mapping /path/to/tab-separated-file.txt KEYNAME VALUENAME where:\n\n\n\npath refers to a simple tabular file\n\n\n\n\nKEYNAME is the name of the column with your input variants\n\n\n\n\nVALUENAME is the name of the column with the mapped variant as existing in a model (NA would cause the model to be ignored)\n\n\nWhen you are using mashr models they don’t use RSIDs so you need to let S-PrediXcan knows this is the case with the additional command line arguments:\n\n-keep_non_rsid —additional_output –model_db_snp_key varID"
  },
  {
    "objectID": "post/2020-12-01-how-to-interpret-a-p-value-of-0/index.html",
    "href": "post/2020-12-01-how-to-interpret-a-p-value-of-0/index.html",
    "title": "How to interpret a p-value of 0",
    "section": "",
    "text": "A p-value of zero should be interpreted as an extremely small positive value.\nS-PrediXcan or PrediXcan will provide the zscore as well as the p-value. You can calculate the p-value corresponding to the Zscore using the formula below. For example a Zscore of 30 will give you a (natural) log p-value of -453.6280968\n\n\nShow the code\nZscore = 30\npnorm(-abs(Zscore) , log.p = TRUE) + log(2)  \n\n\n[1] -453.6281\n\n\nMultiXcan doesn’t output the test statistics used for the p-value calculation. But you can get a sense of how extreme the p-values can be by looking at the largest Z-scores in absolute value in the z_min and z_max columns. The (natural) log of the p-value corresponding to your results is shown below.\n\n\nShow the code\nsuppressMessages(library(tidyverse))\ntempo = read_tsv(\"~/Downloads/multixcan_output.txt\")\npnorm(- max(abs(c(tempo$z_max,tempo$z_min ) ), na.rm = TRUE), log.p = TRUE) + log(2)"
  },
  {
    "objectID": "post/2021-06-14-subsetting-hapmap3-snps/index.html",
    "href": "post/2021-06-14-subsetting-hapmap3-snps/index.html",
    "title": "Subsetting HapMap3 SNPs",
    "section": "",
    "text": "About\nThis is the readme of the codes here.\nThis module extract HapMap 3 SNPs for downstream use. Download data from https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/hapmap3_r3/plink_format/\nThe workflow is based on snakemake and it contains the following steps: 1. Extract SNP information from PED file (rule get_bim_file) 2. Extract individuals from the desired population (rule extract_individuals) 3. Compute MAF among the extracted individuals from step 2 (rule compute_maf) 4. Filter the SNPs based on the MAF calculated in step 3 (rule filter_by_maf) 5. Filter out ambiguous SNPs. Also, the non-SNV SNPs or SNPs without rsID or SNPs outside chr1-chr22 are filtered out (rule filter_by_ambiguity) 6. Annotate the extracted SNPs (from step 5) with genomic position and MAF. If specified, also do liftover (rule gen_lookup_table)\nIf one wants to do liftover, then chain_file should be specified. The target_build should always be specified and it will be added as an column to the final output. Please make sure that it is correct. For HapMap 3 SNPs considered here, the original build is b36 and if using liftover, one should assign chain_file with the final build after liftover. For instance, if use chain file hg18ToHg19.over.chain.gz, one should set chain_file to hg19 or b37. Note that liftover will result loss of SNPs since some SNPs may fail to be liftover. For instance, in the example run: * MAF = 0.01: For b36 -> b37, the number of SNPs goes from 1108410 to 1108189 * MAF = 0.05: For b36 -> b37, the number of SNPs goes from 1007394 to 1007190\n\n\nDependency\n\npython3: see imlabtools conda env at here\nplink1.9\nsnakemake\nClone the repo: https://github.com/liangyy/misc-tools\n\n\n\nExample run on CEU\nAlso, see run_example/ for the full details.\n# build b36\n## maf = 0.01\nsnakemake -s hapmap.snmk --configfile config.hapmap3_eur.yaml -p --config maf=0.01 target_build=b36\n## maf = 0.05\nsnakemake -s hapmap.snmk --configfile config.hapmap3_eur.yaml -p --config maf=0.05 target_build=b36\n\n# build b37\n## maf = 0.01\nsnakemake -s hapmap.snmk --configfile config.hapmap3_eur.yaml -p --config maf=0.01 target_build=b37 chain_file=[path-to/hg18ToHg19.over.chain.gz]\n## maf = 0.05\nsnakemake -s hapmap.snmk --configfile config.hapmap3_eur.yaml -p --config maf=0.05 target_build=b37 chain_file=[path-to/hg18ToHg19.over.chain.gz]\nThe resulting files of the test run are here: * maf = 0.01, build = b36 * maf = 0.05, build = b36 * maf = 0.01, build = b37 * maf = 0.05, build = b37\n\n\nhow to download the hapmap files\nwget https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/hapmap3_r3/plink_format/00README.txt wget https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/hapmap3_r3/plink_format/hapmap3_r3_b36_fwd.consensus.qc.poly.map.gz wget https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/hapmap3_r3/plink_format/hapmap3_r3_b36_fwd.consensus.qc.poly.ped.gz wget https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/hapmap3_r3/plink_format/hapmap3_r3_b36_fwd.qc.poly.tar.gz\n** see copy of the files in https://uchicago.box.com/s/tccjhavco9cifwddyy2bmd0qtiwk0mp2"
  },
  {
    "objectID": "post/2020-07-30-querying-bigquery/index.html",
    "href": "post/2020-07-30-querying-bigquery/index.html",
    "title": "Querying BigQuery",
    "section": "",
    "text": "find examples here https://hakyimlab.github.io/bigquery-covid19/query-phenomexcan.html\nworflowr code https://github.com/hakyimlab/bigquery-covid19"
  },
  {
    "objectID": "post/2023-03-23-jane-austen-corpus/index.html",
    "href": "post/2023-03-23-jane-austen-corpus/index.html",
    "title": "Jane Austen Corpus",
    "section": "",
    "text": "Show the code\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\n##PRE=\"/Users/margaretperry/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data \"\n##PRE=\"/Users/temi/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data\"\n## COPY THE DATE AND SLUG fields FROM THE HEADER\nSLUG=\"jane-austen-corpus\" ## copy the slug from the header\nbDATE='2023-03238' ## copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n\n## move data to DATA\n#tempodata=(\"~/Downloads/tempo/gwas_catalog_v1.0.2-associations_e105_r2022-04-07.tsv\")\n#system(glue::glue(\"cp {tempodata} {DATA}/\"))\n#system(glue(\"open {DATA}\")) ## this will open the folder \n\n\n\nget jane austen corpus\n\n\n\nShow the code\n##install.packages(\"janeaustenr\")"
  },
  {
    "objectID": "post/2020-07-05-hands-on-training-r/index.html",
    "href": "post/2020-07-05-hands-on-training-r/index.html",
    "title": "Hands-On Training: R",
    "section": "",
    "text": "Swirl is a great and easy way to get you started with R. Install and open it by clicking the green arrow on the right.\nInstall Swirl Package:\n\n\nShow the code\ninstall.packages(\"swirl\")\nlibrary(\"swirl\")\nswirl()\n\n\nOnce you enter your username, it prompts you to choose a course to install. Choose 1 for “R Programming” to learn the basics. Then, enter 1 again and it will take you to a menu of courses you can choose from."
  },
  {
    "objectID": "post/2020-07-05-hands-on-training-r/index.html#quiz",
    "href": "post/2020-07-05-hands-on-training-r/index.html#quiz",
    "title": "Hands-On Training: R",
    "section": "Quiz",
    "text": "Quiz"
  },
  {
    "objectID": "post/2022-11-30-how-to-prepare-effective-presentation-slides/index.html",
    "href": "post/2022-11-30-how-to-prepare-effective-presentation-slides/index.html",
    "title": "How to prepare effective presentation slides",
    "section": "",
    "text": "Citation: Naegle KM (2021) Ten simple rules for effective presentation slides. PLoS Comput Biol 17(12): e1009554. https://doi.org/10.1371/journal.pcbi.1009554\nPublished: December 2, 2021\nCopyright: © 2021 Kristen M. Naegle. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "post/2022-11-30-how-to-prepare-effective-presentation-slides/index.html#itai-yanais-twitter-thread-on-how-to-make-presentations",
    "href": "post/2022-11-30-how-to-prepare-effective-presentation-slides/index.html#itai-yanais-twitter-thread-on-how-to-make-presentations",
    "title": "How to prepare effective presentation slides",
    "section": "Itai Yanai’s twitter thread on how to make presentations",
    "text": "Itai Yanai’s twitter thread on how to make presentations"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Miscel",
    "section": "",
    "text": "Old blog\n\nhttps://archived-web-lab-notes.hakyimlab.org"
  },
  {
    "objectID": "post/2023-06-16-qqunif-with-max-pvalue/index.html",
    "href": "post/2023-06-16-qqunif-with-max-pvalue/index.html",
    "title": "qqunif function with filtered p-values",
    "section": "",
    "text": "expected under the null is (1:nn) * maxp / (nn+1)\n\n\nShow the code\nqqunif_maxp = \n  function(p,BH=T,CI=T,mlog10_p_thres=30,maxp=1,...)\n  {\n    ## thresholded by default at 1e-30\n    p=na.omit(p)\n    nn = length(p)\n    xx =  -log10( (1:nn)*maxp / (nn+1) )\n    \n    p_thres = 10^{-mlog10_p_thres}\n    if( sum( p < p_thres) )\n    {\n      warning(paste(\"thresholding p to \",p_thres) )\n      p = pmax(p, p_thres)\n    }\n    plot( xx,  -sort(log10(p)),\n          xlab=expression(Expected~~-log[10](italic(p))),\n          ylab=expression(Observed~~-log[10](italic(p))),\n          cex.lab=1.4,mgp=c(2,1,0),\n          ... )\n    abline(0,1,col='gray')\n    if(BH)\n    {\n      abline(-log10(0.05),1, col='red',lty=1)\n      abline(-log10(0.10),1, col='orange',lty=2)\n      abline(-log10(0.25),1, col='yellow',lty=3)\n      legend('topleft', c(\"FDR = 0.05\",\"FDR = 0.10\",\"FDR = 0.25\"),\n             col=c('red','orange','yellow'),lty=1:3, cex=1)\n      abline(h=-log10(0.05/nn)) ## bonferroni\n    }\n    if(CI)\n    {\n      ## create the confidence intervals\n      c95 <- rep(0,nn)\n      c05 <- rep(0,nn)\n      ## the jth order statistic from a\n      ## uniform(0,1) sample\n      ## has a beta(j,n-j+1) distribution\n      ## (Casella & Berger, 2002,\n      ## 2nd edition, pg 230, Duxbury)\n      ## this portion was posted by anonymous on\n      ## http://gettinggeneticsdone.blogspot.com/2009/11/qq-plots-of-p-values-in-r-using-ggplot2.html\n\n      for(i in 1:nn)\n      {\n        c95[i] <- qbeta(0.95,i,nn-i+1)*maxp\n        c05[i] <- qbeta(0.05,i,nn-i+1)*maxp\n      }\n\n      lines(xx,-log10(c95),col='gray')\n      lines(xx,-log10(c05),col='gray')\n    }\n  }\n\n\n\nfind distribution of jth order statistic from a uniform(0,maxp) sample\n\n\n\nShow the code\nmaxp=0.1\nnullp = runif(1000,max=maxp)\nqqunif_maxp(nullp,maxp=maxp)"
  },
  {
    "objectID": "post/1999-01-01-iris-dataset-analysis/index.html",
    "href": "post/1999-01-01-iris-dataset-analysis/index.html",
    "title": "iris dataset analysis",
    "section": "",
    "text": "Show the code\n# Load the iris dataset\ndata(iris)\n\n# Perform Fisher's discriminant analysis\nlibrary(MASS)\nlda_model <- lda(Species ~ ., data = iris)\n\n# Print the summary of the analysis\nprint(lda_model)\n\n\nCall:\nlda(Species ~ ., data = iris)\n\nPrior probabilities of groups:\n    setosa versicolor  virginica \n 0.3333333  0.3333333  0.3333333 \n\nGroup means:\n           Sepal.Length Sepal.Width Petal.Length Petal.Width\nsetosa            5.006       3.428        1.462       0.246\nversicolor        5.936       2.770        4.260       1.326\nvirginica         6.588       2.974        5.552       2.026\n\nCoefficients of linear discriminants:\n                    LD1         LD2\nSepal.Length  0.8293776  0.02410215\nSepal.Width   1.5344731  2.16452123\nPetal.Length -2.2012117 -0.93192121\nPetal.Width  -2.8104603  2.83918785\n\nProportion of trace:\n   LD1    LD2 \n0.9912 0.0088 \n\n\nShow the code\n# Predict the species using the model\npredicted_species <- predict(lda_model, iris)$class\n\n# Compare the predicted species with the actual species\naccuracy <- mean(predicted_species == iris$Species)\ncat(\"Accuracy:\", accuracy * 100, \"%\\n\")\n\n\nAccuracy: 98 %"
  },
  {
    "objectID": "post/1999-01-01-iris-dataset-analysis/index.html#show-performance",
    "href": "post/1999-01-01-iris-dataset-analysis/index.html#show-performance",
    "title": "iris dataset analysis",
    "section": "show performance",
    "text": "show performance\n\n\nShow the code\n# Load the iris dataset\ndata(iris)\n\n# Perform Fisher's discriminant analysis\nlibrary(MASS)\nlda_model <- lda(Species ~ ., data = iris)\n\n# Predict the species using the model\npredicted_species <- predict(lda_model, iris)$class\n\n# Create a confusion matrix\nlibrary(caret)\n\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\n\nShow the code\nconfusion <- confusionMatrix(predicted_species, iris$Species)\nprint(confusion)\n\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         50          0         0\n  versicolor      0         48         1\n  virginica       0          2        49\n\nOverall Statistics\n                                          \n               Accuracy : 0.98            \n                 95% CI : (0.9427, 0.9959)\n    No Information Rate : 0.3333          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.97            \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.9600           0.9800\nSpecificity                 1.0000            0.9900           0.9800\nPos Pred Value              1.0000            0.9796           0.9608\nNeg Pred Value              1.0000            0.9802           0.9899\nPrevalence                  0.3333            0.3333           0.3333\nDetection Rate              0.3333            0.3200           0.3267\nDetection Prevalence        0.3333            0.3267           0.3400\nBalanced Accuracy           1.0000            0.9750           0.9800\n\n\nShow the code\n# Create a classification plot\nlibrary(ggplot2)\niris_predicted <- data.frame(iris, Predicted_Species = predicted_species)\nggplot(iris_predicted, aes(x = Species, fill = Predicted_Species)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"LDA Classification Plot\") +\n  scale_fill_manual(values = c(\"#E41A1C\", \"#377EB8\", \"#4DAF4A\")) +\n  theme_minimal()"
  },
  {
    "objectID": "post/1999-01-01-iris-dataset-analysis/irisnet.html",
    "href": "post/1999-01-01-iris-dataset-analysis/irisnet.html",
    "title": "Iris prediction with neural network",
    "section": "",
    "text": "Show the code\n## conda install pytorch torchvision torchaudio cudatoolkit=<version> -c pytorch\n\n## (test-env) MED-ML-4210:1999-01-01-iris-dataset-analysis haekyungim$ conda install pytorch::pytorch torchvision torchaudio -c pytorch  \n## pip install scikit-learn \n\n\n\n\nShow the code\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nShow the code\n# Load the iris dataset\niris = load_iris()\nX = iris.data\nY = iris.target\n\n\n\n\nShow the code\n# Split the data into training and testing sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n\n\n\nShow the code\n# Standardize the features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\n\n\nShow the code\n# Convert the data to PyTorch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\nY_train = torch.tensor(Y_train, dtype=torch.long)\nX_test = torch.tensor(X_test, dtype=torch.float32)\nY_test = torch.tensor(Y_test, dtype=torch.long)\n\n\n\n\nShow the code\n# Define the neural network architecture\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(4, 10)\n        self.fc2 = nn.Linear(10, 10)\n        self.fc3 = nn.Linear(10, 3)\n    \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\n\n\nShow the code\n# Create an instance of the neural network\nmodel = Net()\n\n\n\n\nShow the code\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n\n\n\nShow the code\n# Train the model\nnum_epochs = 100\nbatch_size = 16\n\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for i in range(0, X_train.size(0), batch_size):\n        inputs = X_train[i:i+batch_size]\n        labels = Y_train[i:i+batch_size]\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    if (epoch+1) % 10 == 0:\n        print(f\"Epoch {epoch+1}: Loss = {running_loss:.4f}\")\n\n\nEpoch 10: Loss = 7.9727\nEpoch 20: Loss = 5.2169\nEpoch 30: Loss = 3.1653\nEpoch 40: Loss = 2.2043\nEpoch 50: Loss = 1.5700\nEpoch 60: Loss = 1.1596\nEpoch 70: Loss = 0.9131\nEpoch 80: Loss = 0.7656\nEpoch 90: Loss = 0.6722\nEpoch 100: Loss = 0.6093\n\n\n\n\nShow the code\n# Evaluate the model on the test set\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(X_test)\n    _, predicted = torch.max(outputs, 1)\n    accuracy = torch.sum(predicted == Y_test).item() / Y_test.size(0)\n    print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n\n\nTest Accuracy: 100.00%\n\n\n\n\nShow the code\n#import torch\n#import torch.nn as nn\nimport matplotlib.pyplot as plt\n\n# Assuming you have an input tensor 'input_tensor'\ninput_tensor = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n\n# Create an instance of the 'Net' model\nmodel = Net()\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Store the intermediate layer outputs\noutputs = []\n\n# Forward pass and store intermediate outputs\ndef hook(module, input, output):\n    outputs.append(output)\n\n# Register the hook to capture intermediate outputs\nhook_handle = model.fc3.register_forward_hook(hook)\nmodel(input_tensor.unsqueeze(0))\nhook_handle.remove()\n\n# Plot the intermediate outputs\nfor i, output in enumerate(outputs):\n    print(i)\n    plt.figure()\n    plt.title(f'Layer {i+1} Output')\n    plt.bar(range(output.size(-1)), output.squeeze().detach().numpy())\n    plt.show()\n\n\n0\n\n\n\n\n\n\n\nShow the code\nenumerate(outputs)\n\n\n<enumerate at 0x28f870680>\n\n\n\n\nShow the code\n\noutputs\n\n\n[tensor([[-0.1914,  0.0311, -0.2617]], grad_fn=<AddmmBackward0>)]"
  },
  {
    "objectID": "post/2023-07-03-attenuation-bias/index.html",
    "href": "post/2023-07-03-attenuation-bias/index.html",
    "title": "Attenuation bias in PrediXcan",
    "section": "",
    "text": "Summary\n\n\n\nUncertainty in predicted expression causes attenuation bias, and reduced significance of the association. Hence significance is underestimated.\nPrediXcan seeks to test whether the genetically regulated expression levels (GReX=\\(T_g\\)) of a gene is associated with the phenotype of interest. However, in practice the the GReX is known only with some error. If the error in GReX is independent of the error in Y (\\(\\epsilon_Y\\)) and the GReX, then"
  },
  {
    "objectID": "post/2023-07-03-attenuation-bias/index.html#illustration-of-attenuation-by-simulation",
    "href": "post/2023-07-03-attenuation-bias/index.html#illustration-of-attenuation-by-simulation",
    "title": "Attenuation bias in PrediXcan",
    "section": "illustration of attenuation by simulation",
    "text": "illustration of attenuation by simulation\n\n\nShow the code\nnsim = 100\n\nbeta = 0.5\nnsam = 98\n\nepsiY = rnorm(nsam,mean=0,sd=1)\nepsiT = rnorm(nsam,mean=0,sd=1)\nTsim = rnorm(nsam)\nYsim = beta * Tsim + epsiY\nTtilde = Tsim + epsiT\n\nfit_tilde = summary(lm(Ysim ~ Ttilde))\nfit_sim = summary(lm(Ysim ~ Tsim))\n\npvec_tilde=rep(NA,nsim)\ntvec_tilde=rep(NA,nsim)\nbetavec_tilde = rep(NA,nsim)\n\npvec_hat=rep(NA,nsim)\ntvec_hat=rep(NA,nsim)\nbetavec_hat = rep(NA,nsim)\n\nfor(ss in 1:nsim)\n{\nepsiY = rnorm(nsam,mean=0,sd=1)\nepsiT = rnorm(nsam,mean=0,sd=1)\nTsim = rnorm(nsam)\nYsim = beta * Tsim + epsiY\nTtilde = Tsim + epsiT\n\ncoef_tilde = coef(summary(lm(Ysim ~ Ttilde)))\ncoef_hat = coef(summary(lm(Ysim ~ Tsim)))\n\npvec_tilde[ss] = coef_tilde[2,\"Pr(>|t|)\"]\ntvec_tilde[ss] = coef_tilde[2,\"t value\"]\nbetavec_tilde[ss] = coef_tilde[2,\"Estimate\"]\n\npvec_hat[ss] = coef_hat[2,\"Pr(>|t|)\"]\ntvec_hat[ss] = coef_hat[2,\"t value\"]\nbetavec_hat[ss] = coef_hat[2,\"Estimate\"]\n}\n\nrango=range(-log10(pvec_hat),-log10(pvec_tilde))\nplot(-log10(pvec_hat),-log10(pvec_tilde),xlim=rango,ylim=rango); abline(0,1); title(\"less significant with error in variable\")\n\n\n\n\n\nShow the code\nrango=range(tvec_hat,tvec_tilde)\nplot(tvec_hat,tvec_tilde,xlim=rango,ylim=rango); abline(0,1); title(\"t stat is underestimated; less significant with error in variable\")\n\n\n\n\n\nShow the code\nrango=range(betavec_hat,betavec_tilde)\nplot(betavec_hat,betavec_tilde,xlim=rango,ylim=rango); abline(0,1);  title(\"regression coeff is underestimated\")"
  },
  {
    "objectID": "post/2023-07-03-attenuation-bias/index.html#derivation-of-the-attenuation",
    "href": "post/2023-07-03-attenuation-bias/index.html#derivation-of-the-attenuation",
    "title": "Attenuation bias in PrediXcan",
    "section": "Derivation of the attenuation",
    "text": "Derivation of the attenuation\nBelow, I show that the estimate is attenuated (biased towards zero) and provide a link to the proof that the t statistic (estimated beta divided by its standard error when using the noisy \\(=\\tilde{T_g}\\)) is lower, i.e. less significant.\nLet us assume that the phenotype \\(Y\\) is a linear function of the genetic component of gene expression \\(T_g\\):\n\\[ Y = \\beta \\cdot T_g + \\epsilon_Y\\] And that the genetic component of gene expression has the form\n\\[ T_g = \\sum_k \\omega_k \\cdot X_k\\] In practice, we don’t have the exact value of \\(T_g\\) but a noisy proxy for it:\n\\[\\tilde{T_g} = T_g + \\epsilon_T\\]\n\n\n\n\n\n\nAssumptions\n\n\n\nThe assumption is that \\(\\epsilon_T\\), \\(T_g\\), and \\(\\epsilon_Y\\) are independent. \\(~~~\\epsilon_T \\perp\\!\\!\\!\\!\\perp T_g\\) and \\(\\epsilon_Y \\perp\\!\\!\\!\\!\\perp T_g\\) are typical regression assumptions. \\(\\epsilon_T \\perp\\!\\!\\!\\!\\perp \\epsilon_Y\\) is a reasonable assumption when the training of the expression predictors is independent of the GWAS sample, as is typical.\n\n\nSo what is the effect of using this noisy version of the genetic component of gene expression?\nWhat we want to estimate is\n\\[\\hat{\\beta} = \\frac{T_g' \\cdot Y}{T_g' \\cdot T_g}\\]\nInstead, we get\n\\[\\begin{align}\n\\tilde{\\beta} & = \\frac{\\tilde{T_g}' \\cdot Y}{\\tilde{T_g}' \\cdot \\tilde{T_g}} \\\\\n              & = \\frac{(T_g + \\epsilon_T)' \\cdot Y}{\\tilde{T_g}' \\cdot \\tilde{T_g}} \\\\\n              & =  \\frac{T_g' \\cdot Y}{T_g' \\cdot T_g} \\cdot \\frac{T_g'\\cdot T_g}{\\tilde{T_g}' \\cdot \\tilde{T_g}} ~ + ~  \\frac{\\epsilon_T' \\cdot Y}{\\tilde{T_g}' \\cdot \\tilde{T_g}} \\\\\n              & = \\hat{\\beta} \\cdot \\frac{(T_g'\\cdot T_g)}{\\tilde{T_g}' \\cdot \\tilde{T_g}} ~ + ~  \\frac{\\epsilon_T' \\cdot Y}{\\tilde{T_g}' \\cdot \\tilde{T_g}} \\\\\n              & = \\hat{\\beta} \\cdot \\frac{(T_g'\\cdot T_g)}{\\tilde{T_g}' \\cdot   \\tilde{T_g}} ~ + ~   \\frac{\\epsilon_T' \\cdot Y}{\\tilde{T_g}' \\cdot \\tilde{T_g}}\\\\\n              & \\approx \\hat{\\beta} \\cdot \\frac{(T_g'\\cdot T_g)}{\\tilde{T_g}' \\cdot   \\tilde{T_g}}\n\\end{align}\\]\nThe sample variance of \\(\\tilde{T_g}\\) is \\[\\begin{align}\n\\tilde{T_g}' \\cdot \\tilde{T_g} &= T_g' \\cdot T_g  + 2 \\cdot T_g' \\cdot \\epsilon_T + \\epsilon_T' \\cdot \\epsilon_T \\\\\n& \\approx T_g' \\cdot T_g   + \\epsilon_T' \\cdot \\epsilon_T \\\\\n& \\approx T_g' \\cdot T_g   + n \\text{ var}(\\epsilon_T) \\\\\n\\end{align}\\]\nPutting together \\(\\tilde{\\beta}\\) and \\(\\tilde{T_g}' \\cdot \\tilde{T_g}\\) equations we get\n\\[\\begin{align}\n\\tilde{\\beta}\n    & \\approx \\hat{\\beta} \\cdot \\frac{T_g'\\cdot T_g}{\\tilde{T_g}' \\cdot   \\tilde{T_g}} \\\\\n    & \\approx  \\hat{\\beta} \\cdot \\frac{T_g' \\cdot T_g}{T_g' \\cdot T_g   + \\text{var}(\\epsilon_T)}\\\\\n    & = \\hat{\\beta} \\cdot \\frac{1}{1 + \\text{var}(\\epsilon_T) / T_g' \\cdot T_g }\n\\end{align}\\]\nTherefore\n\\[\\begin{align}\n     |\\tilde{\\beta}|& < |\\hat{\\beta}| + o_p(1) ~~~~~~~~~~~~\\text{if var}(\\epsilon_T) > 0\n\\end{align}\\]\nOne can also show that the standard error of \\(\\tilde\\beta\\) is"
  },
  {
    "objectID": "post/2023-07-03-attenuation-bias/index.html#link-to-proof-that-t-statistics-is-smaller-when-using-hatt_g",
    "href": "post/2023-07-03-attenuation-bias/index.html#link-to-proof-that-t-statistics-is-smaller-when-using-hatt_g",
    "title": "Attenuation bias in PrediXcan",
    "section": "Link to proof that t statistics is smaller when using \\(\\hat{T_g}\\)",
    "text": "Link to proof that t statistics is smaller when using \\(\\hat{T_g}\\)\nSee derivation here or downloaded here in box where it is shown more rigurously using plim rather tha approx signs, that the estimated coefficient \\(\\beta\\) with error in the independent variable is underestimated and that the t-statistics is also underestimated, i.e. less significant than the association should be."
  }
]