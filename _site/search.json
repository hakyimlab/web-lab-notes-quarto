[
  {
    "objectID": "posts/2023-03-28-multiple-testing/index.html",
    "href": "posts/2023-03-28-multiple-testing/index.html",
    "title": "Multiple Testing Vignette",
    "section": "",
    "text": "build intuition about p-values when multiple testing is performed via simulations\nrecognize the need for multiple testing correction\npresent methods to correct for multiple testing\n\nBonferroni correction\nFDR (false discovery rate)"
  },
  {
    "objectID": "posts/2023-03-28-multiple-testing/index.html#why-do-we-need-multiple-testing-correction",
    "href": "posts/2023-03-28-multiple-testing/index.html#why-do-we-need-multiple-testing-correction",
    "title": "Multiple Testing Vignette",
    "section": "Why do we need multiple testing correction",
    "text": "Why do we need multiple testing correction\n\n\n\nxkcd image for significance"
  },
  {
    "objectID": "posts/2023-03-28-multiple-testing/index.html#what-do-p-values-look-like-under-the-null-and-alternative",
    "href": "posts/2023-03-28-multiple-testing/index.html#what-do-p-values-look-like-under-the-null-and-alternative",
    "title": "Multiple Testing Vignette",
    "section": "What do p-values look like under the null and alternative?",
    "text": "What do p-values look like under the null and alternative?\n\nSimulate vectors X, Yalt=\\(X\\cdot \\beta + \\epsilon\\) and Ynull independent of X\nWe start defining some parameters for the simulations. The need for these will become obvious later.\n\n## set seed to make simulations reproducible\n## set.seed(20210108)\n\n## let's start with some parameter definitions\nnsamp = 100\nbeta = 2\nh2 = 0.1\nsig2X = h2\nsig2epsi = (1 - sig2X) * beta^2\nsigX = sqrt(sig2X)\nsigepsi = sqrt(sig2epsi)\n\nNext, we simulate a vectors X and \\(\\epsilon\\), and Ynull, all normally distributed\n\nX = rnorm(nsamp,mean=0, sd= sigX)\nepsi = rnorm(nsamp,mean=0, sd=sigepsi)\n## generate Ynull (X has no effect on Ynull)\nYnull = rnorm(nsamp, mean=0, sd=beta)\n\nCalculate Yalt = X * beta + epsi\n\nYalt = X * beta + epsi\n\nVisualize Yalt vs X\n\nplot(X, Yalt, main=\"Yalt vs X\"); grid()\n\n\n\n\nVisualize Ynull vs X\n\nplot(X, Ynull, main=\"Ynull vs X\");grid()\n\n\n\n\nTest association between Ynull and X\n\nsummary(lm(Ynull ~ X))\n\n\nCall:\nlm(formula = Ynull ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8431 -1.3431 -0.2234  1.2499  5.8569 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   0.1614     0.2046   0.789    0.432\nX            -0.2765     0.6653  -0.416    0.679\n\nResidual standard error: 2.039 on 98 degrees of freedom\nMultiple R-squared:  0.00176,   Adjusted R-squared:  -0.008426 \nF-statistic: 0.1728 on 1 and 98 DF,  p-value: 0.6786\n\n\n\nwhat’s the p-value of the association?\nis the p-value significant at 5% significance leve?\n\nNext, test the association between Yalt and X\n\nsummary(lm(Yalt ~ X))\n\n\nCall:\nlm(formula = Yalt ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.9610 -1.3250  0.0524  1.6622  4.2743 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  -0.2102     0.1976  -1.064  0.29004   \nX             1.8297     0.6424   2.848  0.00536 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.969 on 98 degrees of freedom\nMultiple R-squared:  0.07645,   Adjusted R-squared:  0.06703 \nF-statistic: 8.113 on 1 and 98 DF,  p-value: 0.005357\n\n\n\nwhat’s the p-value of the association?\nis the p-value significant at 5% significance level?"
  },
  {
    "objectID": "posts/2023-03-28-multiple-testing/index.html#calculate-the-empirical-distribution-of-p-values",
    "href": "posts/2023-03-28-multiple-testing/index.html#calculate-the-empirical-distribution-of-p-values",
    "title": "Multiple Testing Vignette",
    "section": "Calculate the empirical distribution of p-values",
    "text": "Calculate the empirical distribution of p-values\nTo calculate the empirical distribution of p-values under the null and alternatives we will simulate X, Yalt, Ynull for 10,000 times.\n\nDefine a convenience function fastlm, will do linear regression much faster\nWe want to run 10,000 times this same regression, so here we define a function fastlm that will get us the p-values and regression coefficients.\n\nfastlm = function(xx,yy)\n{\n  ## compute betahat (regression coef) and pvalue with Ftest\n  ## for now it does not take covariates\n  \n  df1 = 2\n  df0 = 1\n  ind = !is.na(xx) & !is.na(yy)\n  xx = xx[ind]\n  yy = yy[ind]\n  n = sum(ind)\n  xbar = mean(xx)\n  ybar = mean(yy)\n  xx = xx - xbar\n  yy = yy - ybar\n  \n  SXX = sum( xx^2 )\n  SYY = sum( yy^2 )\n  SXY = sum( xx * yy )\n  \n  betahat = SXY / SXX\n  \n  RSS1 = sum( ( yy - xx * betahat )^2 )\n  RSS0 = SYY\n  \n  fstat = ( ( RSS0 - RSS1 ) / ( df1 - df0 ) )  / ( RSS1 / ( n - df1 ) )\n  pval = 1 - pf(fstat, df1 = ( df1 - df0 ), df2 = ( n - df1 ))\n  res = list(betahat = betahat, pval = pval)\n  \n  return(res)\n}\n\n\n\nSimulate vectors X, Ynull, Yalt 10,000 times\n\nnsim = 10000\n## simulate normally distributed X and epsi\nXmat = matrix(rnorm(nsim * nsamp,mean=0, sd= sigX), nsamp, nsim)\nepsimat = matrix(rnorm(nsim * nsamp,mean=0, sd=sigepsi), nsamp, nsim)\n\n## generate Yalt (X has an effect on Yalt)\nYmat_alt = Xmat * beta + epsimat\n\n## generate Ynull (X has no effect on Ynull)\nYmat_null = matrix(rnorm(nsim * nsamp, mean=0, sd=beta), nsamp, nsim)\n\n## let's look at the dimensions of the simulated matrices\n\ndim(Ymat_null)\n\n[1]   100 10000\n\ndim(Ymat_alt)\n\n[1]   100 10000\n\n\nNow we have 10000 independent simulations of X, Ynull, and Yalt\n\n## give them names so that we can refer to them later more easily\ncolnames(Ymat_null) = paste0(\"c\",1:ncol(Ymat_null))\ncolnames(Ymat_alt) = colnames(Ymat_null)\n\n\nTo calculate p-values under the null run 10,000 linear regressions using X and Ynull\n\n\npvec_null = rep(NA,nsim)\nbvec_null = rep(NA,nsim)\n\nfor(ss in 1:nsim)\n{\n  fit = fastlm(Xmat[,ss], Ymat_null[,ss])\n  pvec_null[ss] = fit$pval  \n  bvec_null[ss] = fit$betahat\n}\n\nsummary(pvec_null)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0002453 0.2469528 0.4923302 0.4975967 0.7537629 0.9999578 \n\nhist(pvec_null,xlab=\"p-value\",main=\"Histogram of p-values under Null\")\n\n\n\n\n\nhow many simulations under the null yield p-value below 0.05? What percentage is that?\n\n\nsum(pvec_null<0.05)\n\n[1] 523\n\nmean(pvec_null<0.05)\n\n[1] 0.0523\n\n\n\nhow many simulations under the null yield p-value < 0.20?\nwhat do you think the proportion of simulations with p-values < \\(\\alpha\\) (\\(\\alpha\\) between 0 and 1) will be roughly?\nWhy do we need to use more stringent significance level when we test many times?"
  },
  {
    "objectID": "posts/2023-03-28-multiple-testing/index.html#bonferroni-correction",
    "href": "posts/2023-03-28-multiple-testing/index.html#bonferroni-correction",
    "title": "Multiple Testing Vignette",
    "section": "Bonferroni correction",
    "text": "Bonferroni correction\nUse as the new threshold the original one divided by the number of tests. So typically\n\\[\\frac{0.05}{\\text{total number of tests}}\\]\n\nwhat’s the Bonferroni threshold for significance in this simulation?\nhow many did we find?\n\n\nBF_thres = 0.05/nsim\n## Bonferroni significance threshold\nprint(BF_thres) \n\n[1] 5e-06\n\n## number of Bonferroni significant associations\nsum(pvec_null<BF_thres)\n\n[1] 0\n\n## proportion of Bonferroni significant associations\nmean(pvec_null<BF_thres)\n\n[1] 0"
  },
  {
    "objectID": "posts/2023-03-28-multiple-testing/index.html#mix-of-ynull-and-yalt",
    "href": "posts/2023-03-28-multiple-testing/index.html#mix-of-ynull-and-yalt",
    "title": "Multiple Testing Vignette",
    "section": "Mix of Ynull and Yalt",
    "text": "Mix of Ynull and Yalt\nLet’s see what happens when we add a bunch of true associations in the matrix of null associations\n\nprop_alt=0.20 ## define proportion of alternative Ys in the mixture\nselectvec = rbinom(nsim,1,prop_alt)\nnames(selectvec) = colnames(Ymat_alt)\nselectvec[1:10]\n\n c1  c2  c3  c4  c5  c6  c7  c8  c9 c10 \n  0   0   0   0   0   0   0   0   0   0 \n\nYmat_mix = sweep(Ymat_alt,2,selectvec,FUN='*') + sweep(Ymat_null,2,1-selectvec,FUN='*')\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun linear regression for all 10,000 phenotypes in the mix of true and false associations, Ymat_mix\n\npvec_mix = rep(NA,nsim)\nbvec_mix = rep(NA,nsim)\nfor(ss in 1:nsim)\n{\n  fit = fastlm(Xmat[,ss], Ymat_mix[,ss])\n  pvec_mix[ss] = fit$pval  \n  bvec_mix[ss] = fit$betahat\n}\nsummary(pvec_mix)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.08125 0.37643 0.40485 0.68693 0.99988 \n\nhist(pvec_mix,xlab=\"p-value\",main=\"Histogram of p-values under mixture of null and alt\")\n\n\n\nm_signif = sum(pvec_mix < 0.05) ## observed number of significant associations\nm_expected = 0.05*nsim ## expected number of significant associations under the worst case scenario, where all features belong to the null \nm_signif\n\n[1] 2191\n\nm_expected\n\n[1] 500\n\n\nUnder the null, we were expecting 500 significant columns by chance but got 2191\nQ: how can we estimate the proportion of true positives?\nWe got 1691 extra columns, so it’s reasonable to expect that the extra significant results come from the alternative distribution (Yalt). So \\[\\frac{\\text{observed number of significant} - \\text{expected number of significant}}{\\text{observed number of significant}}\\] should be a good estimate of the true discovery rate. False discovery rate is defined as 1 - the true discovery rate.\n\nthres = 0.05\nFDR = sum((pvec_mix<thres & selectvec==0)) / sum(pvec_mix<thres)\n## proportion of null columns that are significant among all significant\nFDR\n\n[1] 0.1866728\n\n\nIf we use a p-value threshold of 0.05, 81.33 percent of the signficant columns are true discoveries.  In this case, we know which ones are true or false associations because we decided using the selectvec vectors which simulated Y would be a function of X or unrelated to X.\n\nwhat’s the proportion of false discoveries if we use a significance level of 0.01\nwhat’s the proportion of false discoveries if we use Bonferroni correction as the significance level?\nWhat’s the proportion of missed signals, proportion of true associations that have p-values greater than the Bonferroni threshold?"
  },
  {
    "objectID": "posts/2023-03-28-multiple-testing/index.html#common-approaches-to-control-type-i-errors",
    "href": "posts/2023-03-28-multiple-testing/index.html#common-approaches-to-control-type-i-errors",
    "title": "Multiple Testing Vignette",
    "section": "Common approaches to control type I errors",
    "text": "Common approaches to control type I errors\nAssuming we are testing \\(m\\) hypothesis, let’s define the following terms for the different errors.\n\n\n\n\n\n\n\n\n\n\nCalled Significant\nCalled not significant\nTotal\n\n\n\n\nNull true\n\\(F\\)\n\\(m_0 - F\\)\n\\(m_0\\)\n\n\nAlt true\n\\(T\\)\n\\(m_1 - T\\)\n\\(m_1\\)\n\n\nTotal\n\\(S\\)\n\\(m - S\\)\n\\(m\\)\n\n\n\n\nBonferroni correction assures that the FWER (Familywise error rate) \\(P(F \\ge 1)\\) is below the acceptable type I error, typically 0.05. \\[P(F \\ge 1) < \\alpha. \\] We achieve that by requiring that for each test \\[p<\\alpha/\\text{# tests}.\\] This can be too stringent and lead to miss real signals.\npFDR (positive false discovery rate) \\[E\\left(\\frac{F}{S} \\rvert S>0\\right)\\]\nqvalue is the minimum false discovery rate attainable when the feature (SNP) is called significant"
  },
  {
    "objectID": "posts/2023-03-28-multiple-testing/index.html#table-of-null-or-alternative-vs.-significant-or-not-significant",
    "href": "posts/2023-03-28-multiple-testing/index.html#table-of-null-or-alternative-vs.-significant-or-not-significant",
    "title": "Multiple Testing Vignette",
    "section": "Table of null or alternative vs. significant or not significant",
    "text": "Table of null or alternative vs. significant or not significant\n\ncount_table = t(table(pvec_mix>0.05, selectvec))\ncolnames(count_table) = c(\"Called significant\", \"Called not significant\")\nrownames(count_table) = c(\"Null true\", \"Alt true\")\nknitr::kable(count_table)\n\n\n\n\n\nCalled significant\nCalled not significant\n\n\n\n\nNull true\n409\n7611\n\n\nAlt true\n1782\n198\n\n\n\n\n\nLet’s calculate the qvalue"
  },
  {
    "objectID": "posts/2023-03-28-multiple-testing/index.html#use-qvalue-package-to-calculate-fdr-and",
    "href": "posts/2023-03-28-multiple-testing/index.html#use-qvalue-package-to-calculate-fdr-and",
    "title": "Multiple Testing Vignette",
    "section": "Use qvalue package to calculate FDR and",
    "text": "Use qvalue package to calculate FDR and\nLet’s check whether small qvalues correspond to true associations (i.e. the phenotype was generated under the alternative distribution)\n\n## install qvalue if not available.\nif(F) ## I set it to F now because I already installed the qvalue package\n{if (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"qvalue\")\n}\n\n## calculate qvalue using the qvalue function, which returns a list of values, we select the qvalue vector, which assigns the false discovery rate if the threshold for significance was the p-value of the same simulation vector\n\nqres_mix = qvalue::qvalue(pvec_mix)\nqvec_mix = qres_mix$qvalue\n\nqres_null = qvalue::qvalue(pvec_null)\nqvec_null = qres_null$qvalue\n\nboxplot(qvec_mix~selectvec)\n\n\n\n##plot(qvec_mix,col=selectvec*2 + 1, pch=selectvec + 1, lwd=selectvec*2 + 1) \n## using selectvec*2 + 1 as a quick way to get the vector to be 1 and 3 (1 is black 3 is green) instead of 1 and 2 (2 is read and can be difficult to read for color blind people)\n\nPlot sorted qvalues and color by the selectvec status (true association status)\n\nind=order(qvec_mix,decreasing=FALSE)\nplot(sort(qvec_mix),col=selectvec[ind]*2 + 1, pch=selectvec[ind] + 1, lwd=selectvec[ind]*2 + 1) \n\n\n\nsummary(qvec_mix) \n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0000001 0.2605298 0.6035350 0.4948872 0.7344018 0.8018152 \n\n\n\ndo small qvalues tend to be true?\ninterpret the figure\n\n\n## distribution of qvalues and pvalues by causal status\nboxplot(pvec_mix ~ selectvec, main='pvalue by causal status; 1=alt, 0=null')\n\n\n\nboxplot(qvec_mix ~ selectvec, main='qvalue by causal status; 1=alt, 0=null')\n\n\n\n\n\ninterpret these figures"
  },
  {
    "objectID": "posts/2023-03-28-multiple-testing/index.html#how-do-qvalues-and-pvalues-relate-to-each-other",
    "href": "posts/2023-03-28-multiple-testing/index.html#how-do-qvalues-and-pvalues-relate-to-each-other",
    "title": "Multiple Testing Vignette",
    "section": "How do qvalues and pvalues relate to each other?",
    "text": "How do qvalues and pvalues relate to each other?\n\nplot(pvec_null,qvec_null,main='qvalue vs pvalue for null')\n\n\n\nplot(pvec_mix,qvec_mix,main='qvalue vs pvalue for mixture of null and alt')\n\n\n\n\n\nq-values are monotonic functions of p-values\n\n\nwhat’s the smallest qvalue when all simulations are from the null?\nwhat’s the smallest qvalue when all simulations are from the mixture?"
  },
  {
    "objectID": "posts/2023-03-28-multiple-testing/index.html#pi0-and-pi1",
    "href": "posts/2023-03-28-multiple-testing/index.html#pi0-and-pi1",
    "title": "Multiple Testing Vignette",
    "section": "pi0 and pi1",
    "text": "pi0 and pi1\npi0 is the proportion of null hypothesis which can be estimated using the qvalue package 1 - pi1 is the proportion of true positive associations. This is a useful parameter as we will see in later classes.\n\nqres_null$pi0\n\n[1] 0.9846734\n\nqres_mix$pi0\n\n[1] 0.8019101\n\n\n\nhow many true positive proportion did you expect given the simulations we performed?"
  },
  {
    "objectID": "posts/2023-03-28-multiple-testing/index.html#references",
    "href": "posts/2023-03-28-multiple-testing/index.html#references",
    "title": "Multiple Testing Vignette",
    "section": "References",
    "text": "References\nStorey, John D., and Robert Tibshirani. 2003. “Statistical Significance for Genomewide Studies.” Proceedings of the National Academy of Sciences 100 (16): 9440–45."
  },
  {
    "objectID": "posts/2023-03-28-erap2-finemapping/index.html",
    "href": "posts/2023-03-28-erap2-finemapping/index.html",
    "title": "ERAP2 fine-mapping",
    "section": "",
    "text": "Show the code\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\n##PRE=\"/Users/margaretperry/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data \"\n##PRE=\"/Users/temi/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data\"\n## COPY THE DATE AND SLUG fields FROM THE HEADER\nSLUG=\"erap2-fine-mapping\" ## copy the slug from the header\nbDATE='2023-03-28' ## copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n\n## move data to DATA\n#tempodata=(\"~/Downloads/tempo/gwas_catalog_v1.0.2-associations_e105_r2022-04-07.tsv\")\n#system(glue::glue(\"cp {tempodata} {DATA}/\"))\n\n## system(glue(\"open {DATA}\")) ## this will open the folder \n\n\nERAP2 fine-mapping results DAPG\n\n\nShow the code\n## query\n## SELECT * FROM `gtex-awg-im.GTEx_V8_DAPG.variants_pip_eqtl` where gene like \"ENSG00000164308%\"\nerap2 = read_csv(glue(\"{DATA}/bquxjob_41de6a2f_18728cf1999.csv\"))\n\n\nRows: 2260 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): tissue, gene, variant_id\ndbl (4): rank, pip, log10_abvf, cluster_id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\n## SELECT * FROM `gtex-awg-im.GTEx_V8_DAPG.variants_pip_sqtl` where variant_id like \"chr5_96900192%\" order by pip desc\ntauras_snp = read_csv(glue(\"{DATA}/bquxjob_719c0131_18728db8878.csv\"))\n\n\nRows: 113 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): tissue, gene_id, variant_id\ndbl (4): rank, pip, log10_abvf, cluster_id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\n## finemapping for the intron affected by chr5_96900192\n## SELECT * FROM `gtex-awg-im.GTEx_V8_DAPG.variants_pip_sqtl` where gene_id like \"intron_5_96900189_96901506\" order by pip desc\n\nintron = read_csv(glue(\"{DATA}/bquxjob_41bc6351_18728e4ee94.csv\"))\n\n\nRows: 2439 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): tissue, gene_id, variant_id\ndbl (4): rank, pip, log10_abvf, cluster_id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\n##\nintron %>% filter(tissue==\"Cells_EBV-transformed_lymphocytes\") %>% arrange(desc(pip))\n\n\n# A tibble: 122 × 7\n   tissue                           gene_id  rank varia…¹    pip log10…² clust…³\n   <chr>                            <chr>   <dbl> <chr>    <dbl>   <dbl>   <dbl>\n 1 Cells_EBV-transformed_lymphocyt… intron…     2 chr5_9… 0.250     25.8       2\n 2 Cells_EBV-transformed_lymphocyt… intron…     1 chr5_9… 0.250     25.8       2\n 3 Cells_EBV-transformed_lymphocyt… intron…     3 chr5_9… 0.218     27.7       1\n 4 Cells_EBV-transformed_lymphocyt… intron…     4 chr5_9… 0.218     27.7       1\n 5 Cells_EBV-transformed_lymphocyt… intron…     5 chr5_9… 0.0902    27.7       1\n 6 Cells_EBV-transformed_lymphocyt… intron…     7 chr5_9… 0.0426    27.7       1\n 7 Cells_EBV-transformed_lymphocyt… intron…     6 chr5_9… 0.0426    27.7       1\n 8 Cells_EBV-transformed_lymphocyt… intron…     8 chr5_9… 0.0426    27.7       1\n 9 Cells_EBV-transformed_lymphocyt… intron…     9 chr5_9… 0.0380    25.8       2\n10 Cells_EBV-transformed_lymphocyt… intron…    10 chr5_9… 0.0371    25.8       2\n# … with 112 more rows, and abbreviated variable names ¹​variant_id,\n#   ²​log10_abvf, ³​cluster_id\n# ℹ Use `print(n = ...)` to see more rows\n\n\nShow the code\n## \n\n## erap2 %>% filter(pip>0.1) %>% group_by(variant_id) %>% summarise(sumpip=sum(pip),ntissues=n()) %>% ggplot(aes(variant_id,sumpip)) + geom_bar(stat = \"identity\") + geom_point() + ggtitle(\"ERAP2 expr: most tissues assign pip to 16728 & 16885\")\n\nerap2 %>% filter(pip>0.1) %>% ggplot(aes(variant_id,pip)) + geom_violin() + geom_boxplot(width=0.05,alpha=0.5,outlier.shape = NA) + geom_point() + ggtitle(\"ERAP2 expr: most tissues assign pip to 16728 & 16885\") + ylim(0,NA)\n\n\nWarning: Groups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\n\n\n\n\n\nShow the code\nprint(\"intron intron_5_96900189_96901506 \")\n\n\n[1] \"intron intron_5_96900189_96901506 \"\n\n\nShow the code\nintron %>% filter(pip>0.1) %>% ggplot(aes(variant_id,pip)) + geom_violin() + geom_boxplot(width=0.05,alpha=0.5,outlier.shape = NA) + geom_point() + ggtitle(\"ERAP2 intron_5_96900189_96901506:\") + ylim(0,NA) + coord_flip()\n\n\nWarning: Groups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\n\n\n\n\n\nShow the code\n#intron %>% filter(pip>0.1) %>% group_by(variant_id) %>% summarise(sumpip=sum(pip),ntissues=n()) %>% ggplot(aes(variant_id,sumpip)) + geom_bar(stat = \"identity\") + ggtitle(\"ERAP2 intron_5_96900189_96901506: \") + coord_flip()\n\n#ggplot(aes(variant_id,sumpip)) + geom_bar() \n\n\nCausal SNP according to the black death paper and others is rs2248374 chr5_96900192"
  },
  {
    "objectID": "posts/2023-03-28-cistromedb-data/index.html",
    "href": "posts/2023-03-28-cistromedb-data/index.html",
    "title": "Cistrome DB data",
    "section": "",
    "text": "suppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\n##PRE=\"/Users/margaretperry/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data \"\n##PRE=\"/Users/temi/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data\"\n## COPY THE DATE AND SLUG fields FROM THE HEADER\nSLUG=\"cistromedb-data\" ## copy the slug from the header\nbDATE='2023-03-28' ## copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n\n## move data to DATA\n#tempodata=(\"~/Downloads/tempo/gwas_catalog_v1.0.2-associations_e105_r2022-04-07.tsv\")\n#system(glue::glue(\"cp {tempodata} {DATA}/\"))\n\n## system(glue(\"open {DATA}\")) ## this will open the folder \n\n\ndata = read_tsv(glue(\"{DATA}/human_factor_full_QC.txt\"))\n\nRows: 11348 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (6): Species, GSMID, Factor, Cell_line, Cell_type, Tissue_type\ndbl (7): DCid, FastQC, UniquelyMappedRatio, PBC, PeaksFoldChangeAbove10, FRi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnames(data)\n\n [1] \"DCid\"                   \"Species\"                \"GSMID\"                 \n [4] \"Factor\"                 \"Cell_line\"              \"Cell_type\"             \n [7] \"Tissue_type\"            \"FastQC\"                 \"UniquelyMappedRatio\"   \n[10] \"PBC\"                    \"PeaksFoldChangeAbove10\" \"FRiP\"                  \n[13] \"PeaksUnionDHSRatio\"    \n\ndata %>% select(Factor,Cell_line,Cell_type,Tissue_type) %>% unique() %>% dim()\n\n[1] 4426    4\n\ndata %>% count(Factor,Cell_line,Cell_type,Tissue_type) %>% arrange(desc(n))\n\n# A tibble: 4,426 × 5\n   Factor Cell_line Cell_type  Tissue_type     n\n   <chr>  <chr>     <chr>      <chr>       <int>\n 1 ESR1   MCF-7     Epithelium Breast        213\n 2 AR     LNCaP     Epithelium Prostate      143\n 3 POLR2A HeLa      Epithelium Cervix         76\n 4 AR     VCaP      Epithelium Prostate       64\n 5 POLR2A MCF-7     Epithelium Breast         64\n 6 NR3C1  A549      Epithelium Lung           46\n 7 POLR2A HCT-116   None       HCT116         46\n 8 CTCF   MCF-7     Epithelium Breast         45\n 9 FOXA1  LNCaP     Epithelium Prostate       45\n10 ESR1   None      None       Breast         42\n# … with 4,416 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\ndata %>% count(Factor,Cell_line,Cell_type,Tissue_type) %>% .[[\"n\"]] %>% table()\n\n.\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n2223 1196  299  282   89  106   36   43   18   28   12   17    4    8    7    6 \n  17   18   19   20   22   23   24   25   26   28   29   30   31   32   33   34 \n   1    8    3    2    1    1    2    3    2    1    1    3    2    2    3    3 \n  37   38   40   42   45   46   64   76  143  213 \n   2    1    1    1    2    2    2    1    1    1 \n\n## are cell line==None non tumor?\ndata %>% filter(Cell_line==\"None\") %>% dim()\n\n[1] 1817   13\n\n## how many unique cell lines?\ndata %>% count(Cell_line) %>% dim()\n\n[1] 520   2\n\n## how many unique cell types?\ndata %>% count(Cell_type) %>% dim()\n\n[1] 153   2\n\n## how many unique tissue types?\ndata %>% count(Tissue_type) %>% dim()\n\n[1] 84  2"
  },
  {
    "objectID": "posts/migrating-instructions/index.html",
    "href": "posts/migrating-instructions/index.html",
    "title": "Migrating",
    "section": "",
    "text": "Here will come the list of migrating instructions\n\ncreate folder under post\nname the folder year-month-date-slug (slug=summary of title of the post)"
  },
  {
    "objectID": "posts/2023-03-01-how-to-create-new-posts/index.html",
    "href": "posts/2023-03-01-how-to-create-new-posts/index.html",
    "title": "How to create a new blog post",
    "section": "",
    "text": "create a new folder under post/new_folder\nname the folder with year-month-date-slug, 2023-03-02-descriptive-title\ncreate a file named index.qmd and add the following header, update the title, your name and the date"
  },
  {
    "objectID": "posts/2023-03-01-how-to-create-new-posts/index.html#other-links",
    "href": "posts/2023-03-01-how-to-create-new-posts/index.html#other-links",
    "title": "How to create a new blog post",
    "section": "other links",
    "text": "other links\nUseful tips for new posts here"
  },
  {
    "objectID": "posts/2023-03-01-how-to-create-new-posts/index.html#functions",
    "href": "posts/2023-03-01-how-to-create-new-posts/index.html#functions",
    "title": "How to create a new blog post",
    "section": "functions",
    "text": "functions"
  },
  {
    "objectID": "posts/2023-04-07-inflation-brainxcan/index.html",
    "href": "posts/2023-04-07-inflation-brainxcan/index.html",
    "title": "Inflation Brainxcan",
    "section": "",
    "text": "Summary:\n\n\nShow the code\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\n\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\nSLUG=\"inflation-brainxcan\" ## copy the slug from the header\nbDATE='2023-04-07' ## copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n##system(glue(\"open {DATA}\")) ## this will open the folder \n\n\n\nsimulate Xk\n\n\n\nShow the code\nnsam = 10000\nnsim = 100\nmsnp = 1000\n\nXmat = matrix(rbinom(nsam*msnp, 2, 0.4), nsam, msnp)\n\n\n\nsimulate polygenic model: Y and IDP (nsam x nsim)\n\n\\[\\begin{align}\nY &= \\beta \\cdot \\text{IDP} + \\sum_k X_k \\cdot b_k + \\epsilon \\\\\n\\text{IDP} &= \\sum_k \\gamma_k \\cdot X_k + \\epsilon',\n\\end{align}\\]\n\n\nShow the code\n## betamat msnp x 1 \nbetamat = matrix(rnorm(msnp),msnp, 1)\nepsimat = matrix(rnorm(nsam),nsam, 1)\ngYmat = Xmat %*% betamat\nYmat = gYmat + epsimat\n## Ymat = scale(Ymat)\n\n\n\n\nShow the code\ngammamat = matrix(rnorm(msnp * nsim),msnp, nsim)\nepsimat2 = matrix(rnorm(nsam * nsim),nsam, nsim)\ngIDPmat = Xmat %*% gammamat\nIDPmat = gIDPmat + epsimat2\n\n\n\ncalculate pvec of Y ~ IDP\n\n\n\nShow the code\n# ## load fast lm\n# devtools::source_gist(\"5d2251ea1a86009499e4ffdf47fe2735\")\n\n## load fn_ratxcan: 115403f16bec0a0e871f3616d552ce9b\ndevtools::source_gist(\"115403f16bec0a0e871f3616d552ce9b\")\n\n\nℹ Sourcing gist \"115403f16bec0a0e871f3616d552ce9b\"\nℹ SHA-1 hash of file is \"06f294311936f3783e4d652bac96a59da93a765e\"\n\n\nShow the code\ndevtools::source_gist(\"38431b74c6c0bf90c12f\")\n\n\nℹ Sourcing gist \"38431b74c6c0bf90c12f\"\nℹ SHA-1 hash of file is \"cbeca7fd9bf1602dee41c4f1880cc3a5e8992303\"\n\n\nShow the code\n## run fast_predixcan_assoc\nidnum=1:nsam\nidvec = glue(\"id-{idnum}\")\nres <- fast_predixcan_assoc(data.frame(IID=idvec,IDPmat), data.frame(IID=idvec,Ymat), idlist=idvec)\n\n\n[1] 10000   100\n[1] 10000     1\nA sample size of 10000 was used\n\n\nShow the code\nqqunif(res$pval)\n\n\n\n\n\n\ncalculate pvec of cor(Y, IDP): this is the same as running linear regression\n\n\n\nShow the code\n# > summary(lm(Ymat~ IDPmat[,32]))\n# \n# Call:\n# lm(formula = Ymat ~ IDPmat[, 32])\n# \n# Residuals:\n#      Min       1Q   Median       3Q      Max \n# -201.069  -46.406   -0.164   46.472  205.379 \n# \n# Coefficients:\n#               Estimate Std. Error t value Pr(>|t|)    \n# (Intercept)  -37.14480    4.61728  -8.045 2.44e-15 ***\n# IDPmat[, 32]  -0.08780    0.03051  -2.878  0.00409 ** \n# ---\n# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n# \n# Residual standard error: 68.63 on 998 degrees of freedom\n# Multiple R-squared:  0.008231,    Adjusted R-squared:  0.007237 \n# F-statistic: 8.282 on 1 and 998 DF,  p-value: 0.004088\n# \n# > cor.test(Ymat, IDPmat[,32])\n# \n#   Pearson's product-moment correlation\n# \n# data:  Ymat and IDPmat[, 32]\n# t = -2.8779, df = 998, p-value = 0.004088\n# alternative hypothesis: true correlation is not equal to 0\n# 95 percent confidence interval:\n#  -0.15186241 -0.02889285\n# sample estimates:\n#         cor \n# -0.09072344 \n\n\n\ncalculate pvec of cor(sum bk Xk, sum gammak Xk)\n\n\n\nShow the code\n## run fast_predixcan_assoc\nidnum=1:nsam\nidvec = glue(\"id-{idnum}\")\ngres <- fast_predixcan_assoc(data.frame(IID=idvec,gIDPmat), data.frame(IID=idvec,gYmat), idlist=idvec)\n\n\n[1] 10000   100\n[1] 10000     1\nA sample size of 10000 was used\n\n\nShow the code\nqqunif(gres$pval)\n\n\n\n\n\n\ncheck FDR of BrainXcan schizophrenia associations\n\n\n\nShow the code\n## google sheets prepared by Yanyu for revision 4/7/2023\ns2 <- read_csv(glue(\"{DATA}/Table_S2.xlsx - ..csv\"))\n\n\nRows: 459 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): IDP, modality, subtype, pc1_name, region, side, measurement_type, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\ns8 <- read_csv(glue(\"{DATA}/Table_S8-w-factor.xlsx - Table_S8-w-factor.csv\"))\n\n\nRows: 16380 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): model, IDP, phenotype\ndbl (7): bhat, pval, zscore, nsnp_used, nsnp_total, z_adj_perm_null, pval_ad...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\ntempo <- s8 %>% left_join(s2 %>% select(IDP,modality,notes,subtype),by=c(\"IDP\")) %>% filter(model==\"ridge\",phenotype==\"SCZ_PGC_2020\") %>% filter(substr(subtype, 1, 2) != 'w-' | is.na(subtype) ) %>% filter(!grepl(\"ProbTrack-1\",IDP))\n\n## qq <- tempo %>% filter(modality==\"dMRI\") %>% .[[\"pval_adj_perm_null\"]] %>% qvalue::qvalue()\n\nqq <- tempo  %>% .[[\"pval_adj_perm_null\"]] %>% qvalue::qvalue()\n\n\n\nread hapmap file\n\n\n\nShow the code\n# if (!require(\"BiocManager\", quietly = TRUE))\n#     install.packages(\"BiocManager\")\n# BiocManager::install(\"snpStats\")\nsuppressMessages(library(snpStats))\n# Set the path to the binary files (without file extensions)\nplink_file_path <- \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/Reference-Data/GWAS-tutorial-Marees/data_1_QC_GWAS/HapMap_3_r3_1\"\n# Read the binary files into an object of class \"snpMatrix\"\nsnp_matrix <- read.plink(plink_file_path)"
  },
  {
    "objectID": "posts/2023-03-23-jane-austen-corpus/index.html",
    "href": "posts/2023-03-23-jane-austen-corpus/index.html",
    "title": "Jane Austen Corpus",
    "section": "",
    "text": "Show the code\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\n##PRE=\"/Users/margaretperry/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data \"\n##PRE=\"/Users/temi/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data\"\n## COPY THE DATE AND SLUG fields FROM THE HEADER\nSLUG=\"jane-austen-corpus\" ## copy the slug from the header\nbDATE='2023-03238' ## copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n\n## move data to DATA\n#tempodata=(\"~/Downloads/tempo/gwas_catalog_v1.0.2-associations_e105_r2022-04-07.tsv\")\n#system(glue::glue(\"cp {tempodata} {DATA}/\"))\nsystem(glue(\"open {DATA}\")) ## this will open the folder \n\n\n\nget jane austen corpus\n\n\n\nShow the code\n##install.packages(\"janeaustenr\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nTESTING"
  },
  {
    "objectID": "posts/2023-05-26-metaboxcan-paper-analysis/index.html",
    "href": "posts/2023-05-26-metaboxcan-paper-analysis/index.html",
    "title": "Miscel MetaboXcan paper analysis",
    "section": "",
    "text": "#gene2metabo Metsim - `/gpfs/data/im-lab/nas40t2/festus/metabolomics/paper-data/gene2metabo-metsim-softimpute-all.db`\n\n#gene2metabo Guardian - `/gpfs/data/im-lab/nas40t2/festus/metabolomics/paper-data/gene2metabo-guardian-irasfs-all.db`\n\n#Metsim-lasso-model - `/gpfs/data/im-lab/nas40t2/festus/metabolomics/paper-data/metsim-lasso-softimpute.db`\n\n#Guardian-lasso-model (abit older) - `/gpfs/data/im-lab/nas40t2/festus/metabolomics/paper-data/guardian-irasfs-lasso.db`\n\n#Guardian-bslmm-model - `/gpfs/data/im-lab/nas40t2/festus/metabolomics/paper-data/guardian-irasfs-bslmm.db`\n\n#Hybrid best model (cor > 0.1) - `/gpfs/data/im-lab/nas40t2/festus/metabolomics/paper-data/hybrid-model-best-cor0.1.db`\n\n#Hybrid all models (guardian, omicspred, metsim) - `/gpfs/data/im-lab/nas40t2/festus/metabolomics/paper-data/hybrid-model-all-metabolon.db`\n\n#Metaboxcan reports (v1) using the Hybrid-models - `/Users/user/Box/imlab-data/Metaboxcan/old-summaries` - the metaboxcan_best contains the results from the best models above and pvalue corrected while the metaboxcan_all contains the results from all of the hybrid models combined without filtering. The metsim model used in the hybrid was the one generated using PRScs from the summary stats.\n\n#Metaboxcan reports (v2) using the Metsim lasso models - `/Users/user/Box/imlab-data/Metaboxcan/summaries` - the metaboxcan_best and metaboxcan_all sheet contain same results only one difference is best contains pvalues corrected by bacon."
  },
  {
    "objectID": "posts/2023-04-27-how-to-read-gencode-gff/index.html",
    "href": "posts/2023-04-27-how-to-read-gencode-gff/index.html",
    "title": "How to read gencode gtf file",
    "section": "",
    "text": "Show the code\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\nSLUG=\"how-to-read-gencode-gtf-file\" ## copy the slug from the header\nbDATE='2023-04-27' ## copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n\n## move data to DATA\n#tempodata=(\"~/Downloads/tempo/gwas_catalog_v1.0.2-associations_e105_r2022-04-07.tsv\")\n#system(glue::glue(\"cp {tempodata} {DATA}/\"))\n\n#system(glue(\"open {DATA}\")) ## this will open the folder \n\n\n\ninstall if necessary\n\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"rtracklayer\")\n\n\nShow the code\n# Load the rtracklayer package\nlibrary(rtracklayer)\n\n# Define the path to your GFF file\ngff_file_path <- glue(\"{DATA}/gencode.v43.annotation.gff3.gz\")\n\n# Read the GFF file\ngff_data <- import(gff_file_path, format = \"gff3\")\n\n# Convert the GFF data to a data frame\ngff_data_frame <- as.data.frame(gff_data)\n\n# Display the first few rows of the data frame\nhead(gff_data_frame)\n\n\n\n\nShow the code\n\"ENSG00000164308\"\n\n## all gene_id start with ENSG\ntable(substr(gff_data_frame$gene_id,1,4))\n## all entries have a gene_id\ndim(gff_data_frame)\n## number of genes with positive vs negative strands\n\n\nto load version 75 of ensembl\n\n\nShow the code\nlibrary(biomaRt)\n\n# mart <- useMart(biomart = \"ENSEMBL_MART_ENSEMBL\", \n#                 host = \"https://dec2013.archive.ensembl.org\", # This host corresponds to Ensembl version 75\n#                 path = \"/biomart/martservice\")\nmart <- useMart(biomart = \"ENSEMBL_MART_ENSEMBL\", \n                host = \"https://feb2014.archive.ensembl.org\", # This host corresponds to Ensembl version 75\n                path = \"/biomart/martservice\")"
  },
  {
    "objectID": "content/posts/2023-04-27-how-to-read-gencode-gff/index.html",
    "href": "content/posts/2023-04-27-how-to-read-gencode-gff/index.html",
    "title": "How to read gencode gtf file",
    "section": "",
    "text": "Show the code\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\nSLUG=\"how-to-read-gencode-gtf-file\" ## copy the slug from the header\nbDATE='2023-04-27' ## copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n\n## move data to DATA\n#tempodata=(\"~/Downloads/tempo/gwas_catalog_v1.0.2-associations_e105_r2022-04-07.tsv\")\n#system(glue::glue(\"cp {tempodata} {DATA}/\"))\n\n#system(glue(\"open {DATA}\")) ## this will open the folder \n\n\n\ninstall if necessary\n\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"rtracklayer\")\n\n\nShow the code\n# Load the rtracklayer package\nlibrary(rtracklayer)\n\n# Define the path to your GFF file\ngff_file_path <- glue(\"{DATA}/gencode.v43.annotation.gff3.gz\")\n\n# Read the GFF file\ngff_data <- import(gff_file_path, format = \"gff3\")\n\n# Convert the GFF data to a data frame\ngff_data_frame <- as.data.frame(gff_data)\n\n# Display the first few rows of the data frame\nhead(gff_data_frame)\n\n\n\n\nShow the code\n\"ENSG00000164308\"\n\n## all gene_id start with ENSG\ntable(substr(gff_data_frame$gene_id,1,4))\n## all entries have a gene_id\ndim(gff_data_frame)\n## number of genes with positive vs negative strands\n\n\nto load version 75 of ensembl\n\n\nShow the code\nlibrary(biomaRt)\n\n# mart <- useMart(biomart = \"ENSEMBL_MART_ENSEMBL\", \n#                 host = \"https://dec2013.archive.ensembl.org\", # This host corresponds to Ensembl version 75\n#                 path = \"/biomart/martservice\")\nmart <- useMart(biomart = \"ENSEMBL_MART_ENSEMBL\", \n                host = \"https://feb2014.archive.ensembl.org\", # This host corresponds to Ensembl version 75\n                path = \"/biomart/martservice\")"
  },
  {
    "objectID": "content/posts/2023-04-07-inflation-brainxcan/index.html",
    "href": "content/posts/2023-04-07-inflation-brainxcan/index.html",
    "title": "Inflation Brainxcan",
    "section": "",
    "text": "Summary:\n\n\nShow the code\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\n\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\nSLUG=\"inflation-brainxcan\" ## copy the slug from the header\nbDATE='2023-04-07' ## copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n##system(glue(\"open {DATA}\")) ## this will open the folder \n\n\n\nsimulate Xk\n\n\n\nShow the code\nnsam = 10000\nnsim = 100\nmsnp = 1000\n\nXmat = matrix(rbinom(nsam*msnp, 2, 0.4), nsam, msnp)\n\n\n\nsimulate polygenic model: Y and IDP (nsam x nsim)\n\n\\[\\begin{align}\nY &= \\beta \\cdot \\text{IDP} + \\sum_k X_k \\cdot b_k + \\epsilon \\\\\n\\text{IDP} &= \\sum_k \\gamma_k \\cdot X_k + \\epsilon',\n\\end{align}\\]\n\n\nShow the code\n## betamat msnp x 1 \nbetamat = matrix(rnorm(msnp),msnp, 1)\nepsimat = matrix(rnorm(nsam),nsam, 1)\ngYmat = Xmat %*% betamat\nYmat = gYmat + epsimat\n## Ymat = scale(Ymat)\n\n\n\n\nShow the code\ngammamat = matrix(rnorm(msnp * nsim),msnp, nsim)\nepsimat2 = matrix(rnorm(nsam * nsim),nsam, nsim)\ngIDPmat = Xmat %*% gammamat\nIDPmat = gIDPmat + epsimat2\n\n\n\ncalculate pvec of Y ~ IDP\n\n\n\nShow the code\n# ## load fast lm\n# devtools::source_gist(\"5d2251ea1a86009499e4ffdf47fe2735\")\n\n## load fn_ratxcan: 115403f16bec0a0e871f3616d552ce9b\ndevtools::source_gist(\"115403f16bec0a0e871f3616d552ce9b\")\n\n\nℹ Sourcing gist \"115403f16bec0a0e871f3616d552ce9b\"\nℹ SHA-1 hash of file is \"06f294311936f3783e4d652bac96a59da93a765e\"\n\n\nShow the code\ndevtools::source_gist(\"38431b74c6c0bf90c12f\")\n\n\nℹ Sourcing gist \"38431b74c6c0bf90c12f\"\nℹ SHA-1 hash of file is \"cbeca7fd9bf1602dee41c4f1880cc3a5e8992303\"\n\n\nShow the code\n## run fast_predixcan_assoc\nidnum=1:nsam\nidvec = glue(\"id-{idnum}\")\nres <- fast_predixcan_assoc(data.frame(IID=idvec,IDPmat), data.frame(IID=idvec,Ymat), idlist=idvec)\n\n\n[1] 10000   100\n[1] 10000     1\nA sample size of 10000 was used\n\n\nShow the code\nqqunif(res$pval)\n\n\n\n\n\n\ncalculate pvec of cor(Y, IDP): this is the same as running linear regression\n\n\n\nShow the code\n# > summary(lm(Ymat~ IDPmat[,32]))\n# \n# Call:\n# lm(formula = Ymat ~ IDPmat[, 32])\n# \n# Residuals:\n#      Min       1Q   Median       3Q      Max \n# -201.069  -46.406   -0.164   46.472  205.379 \n# \n# Coefficients:\n#               Estimate Std. Error t value Pr(>|t|)    \n# (Intercept)  -37.14480    4.61728  -8.045 2.44e-15 ***\n# IDPmat[, 32]  -0.08780    0.03051  -2.878  0.00409 ** \n# ---\n# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n# \n# Residual standard error: 68.63 on 998 degrees of freedom\n# Multiple R-squared:  0.008231,    Adjusted R-squared:  0.007237 \n# F-statistic: 8.282 on 1 and 998 DF,  p-value: 0.004088\n# \n# > cor.test(Ymat, IDPmat[,32])\n# \n#   Pearson's product-moment correlation\n# \n# data:  Ymat and IDPmat[, 32]\n# t = -2.8779, df = 998, p-value = 0.004088\n# alternative hypothesis: true correlation is not equal to 0\n# 95 percent confidence interval:\n#  -0.15186241 -0.02889285\n# sample estimates:\n#         cor \n# -0.09072344 \n\n\n\ncalculate pvec of cor(sum bk Xk, sum gammak Xk)\n\n\n\nShow the code\n## run fast_predixcan_assoc\nidnum=1:nsam\nidvec = glue(\"id-{idnum}\")\ngres <- fast_predixcan_assoc(data.frame(IID=idvec,gIDPmat), data.frame(IID=idvec,gYmat), idlist=idvec)\n\n\n[1] 10000   100\n[1] 10000     1\nA sample size of 10000 was used\n\n\nShow the code\nqqunif(gres$pval)\n\n\n\n\n\n\ncheck FDR of BrainXcan schizophrenia associations\n\n\n\nShow the code\n## google sheets prepared by Yanyu for revision 4/7/2023\ns2 <- read_csv(glue(\"{DATA}/Table_S2.xlsx - ..csv\"))\n\n\nRows: 459 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): IDP, modality, subtype, pc1_name, region, side, measurement_type, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\ns8 <- read_csv(glue(\"{DATA}/Table_S8-w-factor.xlsx - Table_S8-w-factor.csv\"))\n\n\nRows: 16380 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): model, IDP, phenotype\ndbl (7): bhat, pval, zscore, nsnp_used, nsnp_total, z_adj_perm_null, pval_ad...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\ntempo <- s8 %>% left_join(s2 %>% select(IDP,modality,notes,subtype),by=c(\"IDP\")) %>% filter(model==\"ridge\",phenotype==\"SCZ_PGC_2020\") %>% filter(substr(subtype, 1, 2) != 'w-' | is.na(subtype) ) %>% filter(!grepl(\"ProbTrack-1\",IDP))\n\n## qq <- tempo %>% filter(modality==\"dMRI\") %>% .[[\"pval_adj_perm_null\"]] %>% qvalue::qvalue()\n\nqq <- tempo  %>% .[[\"pval_adj_perm_null\"]] %>% qvalue::qvalue()\n\n\n\nread hapmap file\n\n\n\nShow the code\n# if (!require(\"BiocManager\", quietly = TRUE))\n#     install.packages(\"BiocManager\")\n# BiocManager::install(\"snpStats\")\nsuppressMessages(library(snpStats))\n# Set the path to the binary files (without file extensions)\nplink_file_path <- \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/Reference-Data/GWAS-tutorial-Marees/data_1_QC_GWAS/HapMap_3_r3_1\"\n# Read the binary files into an object of class \"snpMatrix\"\nsnp_matrix <- read.plink(plink_file_path)"
  },
  {
    "objectID": "content/posts/migrating-instructions/index.html",
    "href": "content/posts/migrating-instructions/index.html",
    "title": "Migrating",
    "section": "",
    "text": "Here will come the list of migrating instructions\n\ncreate folder under post\nname the folder year-month-date-slug (slug=summary of title of the post)"
  },
  {
    "objectID": "content/posts/2023-03-01-gtex-sample-size-by-tissue/index.html",
    "href": "content/posts/2023-03-01-gtex-sample-size-by-tissue/index.html",
    "title": "gtex-sample-size-by-tissue",
    "section": "",
    "text": "GTEx Sample Size by Tissue\n\n\n\n\n\n\n\n\n\n\nname\neuropean samples\nabbreviation\nexpression models\nsplicing models\n\n\n\n\nAdipose - Subcutaneous\n491\nADPSBQ\n14732\n42912\n\n\nAdipose - Visceral (Omentum)\n401\nADPVSC\n14640\n41720\n\n\nAdrenal Gland\n200\nADRNLG\n13622\n36754\n\n\nArtery - Aorta\n338\nARTAORT\n14396\n40474\n\n\nArtery - Coronary\n180\nARTCRN\n13878\n40579\n\n\nArtery - Tibial\n489\nARTTBL\n14493\n40690\n\n\nBrain - Amygdala\n119\nBRNAMY\n12814\n24236\n\n\nBrain - Anterior cingulate cortex (BA24)\n135\nBRNACC\n13528\n28806\n\n\nBrain - Caudate (basal ganglia)\n172\nBRNCDT\n14118\n32127\n\n\nBrain - Cerebellar Hemisphere\n157\nBRNCHB\n13771\n39862\n\n\nBrain - Cerebellum\n188\nBRNCHA\n13992\n40747\n\n\nBrain - Cortex\n184\nBRNCTXA\n14284\n35086\n\n\nBrain - Frontal Cortex (BA9)\n158\nBRNCTXB\n14091\n32031\n\n\nBrain - Hippocampus\n150\nBRNHPP\n13526\n27437\n\n\nBrain - Hypothalamus\n157\nBRNHPT\n13741\n30326\n\n\nBrain - Nucleus accumbens (basal ganglia)\n181\nBRNNCC\n14062\n32670\n\n\nBrain - Putamen (basal ganglia)\n153\nBRNPTM\n13694\n28461\n\n\nBrain - Spinal cord (cervical c-1)\n115\nBRNSPC\n13096\n28883\n\n\nBrain - Substantia nigra\n101\nBRNSNG\n12637\n23677\n\n\nBreast - Mammary Tissue\n337\nBREAST\n14654\n44613\n\n\nCells - Cultured fibroblasts\n417\nFIBRBLS\n13976\n36809\n\n\nCells - EBV-transformed lymphocytes\n116\nLCL\n12398\n37627\n\n\nColon - Sigmoid\n274\nCLNSGM\n14363\n41581\n\n\nColon - Transverse\n306\nCLNTRN\n14582\n41215\n\n\nEsophagus - Gastroesophageal Junction\n281\nESPGEJ\n14285\n41004\n\n\nEsophagus - Mucosa\n423\nESPMCS\n14589\n37186\n\n\nEsophagus - Muscularis\n399\nESPMSL\n14603\n40376\n\n\nHeart - Atrial Appendage\n322\nHRTAA\n14035\n36322\n\n\nHeart - Left Ventricle\n334\nHRTLV\n13200\n29470\n\n\nKidney - Cortex\n65\nKDNCTX\n11164\n24571\n\n\nLiver\n183\nLIVER\n12714\n27011\n\n\nLung\n444\nLUNG\n15058\n44346\n\n\nMinor Salivary Gland\n119\nSLVRYG\n13884\n38380\n\n\nMuscle - Skeletal\n602\nMSCLSK\n13381\n31855\n\n\nNerve - Tibial\n449\nNERVET\n15373\n45478\n\n\nOvary\n140\nOVARY\n13738\n40857\n\n\nPancreas\n253\nPNCREAS\n13695\n31203\n\n\nPituitary\n219\nPTTARY\n14647\n42343\n\n\nProstate\n186\nPRSTTE\n14450\n41991\n\n\nSkin - Not Sun Exposed (Suprapubic)\n440\nSKINNS\n14932\n42005\n\n\nSkin - Sun Exposed (Lower leg)\n517\nSKINS\n15204\n42219\n\n\nSmall Intestine - Terminal Ileum\n144\nSNTTRM\n14065\n39864\n\n\nSpleen\n186\nSPLEEN\n14073\n40290\n\n\nStomach\n269\nSTMACH\n14102\n36624\n\n\nTestis\n277\nTESTIS\n17867\n67784\n\n\nThyroid\n494\nTHYROID\n15303\n45217\n\n\nUterus\n108\nUTERUS\n13199\n39485\n\n\nVagina\n122\nVAGINA\n12969\n36931\n\n\nWhole Blood\n573\nWHLBLD\n12623\n24568"
  },
  {
    "objectID": "content/posts/2023-02-21-power-calculator-for-mol-qtls/index.html",
    "href": "content/posts/2023-02-21-power-calculator-for-mol-qtls/index.html",
    "title": "Power calculator for mol QTLs",
    "section": "",
    "text": "The success of the prediction training depends mostly on whether the corresponding QTL study will be powered. Thus, we provide here the power to detect molecular QTLs for a range of sample sizes, effect sizes, and minor allele frequencies."
  },
  {
    "objectID": "content/posts/2023-02-21-power-calculator-for-mol-qtls/index.html#function-r2-from-beta-assuming-variance-of-y-1",
    "href": "content/posts/2023-02-21-power-calculator-for-mol-qtls/index.html#function-r2-from-beta-assuming-variance-of-y-1",
    "title": "Power calculator for mol QTLs",
    "section": "function: R2 from beta assuming variance of y = 1",
    "text": "function: R2 from beta assuming variance of y = 1\n\\[y = \\delta \\cdot x + \\epsilon\\]\n\\[r^2 = \\delta^2 \\cdot \\text{var}(x) = \\delta^2 \\cdot 2 \\cdot \\text{maf} \\cdot (1-\\text{maf})\\]\n\n\nShow the code\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(knitr))\n## install.packages(\"pwr\")\nif (!(\"pwr\" %in% installed.packages()[, 1])) {\n  install.packages(\"pwr\")\n}"
  },
  {
    "objectID": "content/posts/2023-02-21-power-calculator-for-mol-qtls/index.html#define-ranges-of-maf-eff-sample-sizes",
    "href": "content/posts/2023-02-21-power-calculator-for-mol-qtls/index.html#define-ranges-of-maf-eff-sample-sizes",
    "title": "Power calculator for mol QTLs",
    "section": "define ranges of maf, eff, sample sizes",
    "text": "define ranges of maf, eff, sample sizes\n\n\nShow the code\nmafvec = c(0.05, 0.10, 0.30) \neffvec = c(0.40, 0.60, 0.80) \nnvec = c(200,350,500) \nnsnps = 1000\nalpha = 0.05/nsnps"
  },
  {
    "objectID": "content/posts/2023-02-21-power-calculator-for-mol-qtls/index.html#create-data-frame-with-all-combinations",
    "href": "content/posts/2023-02-21-power-calculator-for-mol-qtls/index.html#create-data-frame-with-all-combinations",
    "title": "Power calculator for mol QTLs",
    "section": "create data frame with all combinations",
    "text": "create data frame with all combinations\n\n\nShow the code\n# mat = matrix(NA,length(mafvec)*length(effvec)*length(nvec),4)\n# colnames(mat) = c(\"maf\",\"eff\",\"nsam\",\"power\")\n# cont = 1\n# for(maf in mafvec)\n# {\n#   for(nn in nvec)\n#   {\n#     for(eff in effvec)\n#     {\n#       r2 = eff^2 * 2 * maf * (1-maf)\n#       rr = sqrt(r2)\n#       pp = pwr::pwr.r.test(n = nn, r= rr , sig.level = alpha)\n#       mat[cont,] = c(maf, eff, nn, pp$power)\n#       cont = cont + 1\n#     }\n#   }\n# }\n# \n# mat %>% data.frame() %>% \n#   pivot_wider(names_from = nsam, values_from = power) %>% \n#   mutate(across(3:ncol(.), ~sprintf(\"%.1f%%\", . * 100))) %>% \n#   kable(format = \"markdown\", align = c(\"l\", \"l\", \"c\", \"c\", \"c\"), \n#         caption = \"Power by sample size\")"
  },
  {
    "objectID": "content/posts/2023-02-21-power-calculator-for-mol-qtls/index.html#calculate-detectable-effect-sizes",
    "href": "content/posts/2023-02-21-power-calculator-for-mol-qtls/index.html#calculate-detectable-effect-sizes",
    "title": "Power calculator for mol QTLs",
    "section": "calculate detectable effect sizes",
    "text": "calculate detectable effect sizes\nThe following table shows the detectable effect sizes at 80% power with a significance level of 0.05/1000. Variance of Y is standardized to 1.\n\n\nShow the code\ncalc_mateff = function(mafvec,nvec,outmat=FALSE)\n{\n  mateff = matrix(NA,length(mafvec)*length(nvec),3)\ncolnames(mateff) = c(\"maf\",\"nsam\",\"power\")\ncont = 1\nfor(maf in mafvec)\n{\n  for(nn in nvec)\n  {\n      # r2 = eff^2 * 2 * maf * (1-maf)\n      # rr = sqrt(r2)\n      pp = pwr::pwr.r.test(n = nn, power=0.80 , sig.level = alpha)\n      eff = pp$r/sqrt(2*maf*(1-maf))\n      mateff[cont,] = c(maf,  nn, eff)\n      cont = cont + 1\n  }\n}\n  mateff = mateff %>% data.frame() %>% \n  pivot_wider(names_from = nsam, values_from = power) %>% \n  mutate_at(vars(-c(1)), ~round(., 2)) \n  print(mateff)\n  if(outmat) mateff\n} \n\nmafvec = c(0.05, 0.10, 0.30) \nnvec = c(100,500,1000,7000) \nnsnps = 1000\nalpha = 0.05/nsnps\n## ---\nmat = calc_mateff(mafvec,nvec,outmat=TRUE)\n\n\n# A tibble: 3 × 5\n    maf `100` `500` `1000` `7000`\n  <dbl> <dbl> <dbl>  <dbl>  <dbl>\n1  0.05  1.5   0.7    0.5    0.19\n2  0.1   1.09  0.51   0.36   0.14\n3  0.3   0.71  0.33   0.24   0.09\n\n\nShow the code\nmat %>%  kable(format = \"markdown\", align = c(\"l\", \"c\", \"c\", \"c\", \"c\"), caption = \"Detectable effects w/1000 SNPs\",label=NA) \n\n\n\nDetectable effects w/1000 SNPs\n\n\nmaf\n100\n500\n1000\n7000\n\n\n\n\n0.05\n1.50\n0.70\n0.50\n0.19\n\n\n0.10\n1.09\n0.51\n0.36\n0.14\n\n\n0.30\n0.71\n0.33\n0.24\n0.09\n\n\n\n\n\nShow the code\nmafvec = c(0.05, 0.10, 0.30) \nnvec = c(1000,6000,10000,100000) \nnsnps = 1e6\nalpha = 0.05/nsnps\n## ---\nmat = calc_mateff(mafvec,nvec,outmat=TRUE)\n\n\n# A tibble: 3 × 5\n    maf `1000` `6000` `10000` `1e+05`\n  <dbl>  <dbl>  <dbl>   <dbl>   <dbl>\n1  0.05   0.64   0.26    0.2     0.06\n2  0.1    0.46   0.19    0.15    0.05\n3  0.3    0.3    0.13    0.1     0.03\n\n\nShow the code\nmat %>%  kable(format = \"markdown\", align = c(\"l\", \"c\", \"c\", \"c\", \"c\"), caption = \"Detectable effects w/1e6 SNPs\",label=NA) \n\n\n\nDetectable effects w/1e6 SNPs\n\n\nmaf\n1000\n6000\n10000\n1e+05\n\n\n\n\n0.05\n0.64\n0.26\n0.20\n0.06\n\n\n0.10\n0.46\n0.19\n0.15\n0.05\n\n\n0.30\n0.30\n0.13\n0.10\n0.03"
  },
  {
    "objectID": "content/posts/2023-02-21-power-calculator-for-mol-qtls/index.html#power-for-xcan-association-continuous-x",
    "href": "content/posts/2023-02-21-power-calculator-for-mol-qtls/index.html#power-for-xcan-association-continuous-x",
    "title": "Power calculator for mol QTLs",
    "section": "power for Xcan association (continuous X)",
    "text": "power for Xcan association (continuous X)\n\n\nShow the code\nnvec = c(10000,100000,500000) \nntests = 10000\nalpha = 0.05/ntests\n\ncalc_matr = function(nvec,outmat=FALSE)\n{\n  mateff = matrix(NA,length(nvec),2)\n  colnames(mateff) = c(\"nsam\",\"r\")\n  cont = 1\n  for(nn in nvec)\n  {\n    pp = pwr::pwr.r.test(n = nn, power=0.80 , sig.level = alpha)\n    mateff[cont,] = c(nn, pp$r)\n    cont = cont + 1\n  }\n  mateff = mateff %>% data.frame() %>%  mutate(r2 = r^2) %>% mutate_at(vars(-c(1)), ~signif(., 2)) \n  print(mateff)\n  if(outmat) mateff\n} \n\ncalc_matr(nvec,outmat=TRUE) %>% knitr::kable()\n\n\n   nsam      r      r2\n1 1e+04 0.0540 2.9e-03\n2 1e+05 0.0170 2.9e-04\n3 5e+05 0.0077 5.9e-05\n\n\n\n\n\nnsam\nr\nr2\n\n\n\n\n1e+04\n0.0540\n2.9e-03\n\n\n1e+05\n0.0170\n2.9e-04\n\n\n5e+05\n0.0077\n5.9e-05\n\n\n\n\n\nWith sample sizes of {`r nvec}, we can detect genes that affect"
  },
  {
    "objectID": "content/posts/2023-03-28-cistromedb-data/index.html",
    "href": "content/posts/2023-03-28-cistromedb-data/index.html",
    "title": "Cistrome DB data",
    "section": "",
    "text": "suppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\n##PRE=\"/Users/margaretperry/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data \"\n##PRE=\"/Users/temi/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data\"\n## COPY THE DATE AND SLUG fields FROM THE HEADER\nSLUG=\"cistromedb-data\" ## copy the slug from the header\nbDATE='2023-03-28' ## copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n\n## move data to DATA\n#tempodata=(\"~/Downloads/tempo/gwas_catalog_v1.0.2-associations_e105_r2022-04-07.tsv\")\n#system(glue::glue(\"cp {tempodata} {DATA}/\"))\n\n## system(glue(\"open {DATA}\")) ## this will open the folder \n\n\ndata = read_tsv(glue(\"{DATA}/human_factor_full_QC.txt\"))\n\nRows: 11348 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (6): Species, GSMID, Factor, Cell_line, Cell_type, Tissue_type\ndbl (7): DCid, FastQC, UniquelyMappedRatio, PBC, PeaksFoldChangeAbove10, FRi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnames(data)\n\n [1] \"DCid\"                   \"Species\"                \"GSMID\"                 \n [4] \"Factor\"                 \"Cell_line\"              \"Cell_type\"             \n [7] \"Tissue_type\"            \"FastQC\"                 \"UniquelyMappedRatio\"   \n[10] \"PBC\"                    \"PeaksFoldChangeAbove10\" \"FRiP\"                  \n[13] \"PeaksUnionDHSRatio\"    \n\ndata %>% select(Factor,Cell_line,Cell_type,Tissue_type) %>% unique() %>% dim()\n\n[1] 4426    4\n\ndata %>% count(Factor,Cell_line,Cell_type,Tissue_type) %>% arrange(desc(n))\n\n# A tibble: 4,426 × 5\n   Factor Cell_line Cell_type  Tissue_type     n\n   <chr>  <chr>     <chr>      <chr>       <int>\n 1 ESR1   MCF-7     Epithelium Breast        213\n 2 AR     LNCaP     Epithelium Prostate      143\n 3 POLR2A HeLa      Epithelium Cervix         76\n 4 AR     VCaP      Epithelium Prostate       64\n 5 POLR2A MCF-7     Epithelium Breast         64\n 6 NR3C1  A549      Epithelium Lung           46\n 7 POLR2A HCT-116   None       HCT116         46\n 8 CTCF   MCF-7     Epithelium Breast         45\n 9 FOXA1  LNCaP     Epithelium Prostate       45\n10 ESR1   None      None       Breast         42\n# … with 4,416 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\ndata %>% count(Factor,Cell_line,Cell_type,Tissue_type) %>% .[[\"n\"]] %>% table()\n\n.\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n2223 1196  299  282   89  106   36   43   18   28   12   17    4    8    7    6 \n  17   18   19   20   22   23   24   25   26   28   29   30   31   32   33   34 \n   1    8    3    2    1    1    2    3    2    1    1    3    2    2    3    3 \n  37   38   40   42   45   46   64   76  143  213 \n   2    1    1    1    2    2    2    1    1    1 \n\n## are cell line==None non tumor?\ndata %>% filter(Cell_line==\"None\") %>% dim()\n\n[1] 1817   13\n\n## how many unique cell lines?\ndata %>% count(Cell_line) %>% dim()\n\n[1] 520   2\n\n## how many unique cell types?\ndata %>% count(Cell_type) %>% dim()\n\n[1] 153   2\n\n## how many unique tissue types?\ndata %>% count(Tissue_type) %>% dim()\n\n[1] 84  2"
  },
  {
    "objectID": "content/posts/2023-03-01-how-to-create-new-posts/index.html",
    "href": "content/posts/2023-03-01-how-to-create-new-posts/index.html",
    "title": "How to create a new blog post",
    "section": "",
    "text": "create a new folder under post/new_folder\nname the folder with year-month-date-slug, 2023-03-02-descriptive-title\ncreate a file named index.qmd and add the following header, update the title, your name and the date"
  },
  {
    "objectID": "content/posts/2023-03-01-how-to-create-new-posts/index.html#other-links",
    "href": "content/posts/2023-03-01-how-to-create-new-posts/index.html#other-links",
    "title": "How to create a new blog post",
    "section": "other links",
    "text": "other links\nUseful tips for new posts here"
  },
  {
    "objectID": "content/posts/2023-03-01-how-to-create-new-posts/index.html#functions",
    "href": "content/posts/2023-03-01-how-to-create-new-posts/index.html#functions",
    "title": "How to create a new blog post",
    "section": "functions",
    "text": "functions"
  },
  {
    "objectID": "content/posts/2023-03-28-erap2-finemapping/index.html",
    "href": "content/posts/2023-03-28-erap2-finemapping/index.html",
    "title": "ERAP2 fine-mapping",
    "section": "",
    "text": "Show the code\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\n##PRE=\"/Users/margaretperry/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data \"\n##PRE=\"/Users/temi/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data\"\n## COPY THE DATE AND SLUG fields FROM THE HEADER\nSLUG=\"erap2-fine-mapping\" ## copy the slug from the header\nbDATE='2023-03-28' ## copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n\n## move data to DATA\n#tempodata=(\"~/Downloads/tempo/gwas_catalog_v1.0.2-associations_e105_r2022-04-07.tsv\")\n#system(glue::glue(\"cp {tempodata} {DATA}/\"))\n\n## system(glue(\"open {DATA}\")) ## this will open the folder \n\n\nERAP2 fine-mapping results DAPG\n\n\nShow the code\n## query\n## SELECT * FROM `gtex-awg-im.GTEx_V8_DAPG.variants_pip_eqtl` where gene like \"ENSG00000164308%\"\nerap2 = read_csv(glue(\"{DATA}/bquxjob_41de6a2f_18728cf1999.csv\"))\n\n\nRows: 2260 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): tissue, gene, variant_id\ndbl (4): rank, pip, log10_abvf, cluster_id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\n## SELECT * FROM `gtex-awg-im.GTEx_V8_DAPG.variants_pip_sqtl` where variant_id like \"chr5_96900192%\" order by pip desc\ntauras_snp = read_csv(glue(\"{DATA}/bquxjob_719c0131_18728db8878.csv\"))\n\n\nRows: 113 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): tissue, gene_id, variant_id\ndbl (4): rank, pip, log10_abvf, cluster_id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\n## finemapping for the intron affected by chr5_96900192\n## SELECT * FROM `gtex-awg-im.GTEx_V8_DAPG.variants_pip_sqtl` where gene_id like \"intron_5_96900189_96901506\" order by pip desc\n\nintron = read_csv(glue(\"{DATA}/bquxjob_41bc6351_18728e4ee94.csv\"))\n\n\nRows: 2439 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): tissue, gene_id, variant_id\ndbl (4): rank, pip, log10_abvf, cluster_id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\n##\nintron %>% filter(tissue==\"Cells_EBV-transformed_lymphocytes\") %>% arrange(desc(pip))\n\n\n# A tibble: 122 × 7\n   tissue                           gene_id  rank varia…¹    pip log10…² clust…³\n   <chr>                            <chr>   <dbl> <chr>    <dbl>   <dbl>   <dbl>\n 1 Cells_EBV-transformed_lymphocyt… intron…     2 chr5_9… 0.250     25.8       2\n 2 Cells_EBV-transformed_lymphocyt… intron…     1 chr5_9… 0.250     25.8       2\n 3 Cells_EBV-transformed_lymphocyt… intron…     3 chr5_9… 0.218     27.7       1\n 4 Cells_EBV-transformed_lymphocyt… intron…     4 chr5_9… 0.218     27.7       1\n 5 Cells_EBV-transformed_lymphocyt… intron…     5 chr5_9… 0.0902    27.7       1\n 6 Cells_EBV-transformed_lymphocyt… intron…     7 chr5_9… 0.0426    27.7       1\n 7 Cells_EBV-transformed_lymphocyt… intron…     6 chr5_9… 0.0426    27.7       1\n 8 Cells_EBV-transformed_lymphocyt… intron…     8 chr5_9… 0.0426    27.7       1\n 9 Cells_EBV-transformed_lymphocyt… intron…     9 chr5_9… 0.0380    25.8       2\n10 Cells_EBV-transformed_lymphocyt… intron…    10 chr5_9… 0.0371    25.8       2\n# … with 112 more rows, and abbreviated variable names ¹​variant_id,\n#   ²​log10_abvf, ³​cluster_id\n# ℹ Use `print(n = ...)` to see more rows\n\n\nShow the code\n## \n\n## erap2 %>% filter(pip>0.1) %>% group_by(variant_id) %>% summarise(sumpip=sum(pip),ntissues=n()) %>% ggplot(aes(variant_id,sumpip)) + geom_bar(stat = \"identity\") + geom_point() + ggtitle(\"ERAP2 expr: most tissues assign pip to 16728 & 16885\")\n\nerap2 %>% filter(pip>0.1) %>% ggplot(aes(variant_id,pip)) + geom_violin() + geom_boxplot(width=0.05,alpha=0.5,outlier.shape = NA) + geom_point() + ggtitle(\"ERAP2 expr: most tissues assign pip to 16728 & 16885\") + ylim(0,NA)\n\n\nWarning: Groups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\n\n\n\n\n\nShow the code\nprint(\"intron intron_5_96900189_96901506 \")\n\n\n[1] \"intron intron_5_96900189_96901506 \"\n\n\nShow the code\nintron %>% filter(pip>0.1) %>% ggplot(aes(variant_id,pip)) + geom_violin() + geom_boxplot(width=0.05,alpha=0.5,outlier.shape = NA) + geom_point() + ggtitle(\"ERAP2 intron_5_96900189_96901506:\") + ylim(0,NA) + coord_flip()\n\n\nWarning: Groups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\n\n\n\n\n\nShow the code\n#intron %>% filter(pip>0.1) %>% group_by(variant_id) %>% summarise(sumpip=sum(pip),ntissues=n()) %>% ggplot(aes(variant_id,sumpip)) + geom_bar(stat = \"identity\") + ggtitle(\"ERAP2 intron_5_96900189_96901506: \") + coord_flip()\n\n#ggplot(aes(variant_id,sumpip)) + geom_bar() \n\n\nCausal SNP according to the black death paper and others is rs2248374 chr5_96900192"
  },
  {
    "objectID": "content/posts/2023-05-26-metaboxcan-paper-analysis/index.html",
    "href": "content/posts/2023-05-26-metaboxcan-paper-analysis/index.html",
    "title": "Miscel MetaboXcan paper analysis",
    "section": "",
    "text": "#gene2metabo Metsim - `/gpfs/data/im-lab/nas40t2/festus/metabolomics/paper-data/gene2metabo-metsim-softimpute-all.db`\n\n#gene2metabo Guardian - `/gpfs/data/im-lab/nas40t2/festus/metabolomics/paper-data/gene2metabo-guardian-irasfs-all.db`\n\n#Metsim-lasso-model - `/gpfs/data/im-lab/nas40t2/festus/metabolomics/paper-data/metsim-lasso-softimpute.db`\n\n#Guardian-lasso-model (abit older) - `/gpfs/data/im-lab/nas40t2/festus/metabolomics/paper-data/guardian-irasfs-lasso.db`\n\n#Guardian-bslmm-model - `/gpfs/data/im-lab/nas40t2/festus/metabolomics/paper-data/guardian-irasfs-bslmm.db`\n\n#Hybrid best model (cor > 0.1) - `/gpfs/data/im-lab/nas40t2/festus/metabolomics/paper-data/hybrid-model-best-cor0.1.db`\n\n#Hybrid all models (guardian, omicspred, metsim) - `/gpfs/data/im-lab/nas40t2/festus/metabolomics/paper-data/hybrid-model-all-metabolon.db`\n\n#Metaboxcan reports (v1) using the Hybrid-models - `/Users/user/Box/imlab-data/Metaboxcan/old-summaries` - the metaboxcan_best contains the results from the best models above and pvalue corrected while the metaboxcan_all contains the results from all of the hybrid models combined without filtering. The metsim model used in the hybrid was the one generated using PRScs from the summary stats.\n\n#Metaboxcan reports (v2) using the Metsim lasso models - `/Users/user/Box/imlab-data/Metaboxcan/summaries` - the metaboxcan_best and metaboxcan_all sheet contain same results only one difference is best contains pvalues corrected by bacon."
  },
  {
    "objectID": "content/posts/2023-02-28-predictdb-weights-distribution/index.html",
    "href": "content/posts/2023-02-28-predictdb-weights-distribution/index.html",
    "title": "PredictDB weight distribution",
    "section": "",
    "text": "Goal: get effect size distribution of omic traits\n\n\nShow the code\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\n##PRE=\"/Users/margaretperry/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data \"\n##PRE=\"/Users/temi/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data\"\n## COPY THE DATE AND SLUG fields FROM THE HEADER\nSLUG=\"predictdb-weight-distribution\" ## copy the slug from the header\nbDATE='2023-02-28' ## copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\nUSERNAME=\"haekyungim\"\n## move data to DATA\n#tempodata=(\"~/Downloads/tempo/gwas_catalog_v1.0.2-associations_e105_r2022-04-07.tsv\")\n#system(glue::glue(\"cp {tempodata} {DATA}/\"))\n## system(glue(\"open {DATA}\")) ## this will open the folder \n\n\n\ndownload mashr gtex v8 prediction models for various\n\n\n\nShow the code\nif(F)\n{\n  setwd(DATA)\n  system(glue(\"wget https://zenodo.org/record/3518299/files/mashr_eqtl.tar?download=1\"))\n  system(glue(\"tar xvf mashr_eqtl.tar\"))\n  system(glue(\"cd eqtl\"))\n  system(\"rm *.txt.gz\")\n}\n\n\n\nplot distribution, show summary\n\n\n\nShow the code\nlibrary(\"RSQLite\")\nsqlite <- dbDriver(\"SQLite\")\n\nmyshowdist = function(dbname,titulo=\"n=\")\n{\nprint(dbname)\ndb = dbConnect(sqlite,dbname)\nweights = dbGetQuery(db, \"select * from weights\")\nabsweivec = abs(weights$weight)\nqlines = quantile(absweivec,c(0.2,0.5)) %>% round(2)\nquantile(absweivec,(0:10)/10) %>% round(2) %>% print()\npp <- weights %>% mutate(abswei=abs(weight)) %>% ggplot(aes(abswei))+geom_histogram() + geom_vline(xintercept=qlines) +ggtitle(glue(\"{titulo}; 20%={qlines[1]}, median={qlines[2]}\"))\nprint(pp)\ndbDisconnect(db)\n}\n\n\n\nread weights adipose with 491 samples\n\n\n\nShow the code\ndbname <- glue(\"{DATA}/eqtl/mashr_Adipose_Subcutaneous.db\") ## add full path if db file not in current directory\nmyshowdist(dbname,titulo=\"Adipose expr n=491\")\n\n\n/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/2023-02-28-predictdb-weight-distribution/eqtl/mashr_Adipose_Subcutaneous.db\n  0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n0.00 0.02 0.05 0.08 0.12 0.15 0.20 0.25 0.34 0.50 2.47 \n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nread weights brain substantia nigra with 101 samples\n\n\n\nShow the code\ndbname <- glue(\"{DATA}/eqtl/mashr_Brain_Substantia_nigra.db\") ## add full path if db file not in current directory\nmyshowdist(dbname,titulo=\"Brain SN expr - n=101\")\n\n\n/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/2023-02-28-predictdb-weight-distribution/eqtl/mashr_Brain_Substantia_nigra.db\n  0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n0.00 0.01 0.02 0.04 0.07 0.10 0.14 0.20 0.29 0.44 2.56 \n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nread protein prediction weights\n\n\n\nShow the code\n## can download from https://uchicago.box.com/shared/static/m3zsxy3oy8kn5gkktkuo8lvui269hxi5.db\nprint(\"AA\")\n\n\n[1] \"AA\"\n\n\nShow the code\ndbname <- glue::glue(\"/Users/{USERNAME}/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/Within-Lab-Sharing/Sabrina-Data/ARIC/ARIC_AA_hg38.db\")\nmyshowdist(dbname,titulo=\"protein ARIC AA n=1,871\")\n\n\n/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/Within-Lab-Sharing/Sabrina-Data/ARIC/ARIC_AA_hg38.db\n  0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.01 0.02 0.03 0.59 \n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nShow the code\n## can download from https://uchicago.box.com/shared/static/m3zsxy3oy8kn5gkktkuo8lvui269hxi5.db\nprint(\"EUR\")\n\n\n[1] \"EUR\"\n\n\nShow the code\ndbname <- glue::glue(\"/Users/{USERNAME}/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/Within-Lab-Sharing/Sabrina-Data/ARIC/ARIC_EA_hg38.db\")\nmyshowdist(dbname,titulo=\"protein ARIC EA n=7,213\")\n\n\n/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/Within-Lab-Sharing/Sabrina-Data/ARIC/ARIC_EA_hg38.db\n  0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.01 0.01 0.03 0.55 \n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nread brainxcan weights (use elastic net to be closer to QTL effect size)\n\n\n\nShow the code\n##install.packages(\"arrow\")\ntempo = arrow::read_parquet(glue::glue(\"{DATA}/brainxcan-gw_lasso_beta.parquet\"))\ntempo = as.matrix(tempo %>% select(starts_with(\"IDP\")))\nkk = abs(tempo[tempo!=0])\nprint(summary(kk))\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0000000 0.0003630 0.0009091 0.0013655 0.0018487 0.0652540 \n\n\nShow the code\nprint(quantile(kk,(0:10)/10))\n\n\n          0%          10%          20%          30%          40%          50% \n1.513009e-11 1.207948e-04 2.761802e-04 4.559484e-04 6.659417e-04 9.090500e-04 \n         60%          70%          80%          90%         100% \n1.205991e-03 1.594788e-03 2.162211e-03 3.165862e-03 6.525400e-02 \n\n\nShow the code\nprint(quantile(kk,c(0.2,0.5)))\n\n\n         20%          50% \n0.0002761802 0.0009090500"
  },
  {
    "objectID": "content/posts/2023-03-28-multiple-testing/index.html",
    "href": "content/posts/2023-03-28-multiple-testing/index.html",
    "title": "Multiple Testing Vignette",
    "section": "",
    "text": "build intuition about p-values when multiple testing is performed via simulations\nrecognize the need for multiple testing correction\npresent methods to correct for multiple testing\n\nBonferroni correction\nFDR (false discovery rate)"
  },
  {
    "objectID": "content/posts/2023-03-28-multiple-testing/index.html#why-do-we-need-multiple-testing-correction",
    "href": "content/posts/2023-03-28-multiple-testing/index.html#why-do-we-need-multiple-testing-correction",
    "title": "Multiple Testing Vignette",
    "section": "Why do we need multiple testing correction",
    "text": "Why do we need multiple testing correction\n\n\n\nxkcd image for significance"
  },
  {
    "objectID": "content/posts/2023-03-28-multiple-testing/index.html#what-do-p-values-look-like-under-the-null-and-alternative",
    "href": "content/posts/2023-03-28-multiple-testing/index.html#what-do-p-values-look-like-under-the-null-and-alternative",
    "title": "Multiple Testing Vignette",
    "section": "What do p-values look like under the null and alternative?",
    "text": "What do p-values look like under the null and alternative?\n\nSimulate vectors X, Yalt=\\(X\\cdot \\beta + \\epsilon\\) and Ynull independent of X\nWe start defining some parameters for the simulations. The need for these will become obvious later.\n\n## set seed to make simulations reproducible\n## set.seed(20210108)\n\n## let's start with some parameter definitions\nnsamp = 100\nbeta = 2\nh2 = 0.1\nsig2X = h2\nsig2epsi = (1 - sig2X) * beta^2\nsigX = sqrt(sig2X)\nsigepsi = sqrt(sig2epsi)\n\nNext, we simulate a vectors X and \\(\\epsilon\\), and Ynull, all normally distributed\n\nX = rnorm(nsamp,mean=0, sd= sigX)\nepsi = rnorm(nsamp,mean=0, sd=sigepsi)\n## generate Ynull (X has no effect on Ynull)\nYnull = rnorm(nsamp, mean=0, sd=beta)\n\nCalculate Yalt = X * beta + epsi\n\nYalt = X * beta + epsi\n\nVisualize Yalt vs X\n\nplot(X, Yalt, main=\"Yalt vs X\"); grid()\n\n\n\n\nVisualize Ynull vs X\n\nplot(X, Ynull, main=\"Ynull vs X\");grid()\n\n\n\n\nTest association between Ynull and X\n\nsummary(lm(Ynull ~ X))\n\n\nCall:\nlm(formula = Ynull ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8431 -1.3431 -0.2234  1.2499  5.8569 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   0.1614     0.2046   0.789    0.432\nX            -0.2765     0.6653  -0.416    0.679\n\nResidual standard error: 2.039 on 98 degrees of freedom\nMultiple R-squared:  0.00176,   Adjusted R-squared:  -0.008426 \nF-statistic: 0.1728 on 1 and 98 DF,  p-value: 0.6786\n\n\n\nwhat’s the p-value of the association?\nis the p-value significant at 5% significance leve?\n\nNext, test the association between Yalt and X\n\nsummary(lm(Yalt ~ X))\n\n\nCall:\nlm(formula = Yalt ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.9610 -1.3250  0.0524  1.6622  4.2743 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  -0.2102     0.1976  -1.064  0.29004   \nX             1.8297     0.6424   2.848  0.00536 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.969 on 98 degrees of freedom\nMultiple R-squared:  0.07645,   Adjusted R-squared:  0.06703 \nF-statistic: 8.113 on 1 and 98 DF,  p-value: 0.005357\n\n\n\nwhat’s the p-value of the association?\nis the p-value significant at 5% significance level?"
  },
  {
    "objectID": "content/posts/2023-03-28-multiple-testing/index.html#calculate-the-empirical-distribution-of-p-values",
    "href": "content/posts/2023-03-28-multiple-testing/index.html#calculate-the-empirical-distribution-of-p-values",
    "title": "Multiple Testing Vignette",
    "section": "Calculate the empirical distribution of p-values",
    "text": "Calculate the empirical distribution of p-values\nTo calculate the empirical distribution of p-values under the null and alternatives we will simulate X, Yalt, Ynull for 10,000 times.\n\nDefine a convenience function fastlm, will do linear regression much faster\nWe want to run 10,000 times this same regression, so here we define a function fastlm that will get us the p-values and regression coefficients.\n\nfastlm = function(xx,yy)\n{\n  ## compute betahat (regression coef) and pvalue with Ftest\n  ## for now it does not take covariates\n  \n  df1 = 2\n  df0 = 1\n  ind = !is.na(xx) & !is.na(yy)\n  xx = xx[ind]\n  yy = yy[ind]\n  n = sum(ind)\n  xbar = mean(xx)\n  ybar = mean(yy)\n  xx = xx - xbar\n  yy = yy - ybar\n  \n  SXX = sum( xx^2 )\n  SYY = sum( yy^2 )\n  SXY = sum( xx * yy )\n  \n  betahat = SXY / SXX\n  \n  RSS1 = sum( ( yy - xx * betahat )^2 )\n  RSS0 = SYY\n  \n  fstat = ( ( RSS0 - RSS1 ) / ( df1 - df0 ) )  / ( RSS1 / ( n - df1 ) )\n  pval = 1 - pf(fstat, df1 = ( df1 - df0 ), df2 = ( n - df1 ))\n  res = list(betahat = betahat, pval = pval)\n  \n  return(res)\n}\n\n\n\nSimulate vectors X, Ynull, Yalt 10,000 times\n\nnsim = 10000\n## simulate normally distributed X and epsi\nXmat = matrix(rnorm(nsim * nsamp,mean=0, sd= sigX), nsamp, nsim)\nepsimat = matrix(rnorm(nsim * nsamp,mean=0, sd=sigepsi), nsamp, nsim)\n\n## generate Yalt (X has an effect on Yalt)\nYmat_alt = Xmat * beta + epsimat\n\n## generate Ynull (X has no effect on Ynull)\nYmat_null = matrix(rnorm(nsim * nsamp, mean=0, sd=beta), nsamp, nsim)\n\n## let's look at the dimensions of the simulated matrices\n\ndim(Ymat_null)\n\n[1]   100 10000\n\ndim(Ymat_alt)\n\n[1]   100 10000\n\n\nNow we have 10000 independent simulations of X, Ynull, and Yalt\n\n## give them names so that we can refer to them later more easily\ncolnames(Ymat_null) = paste0(\"c\",1:ncol(Ymat_null))\ncolnames(Ymat_alt) = colnames(Ymat_null)\n\n\nTo calculate p-values under the null run 10,000 linear regressions using X and Ynull\n\n\npvec_null = rep(NA,nsim)\nbvec_null = rep(NA,nsim)\n\nfor(ss in 1:nsim)\n{\n  fit = fastlm(Xmat[,ss], Ymat_null[,ss])\n  pvec_null[ss] = fit$pval  \n  bvec_null[ss] = fit$betahat\n}\n\nsummary(pvec_null)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0002453 0.2469528 0.4923302 0.4975967 0.7537629 0.9999578 \n\nhist(pvec_null,xlab=\"p-value\",main=\"Histogram of p-values under Null\")\n\n\n\n\n\nhow many simulations under the null yield p-value below 0.05? What percentage is that?\n\n\nsum(pvec_null<0.05)\n\n[1] 523\n\nmean(pvec_null<0.05)\n\n[1] 0.0523\n\n\n\nhow many simulations under the null yield p-value < 0.20?\nwhat do you think the proportion of simulations with p-values < \\(\\alpha\\) (\\(\\alpha\\) between 0 and 1) will be roughly?\nWhy do we need to use more stringent significance level when we test many times?"
  },
  {
    "objectID": "content/posts/2023-03-28-multiple-testing/index.html#bonferroni-correction",
    "href": "content/posts/2023-03-28-multiple-testing/index.html#bonferroni-correction",
    "title": "Multiple Testing Vignette",
    "section": "Bonferroni correction",
    "text": "Bonferroni correction\nUse as the new threshold the original one divided by the number of tests. So typically\n\\[\\frac{0.05}{\\text{total number of tests}}\\]\n\nwhat’s the Bonferroni threshold for significance in this simulation?\nhow many did we find?\n\n\nBF_thres = 0.05/nsim\n## Bonferroni significance threshold\nprint(BF_thres) \n\n[1] 5e-06\n\n## number of Bonferroni significant associations\nsum(pvec_null<BF_thres)\n\n[1] 0\n\n## proportion of Bonferroni significant associations\nmean(pvec_null<BF_thres)\n\n[1] 0"
  },
  {
    "objectID": "content/posts/2023-03-28-multiple-testing/index.html#mix-of-ynull-and-yalt",
    "href": "content/posts/2023-03-28-multiple-testing/index.html#mix-of-ynull-and-yalt",
    "title": "Multiple Testing Vignette",
    "section": "Mix of Ynull and Yalt",
    "text": "Mix of Ynull and Yalt\nLet’s see what happens when we add a bunch of true associations in the matrix of null associations\n\nprop_alt=0.20 ## define proportion of alternative Ys in the mixture\nselectvec = rbinom(nsim,1,prop_alt)\nnames(selectvec) = colnames(Ymat_alt)\nselectvec[1:10]\n\n c1  c2  c3  c4  c5  c6  c7  c8  c9 c10 \n  0   0   0   0   0   0   0   0   0   0 \n\nYmat_mix = sweep(Ymat_alt,2,selectvec,FUN='*') + sweep(Ymat_null,2,1-selectvec,FUN='*')\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun linear regression for all 10,000 phenotypes in the mix of true and false associations, Ymat_mix\n\npvec_mix = rep(NA,nsim)\nbvec_mix = rep(NA,nsim)\nfor(ss in 1:nsim)\n{\n  fit = fastlm(Xmat[,ss], Ymat_mix[,ss])\n  pvec_mix[ss] = fit$pval  \n  bvec_mix[ss] = fit$betahat\n}\nsummary(pvec_mix)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.08125 0.37643 0.40485 0.68693 0.99988 \n\nhist(pvec_mix,xlab=\"p-value\",main=\"Histogram of p-values under mixture of null and alt\")\n\n\n\nm_signif = sum(pvec_mix < 0.05) ## observed number of significant associations\nm_expected = 0.05*nsim ## expected number of significant associations under the worst case scenario, where all features belong to the null \nm_signif\n\n[1] 2191\n\nm_expected\n\n[1] 500\n\n\nUnder the null, we were expecting 500 significant columns by chance but got 2191\nQ: how can we estimate the proportion of true positives?\nWe got 1691 extra columns, so it’s reasonable to expect that the extra significant results come from the alternative distribution (Yalt). So \\[\\frac{\\text{observed number of significant} - \\text{expected number of significant}}{\\text{observed number of significant}}\\] should be a good estimate of the true discovery rate. False discovery rate is defined as 1 - the true discovery rate.\n\nthres = 0.05\nFDR = sum((pvec_mix<thres & selectvec==0)) / sum(pvec_mix<thres)\n## proportion of null columns that are significant among all significant\nFDR\n\n[1] 0.1866728\n\n\nIf we use a p-value threshold of 0.05, 81.33 percent of the signficant columns are true discoveries.  In this case, we know which ones are true or false associations because we decided using the selectvec vectors which simulated Y would be a function of X or unrelated to X.\n\nwhat’s the proportion of false discoveries if we use a significance level of 0.01\nwhat’s the proportion of false discoveries if we use Bonferroni correction as the significance level?\nWhat’s the proportion of missed signals, proportion of true associations that have p-values greater than the Bonferroni threshold?"
  },
  {
    "objectID": "content/posts/2023-03-28-multiple-testing/index.html#common-approaches-to-control-type-i-errors",
    "href": "content/posts/2023-03-28-multiple-testing/index.html#common-approaches-to-control-type-i-errors",
    "title": "Multiple Testing Vignette",
    "section": "Common approaches to control type I errors",
    "text": "Common approaches to control type I errors\nAssuming we are testing \\(m\\) hypothesis, let’s define the following terms for the different errors.\n\n\n\n\n\n\n\n\n\n\nCalled Significant\nCalled not significant\nTotal\n\n\n\n\nNull true\n\\(F\\)\n\\(m_0 - F\\)\n\\(m_0\\)\n\n\nAlt true\n\\(T\\)\n\\(m_1 - T\\)\n\\(m_1\\)\n\n\nTotal\n\\(S\\)\n\\(m - S\\)\n\\(m\\)\n\n\n\n\nBonferroni correction assures that the FWER (Familywise error rate) \\(P(F \\ge 1)\\) is below the acceptable type I error, typically 0.05. \\[P(F \\ge 1) < \\alpha. \\] We achieve that by requiring that for each test \\[p<\\alpha/\\text{# tests}.\\] This can be too stringent and lead to miss real signals.\npFDR (positive false discovery rate) \\[E\\left(\\frac{F}{S} \\rvert S>0\\right)\\]\nqvalue is the minimum false discovery rate attainable when the feature (SNP) is called significant"
  },
  {
    "objectID": "content/posts/2023-03-28-multiple-testing/index.html#table-of-null-or-alternative-vs.-significant-or-not-significant",
    "href": "content/posts/2023-03-28-multiple-testing/index.html#table-of-null-or-alternative-vs.-significant-or-not-significant",
    "title": "Multiple Testing Vignette",
    "section": "Table of null or alternative vs. significant or not significant",
    "text": "Table of null or alternative vs. significant or not significant\n\ncount_table = t(table(pvec_mix>0.05, selectvec))\ncolnames(count_table) = c(\"Called significant\", \"Called not significant\")\nrownames(count_table) = c(\"Null true\", \"Alt true\")\nknitr::kable(count_table)\n\n\n\n\n\nCalled significant\nCalled not significant\n\n\n\n\nNull true\n409\n7611\n\n\nAlt true\n1782\n198\n\n\n\n\n\nLet’s calculate the qvalue"
  },
  {
    "objectID": "content/posts/2023-03-28-multiple-testing/index.html#use-qvalue-package-to-calculate-fdr-and",
    "href": "content/posts/2023-03-28-multiple-testing/index.html#use-qvalue-package-to-calculate-fdr-and",
    "title": "Multiple Testing Vignette",
    "section": "Use qvalue package to calculate FDR and",
    "text": "Use qvalue package to calculate FDR and\nLet’s check whether small qvalues correspond to true associations (i.e. the phenotype was generated under the alternative distribution)\n\n## install qvalue if not available.\nif(F) ## I set it to F now because I already installed the qvalue package\n{if (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"qvalue\")\n}\n\n## calculate qvalue using the qvalue function, which returns a list of values, we select the qvalue vector, which assigns the false discovery rate if the threshold for significance was the p-value of the same simulation vector\n\nqres_mix = qvalue::qvalue(pvec_mix)\nqvec_mix = qres_mix$qvalue\n\nqres_null = qvalue::qvalue(pvec_null)\nqvec_null = qres_null$qvalue\n\nboxplot(qvec_mix~selectvec)\n\n\n\n##plot(qvec_mix,col=selectvec*2 + 1, pch=selectvec + 1, lwd=selectvec*2 + 1) \n## using selectvec*2 + 1 as a quick way to get the vector to be 1 and 3 (1 is black 3 is green) instead of 1 and 2 (2 is read and can be difficult to read for color blind people)\n\nPlot sorted qvalues and color by the selectvec status (true association status)\n\nind=order(qvec_mix,decreasing=FALSE)\nplot(sort(qvec_mix),col=selectvec[ind]*2 + 1, pch=selectvec[ind] + 1, lwd=selectvec[ind]*2 + 1) \n\n\n\nsummary(qvec_mix) \n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0000001 0.2605298 0.6035350 0.4948872 0.7344018 0.8018152 \n\n\n\ndo small qvalues tend to be true?\ninterpret the figure\n\n\n## distribution of qvalues and pvalues by causal status\nboxplot(pvec_mix ~ selectvec, main='pvalue by causal status; 1=alt, 0=null')\n\n\n\nboxplot(qvec_mix ~ selectvec, main='qvalue by causal status; 1=alt, 0=null')\n\n\n\n\n\ninterpret these figures"
  },
  {
    "objectID": "content/posts/2023-03-28-multiple-testing/index.html#how-do-qvalues-and-pvalues-relate-to-each-other",
    "href": "content/posts/2023-03-28-multiple-testing/index.html#how-do-qvalues-and-pvalues-relate-to-each-other",
    "title": "Multiple Testing Vignette",
    "section": "How do qvalues and pvalues relate to each other?",
    "text": "How do qvalues and pvalues relate to each other?\n\nplot(pvec_null,qvec_null,main='qvalue vs pvalue for null')\n\n\n\nplot(pvec_mix,qvec_mix,main='qvalue vs pvalue for mixture of null and alt')\n\n\n\n\n\nq-values are monotonic functions of p-values\n\n\nwhat’s the smallest qvalue when all simulations are from the null?\nwhat’s the smallest qvalue when all simulations are from the mixture?"
  },
  {
    "objectID": "content/posts/2023-03-28-multiple-testing/index.html#pi0-and-pi1",
    "href": "content/posts/2023-03-28-multiple-testing/index.html#pi0-and-pi1",
    "title": "Multiple Testing Vignette",
    "section": "pi0 and pi1",
    "text": "pi0 and pi1\npi0 is the proportion of null hypothesis which can be estimated using the qvalue package 1 - pi1 is the proportion of true positive associations. This is a useful parameter as we will see in later classes.\n\nqres_null$pi0\n\n[1] 0.9846734\n\nqres_mix$pi0\n\n[1] 0.8019101\n\n\n\nhow many true positive proportion did you expect given the simulations we performed?"
  },
  {
    "objectID": "content/posts/2023-03-28-multiple-testing/index.html#references",
    "href": "content/posts/2023-03-28-multiple-testing/index.html#references",
    "title": "Multiple Testing Vignette",
    "section": "References",
    "text": "References\nStorey, John D., and Robert Tibshirani. 2003. “Statistical Significance for Genomewide Studies.” Proceedings of the National Academy of Sciences 100 (16): 9440–45."
  },
  {
    "objectID": "content/posts/2023-03-23-jane-austen-corpus/index.html",
    "href": "content/posts/2023-03-23-jane-austen-corpus/index.html",
    "title": "Jane Austen Corpus",
    "section": "",
    "text": "Show the code\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(glue))\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\n##PRE=\"/Users/margaretperry/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data \"\n##PRE=\"/Users/temi/Library/CloudStorage/Box-Box/imlab-data/data-Github/web-data\"\n## COPY THE DATE AND SLUG fields FROM THE HEADER\nSLUG=\"jane-austen-corpus\" ## copy the slug from the header\nbDATE='2023-03238' ## copy the date from the blog's header here\nDATA = glue(\"{PRE}/{bDATE}-{SLUG}\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n\n## move data to DATA\n#tempodata=(\"~/Downloads/tempo/gwas_catalog_v1.0.2-associations_e105_r2022-04-07.tsv\")\n#system(glue::glue(\"cp {tempodata} {DATA}/\"))\nsystem(glue(\"open {DATA}\")) ## this will open the folder \n\n\n\nget jane austen corpus\n\n\n\nShow the code\n##install.packages(\"janeaustenr\")"
  }
]